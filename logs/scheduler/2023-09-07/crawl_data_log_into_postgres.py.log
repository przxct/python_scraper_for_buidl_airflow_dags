[2023-09-07T13:07:15.792+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:07:15.797+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:07:15.808+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:07:15.808+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:07:16.492+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:07:16.952+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:07:16.951+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:07:17.030+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:07:17.030+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-02T00:00:00+00:00, run_after=2023-09-03T00:00:00+00:00
[2023-09-07T13:07:17.191+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:07:17.185+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"dataset_triggers": [], "_dag_id": "website_crawler", "edge_info": {}, "dagrun_timeout": 60.0, "schedule_interval": "@daily" ... (1131 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 13, 7, 16, 550693, tzinfo=Timezone('UTC')), 'dag_hash': 'dec733f1511b4e8dc89b268c47ae746f', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T13:07:17.198+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:07:17.198+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:07:17.201+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"dataset_triggers": [], "_dag_id": "website_crawler", "edge_info": {}, "dagrun_timeout": 60.0, "schedule_interval": "@daily" ... (1131 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 13, 7, 16, 550693, tzinfo=Timezone('UTC')), 'dag_hash': 'dec733f1511b4e8dc89b268c47ae746f', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T13:07:49.453+0000] {processor.py:157} INFO - Started process (PID=44) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:07:49.458+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:07:49.467+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:07:49.466+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:07:49.659+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:07:49.965+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:07:49.964+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:07:49.989+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:07:49.989+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:07:50.021+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.576 seconds
[2023-09-07T13:08:20.485+0000] {processor.py:157} INFO - Started process (PID=54) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:08:20.493+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:08:20.516+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:08:20.515+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:08:20.679+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:08:20.752+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:08:20.751+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:08:20.791+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:08:20.790+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:08:20.813+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.343 seconds
[2023-09-07T13:08:51.513+0000] {processor.py:157} INFO - Started process (PID=64) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:08:51.518+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:08:51.532+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:08:51.525+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:08:51.774+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:08:52.085+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:08:52.080+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:08:52.124+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:08:52.124+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:08:52.200+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.696 seconds
[2023-09-07T13:09:22.633+0000] {processor.py:157} INFO - Started process (PID=74) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:09:22.646+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:09:22.652+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:09:22.651+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:09:22.852+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:09:22.921+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:09:22.920+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:09:22.962+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:09:22.962+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:09:23.014+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.398 seconds
[2023-09-07T13:09:53.550+0000] {processor.py:157} INFO - Started process (PID=84) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:09:53.553+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:09:53.559+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:09:53.558+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:09:53.700+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:09:53.922+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:09:53.921+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:09:53.951+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:09:53.950+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:09:53.980+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.439 seconds
[2023-09-07T13:10:24.373+0000] {processor.py:157} INFO - Started process (PID=93) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:10:24.378+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:10:24.393+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:10:24.387+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:10:24.787+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:10:24.916+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:10:24.915+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:10:25.004+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:10:25.003+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:10:25.034+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.674 seconds
[2023-09-07T13:10:55.798+0000] {processor.py:157} INFO - Started process (PID=103) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:10:55.804+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:10:55.826+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:10:55.824+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:10:56.203+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:10:56.648+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:10:56.648+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:10:56.693+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:10:56.693+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:10:56.774+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.005 seconds
[2023-09-07T13:11:27.254+0000] {processor.py:157} INFO - Started process (PID=113) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:11:27.260+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:11:27.269+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:11:27.268+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:11:27.427+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:11:27.499+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:11:27.498+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:11:27.534+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:11:27.534+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:11:27.559+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.322 seconds
[2023-09-07T13:11:58.091+0000] {processor.py:157} INFO - Started process (PID=123) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:11:58.095+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:11:58.110+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:11:58.105+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:11:58.279+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:11:58.516+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:11:58.516+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:11:58.550+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:11:58.550+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:11:58.574+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.493 seconds
[2023-09-07T13:12:29.198+0000] {processor.py:157} INFO - Started process (PID=132) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:12:29.203+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:12:29.215+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:12:29.214+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:12:29.451+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:12:29.506+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:12:29.506+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:12:29.546+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:12:29.545+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:12:29.573+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.404 seconds
[2023-09-07T13:13:00.096+0000] {processor.py:157} INFO - Started process (PID=143) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:13:00.103+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:13:00.112+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:13:00.111+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:13:00.253+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:13:00.413+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:13:00.412+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:13:00.438+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:13:00.438+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:13:00.456+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.372 seconds
[2023-09-07T13:13:30.855+0000] {processor.py:157} INFO - Started process (PID=152) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:13:30.858+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:13:30.868+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:13:30.867+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:13:30.998+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:13:31.048+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:13:31.047+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:13:31.079+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:13:31.079+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:13:31.100+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.258 seconds
[2023-09-07T13:14:01.614+0000] {processor.py:157} INFO - Started process (PID=161) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:14:01.619+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:14:01.631+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:14:01.630+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:14:01.769+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:14:01.986+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:14:01.985+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:14:02.005+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:14:02.005+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:14:02.033+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.435 seconds
[2023-09-07T13:14:32.465+0000] {processor.py:157} INFO - Started process (PID=171) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:14:32.470+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:14:32.478+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:14:32.477+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:14:32.644+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:14:32.733+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:14:32.733+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:14:32.781+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:14:32.780+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:14:32.803+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.371 seconds
[2023-09-07T13:15:03.350+0000] {processor.py:157} INFO - Started process (PID=188) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:15:03.366+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:15:03.398+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:15:03.397+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:15:03.651+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:15:03.926+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:15:03.926+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:15:03.959+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:15:03.959+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:15:03.989+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.655 seconds
[2023-09-07T13:15:34.378+0000] {processor.py:157} INFO - Started process (PID=198) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:15:34.382+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:15:34.396+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:15:34.393+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:15:34.550+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:15:34.602+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:15:34.601+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:15:34.633+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:15:34.633+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:15:34.652+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.289 seconds
[2023-09-07T13:16:05.215+0000] {processor.py:157} INFO - Started process (PID=208) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:16:05.224+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:16:05.233+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:16:05.232+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:16:05.377+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:16:05.533+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:16:05.533+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:16:05.553+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:16:05.552+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:16:05.571+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.365 seconds
[2023-09-07T13:16:36.377+0000] {processor.py:157} INFO - Started process (PID=219) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:16:36.383+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:16:36.394+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:16:36.393+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:16:36.585+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:16:36.688+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:16:36.687+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:16:36.747+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:16:36.747+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:16:36.773+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.426 seconds
[2023-09-07T13:17:07.858+0000] {processor.py:157} INFO - Started process (PID=228) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:17:07.867+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:17:07.905+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:17:07.900+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:17:08.192+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:17:08.915+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:17:08.855+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:17:09.285+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:17:09.283+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:17:09.604+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.798 seconds
[2023-09-07T13:17:40.235+0000] {processor.py:157} INFO - Started process (PID=241) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:17:40.243+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:17:40.258+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:17:40.257+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:17:40.433+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:17:40.502+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:17:40.501+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:17:40.580+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:17:40.579+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:17:40.617+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.398 seconds
[2023-09-07T13:18:11.392+0000] {processor.py:157} INFO - Started process (PID=249) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:18:11.395+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:18:11.402+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:18:11.401+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:18:11.514+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:18:11.641+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:18:11.641+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:18:11.662+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:18:11.662+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:18:11.679+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.293 seconds
[2023-09-07T13:18:42.190+0000] {processor.py:157} INFO - Started process (PID=259) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:18:42.195+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:18:42.210+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:18:42.206+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:18:42.532+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:18:42.670+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:18:42.660+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:18:42.786+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:18:42.786+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:18:42.862+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.694 seconds
[2023-09-07T13:19:14.294+0000] {processor.py:157} INFO - Started process (PID=269) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:19:14.332+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:19:14.417+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:19:14.416+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:19:15.500+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:19:17.335+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:19:17.325+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:19:17.641+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:19:17.635+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:19:17.852+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 3.649 seconds
[2023-09-07T13:31:22.930+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:31:22.938+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:31:22.960+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:31:22.955+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:31:24.321+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:31:26.046+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:31:26.026+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:31:26.660+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:31:26.659+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:31:27.039+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:31:26.991+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"timezone": "UTC", "fileloc": "/opt/airflow/dags/crawl_data_log_into_postgres.py", "edge_info": {}, "schedule_interval": "@d ... (1131 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 13, 31, 24, 557273, tzinfo=Timezone('UTC')), 'dag_hash': 'dec733f1511b4e8dc89b268c47ae746f', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T13:31:27.093+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:31:27.093+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:31:27.114+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"timezone": "UTC", "fileloc": "/opt/airflow/dags/crawl_data_log_into_postgres.py", "edge_info": {}, "schedule_interval": "@d ... (1131 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 13, 31, 24, 557273, tzinfo=Timezone('UTC')), 'dag_hash': 'dec733f1511b4e8dc89b268c47ae746f', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T13:31:57.967+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:31:57.971+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:31:57.985+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:31:57.983+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:31:58.575+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:31:58.956+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:31:58.954+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:31:58.983+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:31:58.983+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:31:59.000+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.042 seconds
[2023-09-07T13:32:29.381+0000] {processor.py:157} INFO - Started process (PID=51) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:32:29.386+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:32:29.402+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:32:29.395+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:32:30.241+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:32:30.329+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:32:30.329+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:32:30.362+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:32:30.362+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:32:30.395+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.031 seconds
[2023-09-07T13:33:00.909+0000] {processor.py:157} INFO - Started process (PID=61) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:33:00.914+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:33:00.928+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:33:00.925+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:33:01.374+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:33:01.571+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:33:01.570+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:33:01.593+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:33:01.592+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:33:01.639+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.738 seconds
[2023-09-07T13:33:32.465+0000] {processor.py:157} INFO - Started process (PID=76) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:33:32.478+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T13:33:32.501+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:33:32.496+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:33:32.932+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T13:33:33.098+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:33:33.097+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:33:33.286+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:33:33.285+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:33:33.397+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.946 seconds
[2023-09-07T14:06:49.430+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:06:49.435+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:06:49.444+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:06:49.443+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:06:49.948+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:06:49.936+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/crawl_data_log_into_postgres.py", line 125, in <module>
    create_data_table >> crawl_task
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskmixin.py", line 103, in __rshift__
    self.set_downstream(other)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskmixin.py", line 261, in set_downstream
    self._set_relatives(task_or_task_list, upstream=False, edge_modifier=edge_modifier)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskmixin.py", line 238, in _set_relatives
    self.dag = dag
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 1044, in __setattr__
    super().__setattr__(key, value)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 1105, in dag
    dag.add_task(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2532, in add_task
    raise AirflowException("DAG is missing the start_date parameter")
airflow.exceptions.AirflowException: DAG is missing the start_date parameter
[2023-09-07T14:06:49.952+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:06:49.997+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.575 seconds
[2023-09-07T14:07:20.139+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:07:20.143+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:07:20.148+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:07:20.147+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:07:20.440+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:07:20.435+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/crawl_data_log_into_postgres.py", line 125, in <module>
    create_data_table >> crawl_task
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskmixin.py", line 103, in __rshift__
    self.set_downstream(other)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskmixin.py", line 261, in set_downstream
    self._set_relatives(task_or_task_list, upstream=False, edge_modifier=edge_modifier)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskmixin.py", line 238, in _set_relatives
    self.dag = dag
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 1044, in __setattr__
    super().__setattr__(key, value)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 1105, in dag
    dag.add_task(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2532, in add_task
    raise AirflowException("DAG is missing the start_date parameter")
airflow.exceptions.AirflowException: DAG is missing the start_date parameter
[2023-09-07T14:07:20.443+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:07:20.472+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.339 seconds
[2023-09-07T14:07:50.985+0000] {processor.py:157} INFO - Started process (PID=50) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:07:50.989+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:07:50.998+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:07:50.996+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:07:51.539+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:07:51.534+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/crawl_data_log_into_postgres.py", line 125, in <module>
    create_data_table >> crawl_task
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskmixin.py", line 103, in __rshift__
    self.set_downstream(other)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskmixin.py", line 261, in set_downstream
    self._set_relatives(task_or_task_list, upstream=False, edge_modifier=edge_modifier)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskmixin.py", line 238, in _set_relatives
    self.dag = dag
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 1044, in __setattr__
    super().__setattr__(key, value)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 1105, in dag
    dag.add_task(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2532, in add_task
    raise AirflowException("DAG is missing the start_date parameter")
airflow.exceptions.AirflowException: DAG is missing the start_date parameter
[2023-09-07T14:07:51.543+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:07:51.587+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.610 seconds
[2023-09-07T14:08:22.346+0000] {processor.py:157} INFO - Started process (PID=60) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:08:22.349+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:08:22.357+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:08:22.355+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:08:22.517+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:08:22.512+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/crawl_data_log_into_postgres.py", line 125, in <module>
    create_data_table >> crawl_task
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskmixin.py", line 103, in __rshift__
    self.set_downstream(other)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskmixin.py", line 261, in set_downstream
    self._set_relatives(task_or_task_list, upstream=False, edge_modifier=edge_modifier)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskmixin.py", line 238, in _set_relatives
    self.dag = dag
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 1044, in __setattr__
    super().__setattr__(key, value)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 1105, in dag
    dag.add_task(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2532, in add_task
    raise AirflowException("DAG is missing the start_date parameter")
airflow.exceptions.AirflowException: DAG is missing the start_date parameter
[2023-09-07T14:08:22.520+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:08:22.558+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.222 seconds
[2023-09-07T14:08:53.029+0000] {processor.py:157} INFO - Started process (PID=70) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:08:53.032+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:08:53.039+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:08:53.038+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:08:53.161+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:08:53.156+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/crawl_data_log_into_postgres.py", line 125, in <module>
    create_data_table >> crawl_task
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskmixin.py", line 103, in __rshift__
    self.set_downstream(other)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskmixin.py", line 261, in set_downstream
    self._set_relatives(task_or_task_list, upstream=False, edge_modifier=edge_modifier)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskmixin.py", line 238, in _set_relatives
    self.dag = dag
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 1044, in __setattr__
    super().__setattr__(key, value)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 1105, in dag
    dag.add_task(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2532, in add_task
    raise AirflowException("DAG is missing the start_date parameter")
airflow.exceptions.AirflowException: DAG is missing the start_date parameter
[2023-09-07T14:08:53.163+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:08:53.204+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.184 seconds
[2023-09-07T14:09:23.660+0000] {processor.py:157} INFO - Started process (PID=80) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:09:23.663+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:09:23.668+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:09:23.668+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:09:23.794+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:09:23.788+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/crawl_data_log_into_postgres.py", line 125, in <module>
    create_data_table >> crawl_task
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskmixin.py", line 103, in __rshift__
    self.set_downstream(other)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskmixin.py", line 261, in set_downstream
    self._set_relatives(task_or_task_list, upstream=False, edge_modifier=edge_modifier)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskmixin.py", line 238, in _set_relatives
    self.dag = dag
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 1044, in __setattr__
    super().__setattr__(key, value)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 1105, in dag
    dag.add_task(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2532, in add_task
    raise AirflowException("DAG is missing the start_date parameter")
airflow.exceptions.AirflowException: DAG is missing the start_date parameter
[2023-09-07T14:09:23.796+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:09:23.820+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.167 seconds
[2023-09-07T14:12:21.703+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:12:21.707+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:12:21.713+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:12:21.712+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:12:22.230+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:12:22.540+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:12:22.539+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:12:22.630+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:12:22.630+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:12:23.079+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:12:23.073+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"dagrun_timeout": 60.0, "_dag_id": "website_crawler", "_task_group": {"_group_id": null, "prefix_group_id": true, "tooltip": ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 14, 12, 22, 277729, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T14:12:23.087+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:12:23.087+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:12:23.091+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"dagrun_timeout": 60.0, "_dag_id": "website_crawler", "_task_group": {"_group_id": null, "prefix_group_id": true, "tooltip": ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 14, 12, 22, 277729, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T14:12:53.307+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:12:53.309+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:12:53.313+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:12:53.312+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:12:53.511+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:12:53.606+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:12:53.606+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:12:53.623+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:12:53.622+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:12:53.643+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.340 seconds
[2023-09-07T14:13:24.171+0000] {processor.py:157} INFO - Started process (PID=50) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:13:24.175+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:13:24.190+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:13:24.188+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:13:24.835+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:13:24.877+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:13:24.876+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:13:24.919+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:13:24.919+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:13:24.948+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.792 seconds
[2023-09-07T14:13:55.305+0000] {processor.py:157} INFO - Started process (PID=60) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:13:55.309+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:13:55.319+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:13:55.317+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:13:55.488+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:13:55.658+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:13:55.657+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:13:55.678+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:13:55.678+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:13:55.695+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.404 seconds
[2023-09-07T14:14:26.357+0000] {processor.py:157} INFO - Started process (PID=70) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:14:26.371+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:14:26.383+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:14:26.382+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:14:26.556+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:14:26.626+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:14:26.625+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:14:26.668+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:14:26.667+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:14:26.709+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.363 seconds
[2023-09-07T14:14:57.469+0000] {processor.py:157} INFO - Started process (PID=80) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:14:57.472+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:14:57.478+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:14:57.477+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:14:57.628+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:14:57.809+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:14:57.808+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:14:57.831+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:14:57.830+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:14:57.851+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.391 seconds
[2023-09-07T14:15:28.799+0000] {processor.py:157} INFO - Started process (PID=90) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:15:28.805+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:15:28.831+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:15:28.829+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:15:29.076+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:15:29.197+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:15:29.196+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:15:29.336+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:15:29.332+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:15:29.388+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.600 seconds
[2023-09-07T14:16:00.576+0000] {processor.py:157} INFO - Started process (PID=100) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:16:00.588+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:16:00.653+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:16:00.643+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:16:00.945+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:16:01.275+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:16:01.274+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:16:01.310+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:16:01.309+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:16:01.346+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.794 seconds
[2023-09-07T14:16:32.473+0000] {processor.py:157} INFO - Started process (PID=110) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:16:32.481+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:16:32.508+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:16:32.506+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:16:32.774+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:16:32.922+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:16:32.919+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:16:32.989+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:16:32.989+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:16:33.093+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.648 seconds
[2023-09-07T14:17:03.927+0000] {processor.py:157} INFO - Started process (PID=120) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:17:03.931+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:17:03.939+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:17:03.938+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:17:04.129+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:17:04.315+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:17:04.315+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:17:04.335+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:17:04.335+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:17:04.352+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.437 seconds
[2023-09-07T14:17:34.885+0000] {processor.py:157} INFO - Started process (PID=130) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:17:34.894+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:17:34.908+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:17:34.906+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:17:35.075+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:17:35.138+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:17:35.137+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:17:35.176+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:17:35.176+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:17:35.199+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.328 seconds
[2023-09-07T14:18:05.893+0000] {processor.py:157} INFO - Started process (PID=140) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:18:05.901+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:18:05.913+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:18:05.912+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:18:06.088+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:18:06.265+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:18:06.265+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:18:06.285+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:18:06.285+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:18:06.302+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.431 seconds
[2023-09-07T14:18:37.364+0000] {processor.py:157} INFO - Started process (PID=150) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:18:37.371+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:18:37.388+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:18:37.385+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:18:37.572+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:18:37.660+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:18:37.659+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:18:37.717+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:18:37.717+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:18:37.743+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.401 seconds
[2023-09-07T14:19:08.340+0000] {processor.py:157} INFO - Started process (PID=160) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:19:08.344+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:19:08.354+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:19:08.353+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:19:08.520+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:19:08.692+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:19:08.691+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:19:08.717+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:19:08.717+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:19:08.736+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.408 seconds
[2023-09-07T14:19:39.384+0000] {processor.py:157} INFO - Started process (PID=171) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:19:39.389+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:19:39.398+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:19:39.398+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:19:39.540+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:19:39.587+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:19:39.585+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:19:39.617+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:19:39.616+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:19:39.637+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.263 seconds
[2023-09-07T14:20:10.372+0000] {processor.py:157} INFO - Started process (PID=181) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:20:10.378+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:20:10.387+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:20:10.386+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:20:10.530+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:20:10.689+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:20:10.689+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:20:10.714+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:20:10.713+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:20:10.730+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.379 seconds
[2023-09-07T14:20:41.150+0000] {processor.py:157} INFO - Started process (PID=191) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:20:41.154+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:20:41.160+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:20:41.159+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:20:41.299+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:20:41.351+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:20:41.350+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:20:41.400+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:20:41.399+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:20:41.434+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.292 seconds
[2023-09-07T14:21:12.037+0000] {processor.py:157} INFO - Started process (PID=201) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:21:12.043+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:21:12.052+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:21:12.051+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:21:12.201+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:21:12.391+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:21:12.390+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:21:12.413+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:21:12.413+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:21:12.431+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.403 seconds
[2023-09-07T14:21:42.995+0000] {processor.py:157} INFO - Started process (PID=211) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:21:43.000+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:21:43.008+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:21:43.008+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:21:43.237+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:21:43.293+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:21:43.292+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:21:43.338+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:21:43.338+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:21:43.359+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.380 seconds
[2023-09-07T14:22:13.914+0000] {processor.py:157} INFO - Started process (PID=221) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:22:13.921+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:22:13.945+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:22:13.942+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:22:14.162+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:22:14.564+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:22:14.563+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:22:14.640+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:22:14.640+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:22:14.678+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.803 seconds
[2023-09-07T14:22:45.366+0000] {processor.py:157} INFO - Started process (PID=231) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:22:45.369+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:22:45.395+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:22:45.395+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:23:52.572+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:23:52.581+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:23:52.609+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:23:52.608+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:23:53.139+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:23:53.324+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:23:53.324+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:23:53.362+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:23:53.362+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:23:53.452+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.890 seconds
[2023-09-07T14:24:23.672+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:24:23.674+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:24:23.679+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:24:23.679+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:24:23.903+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:24:23.934+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:24:23.934+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:24:23.953+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:24:23.953+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:24:23.967+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.300 seconds
[2023-09-07T14:24:54.679+0000] {processor.py:157} INFO - Started process (PID=52) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:24:54.681+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:24:54.689+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:24:54.688+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:24:55.458+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:24:55.624+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:24:55.623+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:24:55.663+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:24:55.662+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:24:55.696+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.023 seconds
[2023-09-07T14:25:26.271+0000] {processor.py:157} INFO - Started process (PID=62) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:25:26.275+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:25:26.281+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:25:26.281+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:25:26.419+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:25:26.461+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:25:26.461+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:25:26.487+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:25:26.486+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:25:26.504+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.241 seconds
[2023-09-07T14:25:56.823+0000] {processor.py:157} INFO - Started process (PID=72) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:25:56.826+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:25:56.832+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:25:56.831+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:25:56.957+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:25:56.981+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:25:56.980+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:25:57.006+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:25:57.006+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:25:57.025+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.208 seconds
[2023-09-07T14:26:27.479+0000] {processor.py:157} INFO - Started process (PID=82) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:26:27.482+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:26:27.487+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:26:27.487+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:26:27.614+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:26:27.736+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:26:27.736+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:26:27.757+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:26:27.757+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:26:27.774+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.301 seconds
[2023-09-07T14:26:58.126+0000] {processor.py:157} INFO - Started process (PID=92) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:26:58.129+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:26:58.136+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:26:58.135+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:26:58.276+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:26:58.338+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:26:58.338+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:26:58.374+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:26:58.373+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:26:58.395+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.276 seconds
[2023-09-07T14:27:28.785+0000] {processor.py:157} INFO - Started process (PID=102) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:27:28.790+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:27:28.799+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:27:28.798+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:27:28.954+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:27:28.991+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:27:28.991+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:27:29.028+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:27:29.028+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:27:29.046+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.271 seconds
[2023-09-07T14:27:59.623+0000] {processor.py:157} INFO - Started process (PID=113) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:27:59.627+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:27:59.640+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:27:59.639+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:27:59.796+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:27:59.974+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:27:59.974+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:28:00.007+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:28:00.007+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:28:00.027+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.413 seconds
[2023-09-07T14:28:30.549+0000] {processor.py:157} INFO - Started process (PID=123) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:28:30.552+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:28:30.567+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:28:30.566+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:28:30.712+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:28:30.762+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:28:30.760+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:28:30.790+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:28:30.789+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:28:30.807+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.264 seconds
[2023-09-07T14:29:01.244+0000] {processor.py:157} INFO - Started process (PID=133) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:29:01.246+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:29:01.251+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:29:01.250+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:29:01.396+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:29:01.420+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:29:01.419+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:29:01.449+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:29:01.449+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:29:01.466+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.227 seconds
[2023-09-07T14:29:31.619+0000] {processor.py:157} INFO - Started process (PID=143) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:29:31.623+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:29:31.631+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:29:31.630+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:29:31.785+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:29:31.958+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:29:31.958+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:29:31.984+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:29:31.984+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:29:32.015+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.406 seconds
[2023-09-07T14:30:03.243+0000] {processor.py:157} INFO - Started process (PID=152) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:30:03.259+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:30:03.336+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:30:03.316+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:30:03.975+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:30:04.161+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:30:04.160+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:30:04.227+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:30:04.227+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:30:04.336+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.109 seconds
[2023-09-07T14:30:35.411+0000] {processor.py:157} INFO - Started process (PID=162) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:30:35.415+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:30:35.424+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:30:35.423+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:30:35.566+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:30:35.595+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:30:35.594+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:30:35.635+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:30:35.635+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:30:35.661+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.267 seconds
[2023-09-07T14:31:06.244+0000] {processor.py:157} INFO - Started process (PID=172) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:31:06.247+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:31:06.250+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:31:06.250+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:31:06.343+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:31:06.450+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:31:06.449+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:31:06.469+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:31:06.469+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:31:06.484+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.245 seconds
[2023-09-07T14:31:36.889+0000] {processor.py:157} INFO - Started process (PID=182) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:31:36.892+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:31:36.901+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:31:36.900+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:31:37.062+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:31:37.103+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:31:37.103+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:31:37.128+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:31:37.128+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:31:37.149+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.267 seconds
[2023-09-07T14:31:48.235+0000] {processor.py:157} INFO - Started process (PID=192) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:31:48.240+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:31:48.243+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:31:48.243+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:31:48.358+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:31:48.376+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:31:48.375+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:31:48.397+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:31:48.397+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:31:48.419+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.190 seconds
[2023-09-07T14:32:39.423+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:32:39.426+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:32:39.436+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:32:39.435+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:32:39.856+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:32:40.033+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:32:40.032+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:32:40.102+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:32:40.101+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:32:40.308+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:32:40.305+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"dagrun_timeout": 60.0, "default_args": {"__var": {"owner": "airflow", "depends_on_past": false, "start_date": {"__var": 169 ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 14, 32, 39, 897294, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T14:32:40.313+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:32:40.312+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:32:40.315+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"dagrun_timeout": 60.0, "default_args": {"__var": {"owner": "airflow", "depends_on_past": false, "start_date": {"__var": 169 ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 14, 32, 39, 897294, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T14:33:10.775+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:33:10.785+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:33:10.825+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:33:10.815+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:33:12.676+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:33:13.389+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:33:13.384+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:33:13.467+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:33:13.466+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:33:13.513+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.755 seconds
[2023-09-07T14:33:43.977+0000] {processor.py:157} INFO - Started process (PID=51) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:33:43.981+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:33:43.987+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:33:43.986+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:33:44.534+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:33:44.572+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:33:44.570+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:33:44.636+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:33:44.635+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:33:44.684+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.715 seconds
[2023-09-07T14:34:15.089+0000] {processor.py:157} INFO - Started process (PID=60) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:34:15.092+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:34:15.097+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:34:15.096+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:34:15.199+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:34:15.328+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:34:15.327+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:34:15.351+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:34:15.351+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:34:15.368+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.284 seconds
[2023-09-07T14:34:20.019+0000] {processor.py:157} INFO - Started process (PID=63) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:34:20.027+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:34:20.052+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:34:20.050+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:34:20.299+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:34:20.337+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:34:20.336+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:34:20.362+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:34:20.362+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:34:20.384+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.378 seconds
[2023-09-07T14:34:51.059+0000] {processor.py:157} INFO - Started process (PID=73) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:34:51.063+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:34:51.067+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:34:51.066+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:34:51.185+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:34:51.208+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:34:51.208+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:34:51.246+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:34:51.246+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:34:51.265+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.219 seconds
[2023-09-07T14:35:21.924+0000] {processor.py:157} INFO - Started process (PID=83) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:35:21.928+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:35:21.932+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:35:21.931+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:35:22.032+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:35:22.145+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:35:22.144+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:35:22.162+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:35:22.162+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:35:22.180+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.260 seconds
[2023-09-07T14:35:52.510+0000] {processor.py:157} INFO - Started process (PID=94) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:35:52.512+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:35:52.517+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:35:52.516+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:35:52.627+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:35:52.662+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:35:52.662+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:35:52.685+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:35:52.685+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:35:52.700+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.195 seconds
[2023-09-07T14:36:23.205+0000] {processor.py:157} INFO - Started process (PID=104) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:36:23.211+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:36:23.222+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:36:23.221+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:36:23.440+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:36:23.501+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:36:23.500+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:36:23.545+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:36:23.545+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:36:23.572+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.377 seconds
[2023-09-07T14:36:54.002+0000] {processor.py:157} INFO - Started process (PID=115) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:36:54.005+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:36:54.010+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:36:54.009+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:36:54.124+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:36:54.256+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:36:54.256+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:36:54.279+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:36:54.279+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:36:54.298+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.303 seconds
[2023-09-07T14:37:24.890+0000] {processor.py:157} INFO - Started process (PID=125) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:37:24.894+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:37:24.899+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:37:24.898+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:37:24.998+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:37:25.030+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:37:25.030+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:37:25.051+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:37:25.051+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:37:25.067+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.182 seconds
[2023-09-07T14:37:55.480+0000] {processor.py:157} INFO - Started process (PID=135) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:37:55.483+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:37:55.487+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:37:55.487+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:37:55.586+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:37:55.604+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:37:55.604+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:37:55.625+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:37:55.625+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:37:55.640+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.167 seconds
[2023-09-07T14:38:25.713+0000] {processor.py:157} INFO - Started process (PID=145) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:38:25.716+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:38:25.721+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:38:25.721+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:38:25.851+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:38:25.971+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:38:25.970+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:38:25.991+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:38:25.991+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:38:26.009+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.303 seconds
[2023-09-07T14:38:56.308+0000] {processor.py:157} INFO - Started process (PID=155) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:38:56.311+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:38:56.315+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:38:56.314+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:38:56.438+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:38:56.479+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:38:56.478+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:38:56.509+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:38:56.509+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:38:56.529+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.226 seconds
[2023-09-07T14:39:27.104+0000] {processor.py:157} INFO - Started process (PID=164) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:39:27.107+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:39:27.110+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:39:27.110+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:39:27.208+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:39:27.229+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:39:27.228+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:39:27.256+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:39:27.256+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:39:27.281+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.182 seconds
[2023-09-07T14:39:57.943+0000] {processor.py:157} INFO - Started process (PID=174) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:39:57.946+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:39:57.950+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:39:57.949+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:39:58.059+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:39:58.175+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:39:58.174+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:39:58.193+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:39:58.193+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:39:58.210+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.271 seconds
[2023-09-07T14:40:28.608+0000] {processor.py:157} INFO - Started process (PID=184) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:40:28.612+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:40:28.616+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:40:28.615+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:40:28.710+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:40:28.742+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:40:28.742+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:40:28.765+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:40:28.765+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:40:28.781+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.178 seconds
[2023-09-07T14:49:15.588+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:49:15.611+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:49:15.664+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:49:15.656+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:49:18.701+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:49:19.177+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:49:19.155+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:49:19.415+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:49:19.414+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:49:19.729+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:49:19.715+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"fileloc": "/opt/airflow/dags/crawl_data_log_into_postgres.py", "dataset_triggers": [], "_task_group": {"_group_id": null, " ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 14, 49, 18, 790487, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T14:49:19.736+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:49:19.736+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:49:19.743+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"fileloc": "/opt/airflow/dags/crawl_data_log_into_postgres.py", "dataset_triggers": [], "_task_group": {"_group_id": null, " ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 14, 49, 18, 790487, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T14:49:50.567+0000] {processor.py:157} INFO - Started process (PID=43) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:49:50.571+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:49:50.576+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:49:50.575+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:49:50.850+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:49:51.005+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:49:51.004+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:49:51.044+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:49:51.044+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:49:51.075+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.514 seconds
[2023-09-07T14:50:21.580+0000] {processor.py:157} INFO - Started process (PID=53) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:50:21.589+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:50:21.598+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:50:21.597+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:50:22.314+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:50:22.358+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:50:22.358+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:50:22.382+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:50:22.382+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:50:22.417+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.856 seconds
[2023-09-07T14:50:52.619+0000] {processor.py:157} INFO - Started process (PID=62) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:50:52.624+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T14:50:52.632+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:50:52.631+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:50:52.825+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T14:50:53.099+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:50:53.098+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:50:53.173+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:50:53.172+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:50:53.217+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.610 seconds
[2023-09-07T15:07:06.789+0000] {processor.py:157} INFO - Started process (PID=29) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:07:06.795+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T15:07:06.814+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:07:06.813+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:07:07.355+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:07:07.624+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:07:07.622+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T15:07:07.670+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:07:07.670+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T15:07:07.708+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.935 seconds
[2023-09-07T15:07:37.896+0000] {processor.py:157} INFO - Started process (PID=38) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:07:37.900+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T15:07:37.907+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:07:37.906+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:07:38.311+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:07:38.362+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:07:38.361+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T15:07:38.388+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:07:38.388+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T15:07:38.409+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.521 seconds
[2023-09-07T15:08:09.283+0000] {processor.py:157} INFO - Started process (PID=48) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:08:09.286+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T15:08:09.292+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:08:09.292+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:08:09.765+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:08:09.928+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:08:09.928+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T15:08:09.953+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:08:09.952+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T15:08:09.973+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.697 seconds
[2023-09-07T15:08:40.332+0000] {processor.py:157} INFO - Started process (PID=58) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:08:40.337+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T15:08:40.345+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:08:40.345+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:08:40.515+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:08:40.567+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:08:40.567+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T15:08:40.612+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:08:40.612+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T15:08:40.639+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.320 seconds
[2023-09-07T15:11:48.764+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:11:48.773+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T15:11:48.795+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:11:48.793+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:11:49.258+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:11:49.468+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:11:49.466+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T15:11:49.601+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:11:49.601+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T15:11:50.097+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:11:50.085+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"_dag_id": "website_crawler", "edge_info": {}, "dataset_triggers": [], "timezone": "UTC", "schedule_interval": "@daily", "de ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 15, 11, 49, 311378, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T15:11:50.103+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:11:50.102+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T15:11:50.109+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"_dag_id": "website_crawler", "edge_info": {}, "dataset_triggers": [], "timezone": "UTC", "schedule_interval": "@daily", "de ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 15, 11, 49, 311378, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T15:12:20.400+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:12:20.404+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T15:12:20.410+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:12:20.409+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:12:20.717+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:12:20.854+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:12:20.854+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T15:12:20.875+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:12:20.875+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T15:12:20.893+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.499 seconds
[2023-09-07T15:12:51.559+0000] {processor.py:157} INFO - Started process (PID=52) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:12:51.565+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T15:12:51.577+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:12:51.575+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:12:52.787+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:12:52.919+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:12:52.908+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T15:12:52.976+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:12:52.974+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T15:12:53.022+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.483 seconds
[2023-09-07T15:13:23.448+0000] {processor.py:157} INFO - Started process (PID=63) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:13:23.457+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T15:13:23.467+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:13:23.466+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:13:23.647+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:13:24.124+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:13:24.122+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T15:13:24.160+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:13:24.160+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T15:13:24.211+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.794 seconds
[2023-09-07T15:13:54.902+0000] {processor.py:157} INFO - Started process (PID=73) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:13:54.908+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T15:13:54.919+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:13:54.917+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:13:55.074+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T15:13:55.141+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:13:55.141+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T15:13:55.179+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:13:55.179+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T15:13:55.210+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.320 seconds
[2023-09-07T16:53:52.897+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T16:53:52.901+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T16:53:52.914+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:53:52.913+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T16:53:53.728+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T16:53:54.019+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:53:54.018+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T16:53:54.055+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:53:54.054+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T16:53:54.113+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.225 seconds
[2023-09-07T16:54:24.831+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T16:54:24.871+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T16:54:24.966+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:54:24.961+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T16:54:26.918+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T16:54:27.392+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:54:27.380+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T16:54:27.483+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:54:27.482+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T16:54:27.514+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.774 seconds
[2023-09-07T16:55:49.994+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T16:55:50.000+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T16:55:50.018+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:55:50.015+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T16:55:50.522+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T16:55:50.945+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:55:50.944+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T16:55:51.001+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:55:51.001+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T16:55:51.032+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.050 seconds
[2023-09-07T16:56:21.285+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T16:56:21.290+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T16:56:21.301+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:56:21.299+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T16:56:22.035+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T16:56:22.087+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:56:22.086+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T16:56:22.125+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:56:22.125+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T16:56:22.145+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.872 seconds
[2023-09-07T17:02:18.855+0000] {processor.py:157} INFO - Started process (PID=29) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:02:18.861+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T17:02:18.875+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:02:18.872+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:02:19.346+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:02:19.587+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:02:19.586+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:02:19.629+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:02:19.628+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:02:19.667+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.822 seconds
[2023-09-07T17:02:20.170+0000] {processor.py:157} INFO - Started process (PID=32) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:02:20.174+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T17:02:20.183+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:02:20.182+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:02:20.735+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:02:20.780+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:02:20.779+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:02:20.823+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:02:20.823+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:02:20.850+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.713 seconds
[2023-09-07T17:02:50.974+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:02:50.977+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T17:02:50.990+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:02:50.989+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:02:51.416+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:02:51.449+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:02:51.448+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:02:51.480+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:02:51.480+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:02:51.506+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.538 seconds
[2023-09-07T17:03:21.963+0000] {processor.py:157} INFO - Started process (PID=51) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:03:21.966+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T17:03:21.969+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:03:21.969+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:03:22.298+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:03:22.539+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:03:22.538+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:03:22.556+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:03:22.556+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:03:22.584+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.626 seconds
[2023-09-07T17:03:52.879+0000] {processor.py:157} INFO - Started process (PID=60) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:03:52.905+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T17:03:52.995+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:03:52.991+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:29:12.070+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:29:12.093+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T17:29:12.132+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:29:12.130+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:29:13.305+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:29:13.780+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:29:13.775+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:29:14.048+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:29:14.048+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:29:14.163+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:29:14.139+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"default_args": {"__var": {"owner": "airflow", "depends_on_past": false, "start_date": {"__var": 1693180800.0, "__type": "da ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 17, 29, 13, 395198, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T17:29:14.196+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:29:14.195+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:29:14.203+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"default_args": {"__var": {"owner": "airflow", "depends_on_past": false, "start_date": {"__var": 1693180800.0, "__type": "da ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 17, 29, 13, 395198, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T17:29:45.034+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:29:45.037+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T17:29:45.043+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:29:45.042+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:29:45.464+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:29:45.585+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:29:45.584+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:29:45.604+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:29:45.604+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:29:45.624+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.596 seconds
[2023-09-07T17:30:16.213+0000] {processor.py:157} INFO - Started process (PID=51) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:30:16.217+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T17:30:16.229+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:30:16.227+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:30:16.852+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:30:16.903+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:30:16.903+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:30:16.932+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:30:16.931+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:30:16.951+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.752 seconds
[2023-09-07T17:30:47.138+0000] {processor.py:157} INFO - Started process (PID=61) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:30:47.143+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T17:30:47.161+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:30:47.159+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:30:47.297+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:30:47.341+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:30:47.340+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:30:47.369+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:30:47.369+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:30:47.390+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.263 seconds
[2023-09-07T17:32:06.244+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:32:06.248+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T17:32:06.255+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:32:06.254+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:32:06.675+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:32:06.931+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:32:06.930+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:32:07.020+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:32:07.019+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:32:07.549+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:32:07.539+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"edge_info": {}, "dagrun_timeout": 60.0, "_dag_id": "website_crawler", "timezone": "UTC", "schedule_interval": "@daily", "de ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 17, 32, 6, 717122, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T17:32:07.557+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:32:07.557+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:32:07.561+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"edge_info": {}, "dagrun_timeout": 60.0, "_dag_id": "website_crawler", "timezone": "UTC", "schedule_interval": "@daily", "de ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 17, 32, 6, 717122, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T17:32:37.993+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:32:38.001+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T17:32:38.007+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:32:38.006+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:32:38.329+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:32:38.481+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:32:38.480+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:32:38.503+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:32:38.503+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:32:38.521+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.539 seconds
[2023-09-07T17:33:08.927+0000] {processor.py:157} INFO - Started process (PID=50) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:33:08.932+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T17:33:08.943+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:33:08.941+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:33:09.787+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:33:09.850+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:33:09.849+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:33:09.878+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:33:09.878+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:33:09.924+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.008 seconds
[2023-09-07T17:33:40.576+0000] {processor.py:157} INFO - Started process (PID=60) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:33:40.579+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T17:33:40.584+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:33:40.583+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:33:40.696+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:33:40.816+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:33:40.815+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:33:40.835+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:33:40.835+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:33:40.854+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.283 seconds
[2023-09-07T17:34:11.392+0000] {processor.py:157} INFO - Started process (PID=70) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:34:11.397+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-07T17:34:11.407+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:34:11.406+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:34:11.588+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-07T17:34:11.649+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:34:11.648+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:34:11.691+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:34:11.690+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:34:11.712+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.347 seconds
