[2023-09-07T13:07:15.811+0000] {processor.py:157} INFO - Started process (PID=32) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:07:15.818+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:07:15.832+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:07:15.827+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:07:16.492+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:07:16.943+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:07:16.942+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:07:16.984+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:07:16.984+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-02T00:00:00+00:00, run_after=2023-09-03T00:00:00+00:00
[2023-09-07T13:07:17.013+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.211 seconds
[2023-09-07T13:07:49.475+0000] {processor.py:157} INFO - Started process (PID=45) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:07:49.479+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:07:49.485+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:07:49.484+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:07:49.659+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:07:49.726+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:07:49.724+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:07:49.775+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:07:49.774+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:07:49.810+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.343 seconds
[2023-09-07T13:08:20.518+0000] {processor.py:157} INFO - Started process (PID=55) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:08:20.522+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:08:20.530+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:08:20.529+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:08:20.679+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:08:20.885+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:08:20.884+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:08:20.910+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:08:20.910+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:08:20.928+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.426 seconds
[2023-09-07T13:08:51.532+0000] {processor.py:157} INFO - Started process (PID=65) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:08:51.535+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:08:51.541+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:08:51.540+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:08:51.775+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:08:51.892+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:08:51.891+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:08:51.941+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:08:51.941+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:08:51.987+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.462 seconds
[2023-09-07T13:09:22.635+0000] {processor.py:157} INFO - Started process (PID=75) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:09:22.641+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:09:22.653+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:09:22.650+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:09:22.852+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:09:23.083+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:09:23.082+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:09:23.132+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:09:23.132+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:09:23.165+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.541 seconds
[2023-09-07T13:09:53.563+0000] {processor.py:157} INFO - Started process (PID=85) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:09:53.565+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:09:53.569+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:09:53.569+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:09:53.700+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:09:53.744+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:09:53.744+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:09:53.778+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:09:53.778+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:09:53.809+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.251 seconds
[2023-09-07T13:10:24.458+0000] {processor.py:157} INFO - Started process (PID=94) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:10:24.465+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:10:24.475+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:10:24.474+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:10:24.933+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:10:25.213+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:10:25.212+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:10:25.260+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:10:25.259+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:10:25.303+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.864 seconds
[2023-09-07T13:10:55.800+0000] {processor.py:157} INFO - Started process (PID=104) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:10:55.819+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:10:55.844+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:10:55.836+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:10:56.207+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:10:56.371+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:10:56.370+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:10:56.432+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:10:56.432+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:10:56.512+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.743 seconds
[2023-09-07T13:11:27.254+0000] {processor.py:157} INFO - Started process (PID=114) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:11:27.260+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:11:27.270+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:11:27.269+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:11:27.427+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:11:27.628+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:11:27.627+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:11:27.658+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:11:27.658+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:11:27.682+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.433 seconds
[2023-09-07T13:11:58.118+0000] {processor.py:157} INFO - Started process (PID=124) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:11:58.129+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:11:58.137+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:11:58.136+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:11:58.279+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:11:58.348+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:11:58.347+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:11:58.386+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:11:58.386+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:11:58.411+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.314 seconds
[2023-09-07T13:12:29.210+0000] {processor.py:157} INFO - Started process (PID=133) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:12:29.217+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:12:29.231+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:12:29.229+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:12:29.454+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:12:29.640+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:12:29.640+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:12:29.666+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:12:29.666+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:12:29.686+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.489 seconds
[2023-09-07T13:13:00.109+0000] {processor.py:157} INFO - Started process (PID=144) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:13:00.111+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:13:00.117+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:13:00.116+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:13:00.253+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:13:00.304+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:13:00.304+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:13:00.337+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:13:00.336+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:13:00.357+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.253 seconds
[2023-09-07T13:13:30.861+0000] {processor.py:157} INFO - Started process (PID=153) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:13:30.866+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:13:30.873+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:13:30.873+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:13:30.998+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:13:31.163+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:13:31.162+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:13:31.188+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:13:31.188+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:13:31.206+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.352 seconds
[2023-09-07T13:14:01.624+0000] {processor.py:157} INFO - Started process (PID=162) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:14:01.628+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:14:01.636+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:14:01.635+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:14:01.769+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:14:01.841+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:14:01.841+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:14:01.880+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:14:01.879+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:14:01.902+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.289 seconds
[2023-09-07T13:14:32.476+0000] {processor.py:157} INFO - Started process (PID=172) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:14:32.480+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:14:32.488+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:14:32.487+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:14:32.644+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:14:32.850+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:14:32.849+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:14:32.874+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:14:32.874+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:14:32.892+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.427 seconds
[2023-09-07T13:15:03.349+0000] {processor.py:157} INFO - Started process (PID=189) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:15:03.354+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:15:03.393+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:15:03.390+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:15:03.651+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:15:03.756+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:15:03.754+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:15:03.812+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:15:03.811+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:15:03.846+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.505 seconds
[2023-09-07T13:15:34.400+0000] {processor.py:157} INFO - Started process (PID=199) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:15:34.406+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:15:34.414+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:15:34.413+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:15:34.550+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:15:34.709+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:15:34.708+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:15:34.731+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:15:34.731+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:15:34.749+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.368 seconds
[2023-09-07T13:16:05.224+0000] {processor.py:157} INFO - Started process (PID=209) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:16:05.228+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:16:05.235+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:16:05.234+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:16:05.377+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:16:05.421+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:16:05.420+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:16:05.458+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:16:05.458+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:16:05.481+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.263 seconds
[2023-09-07T13:16:36.386+0000] {processor.py:157} INFO - Started process (PID=220) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:16:36.390+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:16:36.405+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:16:36.402+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:16:36.585+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:16:36.836+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:16:36.836+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:16:36.864+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:16:36.864+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:16:36.890+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.515 seconds
[2023-09-07T13:17:07.860+0000] {processor.py:157} INFO - Started process (PID=229) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:17:07.868+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:17:07.904+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:17:07.900+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:17:08.192+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:17:08.373+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:17:08.372+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:17:08.451+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:17:08.450+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:17:08.514+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.674 seconds
[2023-09-07T13:17:39.237+0000] {processor.py:157} INFO - Started process (PID=240) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:17:39.258+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:17:39.339+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:17:39.324+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:17:40.001+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:17:40.571+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:17:40.567+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:17:40.613+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:17:40.613+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:17:40.682+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.484 seconds
[2023-09-07T13:18:11.404+0000] {processor.py:157} INFO - Started process (PID=250) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:18:11.406+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:18:11.411+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:18:11.411+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:18:11.514+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:18:11.552+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:18:11.550+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:18:11.578+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:18:11.577+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:18:11.595+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.196 seconds
[2023-09-07T13:18:42.222+0000] {processor.py:157} INFO - Started process (PID=260) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:18:42.228+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:18:42.236+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:18:42.235+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:18:42.532+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:18:43.029+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:18:43.028+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:18:43.060+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:18:43.060+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:18:43.094+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.889 seconds
[2023-09-07T13:19:14.338+0000] {processor.py:157} INFO - Started process (PID=270) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:19:14.355+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:19:14.414+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:19:14.411+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:19:15.499+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:19:16.294+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:19:16.278+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:19:16.619+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:19:16.610+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:19:16.813+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 2.610 seconds
[2023-09-07T13:31:22.993+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:31:23.002+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:31:23.023+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:31:23.022+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:31:24.319+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:31:26.050+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:31:26.042+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:31:26.495+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:31:26.494+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:31:26.627+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 3.660 seconds
[2023-09-07T13:31:57.983+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:31:57.990+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:31:57.997+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:31:57.996+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:31:58.575+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:31:58.633+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:31:58.631+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:31:58.749+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:31:58.747+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:31:58.823+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.847 seconds
[2023-09-07T13:32:29.393+0000] {processor.py:157} INFO - Started process (PID=52) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:32:29.399+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:32:29.407+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:32:29.406+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:32:30.241+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:32:30.486+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:32:30.486+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:32:30.514+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:32:30.514+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:32:30.536+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.150 seconds
[2023-09-07T13:33:00.932+0000] {processor.py:157} INFO - Started process (PID=62) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:33:00.935+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:33:00.941+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:33:00.940+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:33:01.374+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:33:01.418+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:33:01.418+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:33:01.456+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:33:01.456+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:33:01.486+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.568 seconds
[2023-09-07T13:33:32.504+0000] {processor.py:157} INFO - Started process (PID=77) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:33:32.508+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T13:33:32.515+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:33:32.514+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:33:32.931+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T13:33:33.966+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:33:33.958+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T13:33:34.241+0000] {logging_mixin.py:151} INFO - [2023-09-07T13:33:34.241+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T13:33:34.428+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.930 seconds
[2023-09-07T14:06:49.478+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:06:49.489+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:06:49.500+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:06:49.499+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:06:49.943+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:06:50.181+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:06:50.181+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:06:50.243+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:06:50.243+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:06:50.352+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.890 seconds
[2023-09-07T14:07:20.529+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:07:20.532+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:07:20.536+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:07:20.536+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:07:20.736+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:07:20.767+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:07:20.767+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:07:20.787+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:07:20.787+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:07:20.800+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.275 seconds
[2023-09-07T14:07:50.995+0000] {processor.py:157} INFO - Started process (PID=51) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:07:50.998+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:07:51.003+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:07:51.002+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:07:51.544+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:07:51.601+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:07:51.600+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:07:51.636+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:07:51.636+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:07:51.651+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.665 seconds
[2023-09-07T14:08:22.355+0000] {processor.py:157} INFO - Started process (PID=61) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:08:22.364+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:08:22.379+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:08:22.378+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:08:22.516+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:08:22.570+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:08:22.569+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:08:22.598+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:08:22.598+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:08:22.615+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.267 seconds
[2023-09-07T14:08:53.037+0000] {processor.py:157} INFO - Started process (PID=71) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:08:53.040+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:08:53.044+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:08:53.044+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:08:53.160+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:08:53.216+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:08:53.215+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:08:53.244+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:08:53.244+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:08:53.260+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.229 seconds
[2023-09-07T14:09:23.678+0000] {processor.py:157} INFO - Started process (PID=81) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:09:23.681+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:09:23.685+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:09:23.684+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:09:23.792+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:09:23.831+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:09:23.830+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:09:23.856+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:09:23.856+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:09:23.872+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.199 seconds
[2023-09-07T14:12:21.730+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:12:21.740+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:12:21.763+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:12:21.762+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:12:22.212+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:12:22.534+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:12:22.533+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:12:22.569+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:12:22.569+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:12:22.603+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.880 seconds
[2023-09-07T14:12:52.919+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:12:52.922+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:12:52.928+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:12:52.927+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:12:53.184+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:12:53.219+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:12:53.219+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:12:53.239+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:12:53.238+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:12:53.255+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.342 seconds
[2023-09-07T14:13:24.185+0000] {processor.py:157} INFO - Started process (PID=51) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:13:24.191+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:13:24.200+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:13:24.199+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:13:24.832+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:13:25.015+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:13:25.015+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:13:25.040+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:13:25.040+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:13:25.077+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.902 seconds
[2023-09-07T14:13:55.334+0000] {processor.py:157} INFO - Started process (PID=61) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:13:55.339+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:13:55.347+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:13:55.347+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:13:55.491+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:13:55.538+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:13:55.538+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:13:55.576+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:13:55.576+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:13:55.598+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.277 seconds
[2023-09-07T14:14:26.400+0000] {processor.py:157} INFO - Started process (PID=71) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:14:26.404+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:14:26.411+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:14:26.410+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:14:26.548+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:14:26.785+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:14:26.784+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:14:26.813+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:14:26.813+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:14:26.831+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.441 seconds
[2023-09-07T14:14:57.478+0000] {processor.py:157} INFO - Started process (PID=81) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:14:57.481+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:14:57.486+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:14:57.485+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:14:57.622+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:14:57.675+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:14:57.675+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:14:57.717+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:14:57.717+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:14:57.758+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.284 seconds
[2023-09-07T14:15:28.833+0000] {processor.py:157} INFO - Started process (PID=91) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:15:28.839+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:15:28.847+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:15:28.846+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:15:29.049+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:15:29.447+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:15:29.444+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:15:29.546+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:15:29.545+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:15:29.650+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.825 seconds
[2023-09-07T14:16:00.590+0000] {processor.py:157} INFO - Started process (PID=101) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:16:00.612+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:16:00.653+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:16:00.648+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:16:00.933+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:16:01.065+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:16:01.064+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:16:01.148+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:16:01.148+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:16:01.198+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.641 seconds
[2023-09-07T14:16:32.506+0000] {processor.py:157} INFO - Started process (PID=111) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:16:32.512+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:16:32.523+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:16:32.522+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:16:32.759+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:16:33.228+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:16:33.221+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:16:33.304+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:16:33.304+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:16:33.355+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.875 seconds
[2023-09-07T14:17:03.942+0000] {processor.py:157} INFO - Started process (PID=121) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:17:03.947+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:17:03.957+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:17:03.956+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:17:04.113+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:17:04.185+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:17:04.185+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:17:04.215+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:17:04.215+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:17:04.235+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.307 seconds
[2023-09-07T14:17:34.908+0000] {processor.py:157} INFO - Started process (PID=131) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:17:34.914+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:17:34.923+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:17:34.922+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:17:35.065+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:17:35.264+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:17:35.263+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:17:35.294+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:17:35.294+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:17:35.313+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.418 seconds
[2023-09-07T14:18:05.929+0000] {processor.py:157} INFO - Started process (PID=141) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:18:05.935+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:18:05.943+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:18:05.943+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:18:06.079+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:18:06.136+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:18:06.136+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:18:06.180+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:18:06.179+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:18:06.202+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.293 seconds
[2023-09-07T14:18:37.381+0000] {processor.py:157} INFO - Started process (PID=151) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:18:37.388+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:18:37.400+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:18:37.399+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:18:37.565+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:18:37.831+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:18:37.830+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:18:37.870+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:18:37.869+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:18:37.896+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.530 seconds
[2023-09-07T14:19:08.348+0000] {processor.py:157} INFO - Started process (PID=161) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:19:08.352+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:19:08.359+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:19:08.358+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:19:08.507+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:19:08.577+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:19:08.576+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:19:08.610+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:19:08.609+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:19:08.634+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.293 seconds
[2023-09-07T14:19:39.394+0000] {processor.py:157} INFO - Started process (PID=172) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:19:39.397+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:19:39.402+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:19:39.402+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:19:39.532+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:19:39.702+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:19:39.701+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:19:39.722+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:19:39.722+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:19:39.738+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.351 seconds
[2023-09-07T14:20:10.380+0000] {processor.py:157} INFO - Started process (PID=182) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:20:10.384+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:20:10.391+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:20:10.390+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:20:10.522+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:20:10.582+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:20:10.581+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:20:10.612+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:20:10.612+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:20:10.636+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.264 seconds
[2023-09-07T14:20:41.164+0000] {processor.py:157} INFO - Started process (PID=192) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:20:41.167+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:20:41.173+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:20:41.173+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:20:41.293+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:20:41.568+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:20:41.566+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:20:41.614+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:20:41.614+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:20:41.664+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.506 seconds
[2023-09-07T14:21:12.053+0000] {processor.py:157} INFO - Started process (PID=202) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:21:12.057+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:21:12.064+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:21:12.064+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:21:12.195+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:21:12.245+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:21:12.245+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:21:12.304+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:21:12.304+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:21:12.325+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.281 seconds
[2023-09-07T14:21:43.002+0000] {processor.py:157} INFO - Started process (PID=212) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:21:43.005+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:21:43.012+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:21:43.012+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:21:43.229+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:21:43.416+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:21:43.415+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:21:43.443+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:21:43.443+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:21:43.461+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.465 seconds
[2023-09-07T14:22:13.949+0000] {processor.py:157} INFO - Started process (PID=222) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:22:13.958+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:22:13.970+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:22:13.969+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:22:14.147+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:22:14.239+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:22:14.238+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:22:14.299+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:22:14.299+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:22:14.333+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.407 seconds
[2023-09-07T14:22:45.465+0000] {processor.py:157} INFO - Started process (PID=232) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:22:45.477+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:22:45.495+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:22:45.494+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:22:50.440+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:22:50.439+0000] {process_utils.py:259} INFO - Waiting up to 5 seconds for processes to exit...
[2023-09-07T14:23:52.606+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:23:52.620+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:23:52.648+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:23:52.648+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:23:53.124+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:23:53.325+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:23:53.324+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:23:53.436+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:23:53.427+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:23:53.944+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:23:53.935+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"_task_group": {"_group_id": null, "prefix_group_id": true, "tooltip": "", "ui_color": "CornflowerBlue", "ui_fgcolor": "#000 ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 14, 23, 53, 177877, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T14:23:53.951+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:23:53.950+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:23:53.959+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"_task_group": {"_group_id": null, "prefix_group_id": true, "tooltip": "", "ui_color": "CornflowerBlue", "ui_fgcolor": "#000 ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 14, 23, 53, 177877, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T14:24:24.702+0000] {processor.py:157} INFO - Started process (PID=42) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:24:24.706+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:24:24.710+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:24:24.710+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:24:24.952+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:24:25.057+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:24:25.056+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:24:25.073+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:24:25.073+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:24:25.088+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.394 seconds
[2023-09-07T14:24:55.718+0000] {processor.py:157} INFO - Started process (PID=53) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:24:55.725+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:24:55.736+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:24:55.735+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:24:56.980+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:24:57.189+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:24:57.182+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:24:57.388+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:24:57.387+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:24:57.548+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.837 seconds
[2023-09-07T14:25:28.318+0000] {processor.py:157} INFO - Started process (PID=63) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:25:28.321+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:25:28.328+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:25:28.327+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:25:28.431+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:25:28.564+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:25:28.563+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:25:28.582+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:25:28.582+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:25:28.601+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.289 seconds
[2023-09-07T14:25:58.887+0000] {processor.py:157} INFO - Started process (PID=73) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:25:58.892+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:25:58.897+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:25:58.896+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:25:59.031+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:25:59.078+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:25:59.077+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:25:59.106+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:25:59.105+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:25:59.120+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.241 seconds
[2023-09-07T14:26:29.558+0000] {processor.py:157} INFO - Started process (PID=83) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:26:29.561+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:26:29.566+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:26:29.565+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:26:29.675+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:26:29.698+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:26:29.698+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:26:29.727+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:26:29.727+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:26:29.741+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.189 seconds
[2023-09-07T14:27:00.188+0000] {processor.py:157} INFO - Started process (PID=93) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:27:00.192+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:27:00.196+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:27:00.196+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:27:00.299+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:27:00.410+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:27:00.410+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:27:00.427+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:27:00.427+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:27:00.444+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.262 seconds
[2023-09-07T14:27:30.833+0000] {processor.py:157} INFO - Started process (PID=103) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:27:30.837+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:27:30.844+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:27:30.844+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:27:31.006+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:27:31.108+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:27:31.107+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:27:31.184+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:27:31.184+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:27:31.216+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.398 seconds
[2023-09-07T14:28:01.777+0000] {processor.py:157} INFO - Started process (PID=114) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:28:01.784+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:28:01.825+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:28:01.822+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:28:02.063+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:28:02.096+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:28:02.096+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:28:02.141+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:28:02.141+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:28:02.167+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.409 seconds
[2023-09-07T14:28:32.610+0000] {processor.py:157} INFO - Started process (PID=124) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:28:32.613+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:28:32.618+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:28:32.617+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:28:32.720+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:28:32.848+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:28:32.848+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:28:32.867+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:28:32.867+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:28:32.883+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.278 seconds
[2023-09-07T14:29:03.565+0000] {processor.py:157} INFO - Started process (PID=134) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:29:03.576+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:29:03.596+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:29:03.592+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:29:04.033+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:29:04.161+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:29:04.152+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:29:04.284+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:29:04.283+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:29:04.321+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.809 seconds
[2023-09-07T14:29:34.775+0000] {processor.py:157} INFO - Started process (PID=144) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:29:34.784+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:29:34.800+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:29:34.798+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:29:35.351+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:29:35.433+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:29:35.432+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:29:35.504+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:29:35.503+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:29:35.546+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.795 seconds
[2023-09-07T14:30:06.349+0000] {processor.py:157} INFO - Started process (PID=153) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:30:06.356+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:30:06.370+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:30:06.369+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:30:06.572+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:30:06.774+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:30:06.773+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:30:06.798+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:30:06.798+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:30:06.820+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.488 seconds
[2023-09-07T14:30:36.915+0000] {processor.py:157} INFO - Started process (PID=164) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:30:36.918+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:30:36.922+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:30:36.922+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:30:37.013+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:30:37.046+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:30:37.046+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:30:37.067+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:30:37.067+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:30:37.085+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.175 seconds
[2023-09-07T14:31:07.385+0000] {processor.py:157} INFO - Started process (PID=173) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:31:07.387+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:31:07.391+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:31:07.391+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:31:07.484+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:31:07.504+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:31:07.504+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:31:07.529+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:31:07.529+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:31:07.543+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.165 seconds
[2023-09-07T14:31:37.934+0000] {processor.py:157} INFO - Started process (PID=183) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:31:37.939+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:31:37.944+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:31:37.943+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:31:38.046+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:31:38.165+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:31:38.165+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:31:38.187+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:31:38.187+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:31:38.206+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.277 seconds
[2023-09-07T14:32:39.463+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:32:39.468+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:32:39.480+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:32:39.479+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:32:39.860+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:32:40.031+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:32:40.031+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:32:40.062+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:32:40.062+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:32:40.087+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.636 seconds
[2023-09-07T14:33:10.825+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:33:10.833+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:33:10.844+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:33:10.843+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:33:12.670+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:33:12.864+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:33:12.856+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:33:12.920+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:33:12.920+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:33:12.998+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 2.180 seconds
[2023-09-07T14:33:43.350+0000] {processor.py:157} INFO - Started process (PID=50) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:33:43.354+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:33:43.361+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:33:43.360+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:33:44.049+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:33:44.328+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:33:44.327+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:33:44.392+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:33:44.392+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:33:44.439+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.102 seconds
[2023-09-07T14:34:15.105+0000] {processor.py:157} INFO - Started process (PID=61) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:34:15.108+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:34:15.112+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:34:15.111+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:34:15.202+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:34:15.234+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:34:15.233+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:34:15.256+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:34:15.256+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:34:15.273+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.174 seconds
[2023-09-07T14:34:45.740+0000] {processor.py:157} INFO - Started process (PID=71) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:34:45.745+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:34:45.751+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:34:45.750+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:34:45.904+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:34:46.040+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:34:46.039+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:34:46.059+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:34:46.059+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:34:46.075+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.351 seconds
[2023-09-07T14:35:16.512+0000] {processor.py:157} INFO - Started process (PID=81) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:35:16.519+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:35:16.531+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:35:16.530+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:35:16.721+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:35:16.794+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:35:16.793+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:35:16.830+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:35:16.830+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:35:16.845+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.349 seconds
[2023-09-07T14:35:47.250+0000] {processor.py:157} INFO - Started process (PID=92) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:35:47.256+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:35:47.264+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:35:47.263+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:35:47.391+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:35:47.412+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:35:47.412+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:35:47.437+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:35:47.437+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:35:47.455+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.214 seconds
[2023-09-07T14:36:17.766+0000] {processor.py:157} INFO - Started process (PID=102) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:36:17.771+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:36:17.791+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:36:17.790+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:36:17.962+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:36:18.172+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:36:18.171+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:36:18.192+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:36:18.192+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:36:18.211+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.460 seconds
[2023-09-07T14:36:48.584+0000] {processor.py:157} INFO - Started process (PID=113) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:36:48.591+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:36:48.612+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:36:48.609+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:36:48.904+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:36:48.941+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:36:48.940+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:36:48.974+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:36:48.974+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:36:48.996+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.422 seconds
[2023-09-07T14:37:19.479+0000] {processor.py:157} INFO - Started process (PID=123) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:37:19.485+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:37:19.496+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:37:19.494+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:37:19.734+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:37:19.774+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:37:19.773+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:37:19.808+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:37:19.808+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:37:19.829+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.375 seconds
[2023-09-07T14:37:50.198+0000] {processor.py:157} INFO - Started process (PID=133) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:37:50.201+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:37:50.207+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:37:50.207+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:37:50.342+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:37:50.462+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:37:50.462+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:37:50.479+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:37:50.479+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:37:50.495+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.303 seconds
[2023-09-07T14:38:20.931+0000] {processor.py:157} INFO - Started process (PID=143) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:38:20.946+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:38:20.966+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:38:20.964+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:38:21.332+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:38:21.432+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:38:21.431+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:38:21.505+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:38:21.505+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:38:21.564+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.664 seconds
[2023-09-07T14:38:51.997+0000] {processor.py:157} INFO - Started process (PID=153) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:38:52.002+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:38:52.012+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:38:52.011+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:38:52.173+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:38:52.199+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:38:52.199+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:38:52.231+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:38:52.231+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:38:52.259+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.268 seconds
[2023-09-07T14:39:22.764+0000] {processor.py:157} INFO - Started process (PID=162) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:39:22.770+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:39:22.778+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:39:22.777+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:39:22.947+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:39:23.121+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:39:23.120+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:39:23.145+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:39:23.144+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:39:23.180+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.436 seconds
[2023-09-07T14:39:53.564+0000] {processor.py:157} INFO - Started process (PID=172) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:39:53.568+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:39:53.579+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:39:53.578+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:39:53.787+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:39:53.834+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:39:53.834+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:39:53.863+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:39:53.863+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:39:53.878+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.331 seconds
[2023-09-07T14:40:24.264+0000] {processor.py:157} INFO - Started process (PID=182) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:40:24.269+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:40:24.280+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:40:24.279+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:40:24.450+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:40:24.487+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:40:24.487+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:40:24.519+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:40:24.519+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:40:24.535+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.280 seconds
[2023-09-07T14:49:15.741+0000] {processor.py:157} INFO - Started process (PID=32) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:49:15.769+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:49:15.996+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:49:15.994+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:49:18.679+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:49:19.177+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:49:19.165+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:49:19.288+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:49:19.287+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:49:19.430+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 3.782 seconds
[2023-09-07T14:49:49.928+0000] {processor.py:157} INFO - Started process (PID=42) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:49:49.931+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:49:49.938+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:49:49.937+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:49:50.392+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:49:50.441+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:49:50.440+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:49:50.474+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:49:50.473+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:49:50.498+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.577 seconds
[2023-09-07T14:50:21.128+0000] {processor.py:157} INFO - Started process (PID=52) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:50:21.134+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:50:21.144+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:50:21.143+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:50:22.311+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:50:22.550+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:50:22.548+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:50:22.571+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:50:22.571+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:50:22.590+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.476 seconds
[2023-09-07T14:50:53.022+0000] {processor.py:157} INFO - Started process (PID=63) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:50:53.027+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T14:50:53.035+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:50:53.034+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:50:53.233+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T14:50:53.266+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:50:53.266+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T14:50:53.328+0000] {logging_mixin.py:151} INFO - [2023-09-07T14:50:53.328+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T14:50:53.350+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.338 seconds
[2023-09-07T15:07:06.836+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:07:06.846+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T15:07:06.864+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:07:06.861+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:07:07.335+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:07:07.618+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:07:07.617+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T15:07:07.709+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:07:07.708+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T15:07:08.146+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:07:08.138+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"dataset_triggers": [], "fileloc": "/opt/airflow/dags/crawl_data_using_BeautifulSoup.py", "_task_group": {"_group_id": null, ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 15, 7, 7, 408940, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T15:07:08.156+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:07:08.155+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T15:07:08.159+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"dataset_triggers": [], "fileloc": "/opt/airflow/dags/crawl_data_using_BeautifulSoup.py", "_task_group": {"_group_id": null, ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 15, 7, 7, 408940, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T15:07:38.515+0000] {processor.py:157} INFO - Started process (PID=39) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:07:38.517+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T15:07:38.521+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:07:38.520+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:07:38.726+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:07:38.830+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:07:38.829+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T15:07:38.848+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:07:38.848+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T15:07:38.863+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.353 seconds
[2023-09-07T15:08:09.303+0000] {processor.py:157} INFO - Started process (PID=49) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:08:09.307+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T15:08:09.314+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:08:09.314+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:08:09.759+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:08:09.809+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:08:09.809+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T15:08:09.836+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:08:09.835+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T15:08:09.858+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.565 seconds
[2023-09-07T15:08:40.352+0000] {processor.py:157} INFO - Started process (PID=59) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:08:40.356+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T15:08:40.365+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:08:40.364+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:08:40.509+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:08:40.758+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:08:40.758+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T15:08:40.789+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:08:40.789+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T15:08:40.814+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.472 seconds
[2023-09-07T15:11:48.812+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:11:48.816+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T15:11:48.825+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:11:48.824+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:11:49.252+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:11:49.465+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:11:49.464+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T15:11:49.507+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:11:49.506+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T15:11:49.586+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.785 seconds
[2023-09-07T15:12:20.412+0000] {processor.py:157} INFO - Started process (PID=42) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:12:20.416+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T15:12:20.422+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:12:20.422+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:12:20.712+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:12:20.752+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:12:20.751+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T15:12:20.775+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:12:20.775+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T15:12:20.798+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.392 seconds
[2023-09-07T15:12:51.578+0000] {processor.py:157} INFO - Started process (PID=53) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:12:51.582+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T15:12:51.591+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:12:51.590+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:12:52.781+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:12:53.160+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:12:53.158+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T15:12:53.201+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:12:53.200+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T15:12:53.227+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.661 seconds
[2023-09-07T15:13:23.474+0000] {processor.py:157} INFO - Started process (PID=64) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:13:23.479+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T15:13:23.489+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:13:23.489+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:13:23.639+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:13:23.783+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:13:23.782+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T15:13:23.941+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:13:23.940+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T15:13:23.996+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.536 seconds
[2023-09-07T15:13:54.926+0000] {processor.py:157} INFO - Started process (PID=74) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:13:54.932+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T15:13:54.940+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:13:54.939+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:13:55.065+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T15:13:55.261+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:13:55.261+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T15:13:55.284+0000] {logging_mixin.py:151} INFO - [2023-09-07T15:13:55.283+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T15:13:55.303+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.386 seconds
[2023-09-07T16:53:52.961+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T16:53:52.975+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T16:53:53.004+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:53:53.001+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T16:53:53.725+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T16:53:54.020+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:53:54.018+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T16:53:54.103+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:53:54.102+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T16:53:54.189+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:53:54.179+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"dagrun_timeout": 60.0, "schedule_interval": "@daily", "dataset_triggers": [], "_dag_id": "website_crawler", "fileloc": "/op ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 16, 53, 53, 834714, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T16:53:54.195+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:53:54.194+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T16:53:54.198+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"dagrun_timeout": 60.0, "schedule_interval": "@daily", "dataset_triggers": [], "_dag_id": "website_crawler", "fileloc": "/op ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 16, 53, 53, 834714, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T16:54:24.884+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T16:54:24.910+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T16:54:24.968+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:54:24.958+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T16:54:26.898+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T16:54:27.811+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:54:27.807+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T16:54:27.904+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:54:27.903+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T16:54:27.949+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 3.101 seconds
[2023-09-07T16:55:50.030+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T16:55:50.035+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T16:55:50.048+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:55:50.047+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T16:55:50.510+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T16:55:50.962+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:55:50.962+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T16:55:51.054+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:55:51.054+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T16:55:51.245+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:55:51.230+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"_task_group": {"_group_id": null, "prefix_group_id": true, "tooltip": "", "ui_color": "CornflowerBlue", "ui_fgcolor": "#000 ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 16, 55, 50, 606523, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T16:55:51.252+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:55:51.251+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T16:55:51.258+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"_task_group": {"_group_id": null, "prefix_group_id": true, "tooltip": "", "ui_color": "CornflowerBlue", "ui_fgcolor": "#000 ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 16, 55, 50, 606523, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T16:56:22.207+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T16:56:22.209+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T16:56:22.212+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:56:22.212+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T16:56:22.424+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T16:56:22.559+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:56:22.559+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T16:56:22.586+0000] {logging_mixin.py:151} INFO - [2023-09-07T16:56:22.586+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T16:56:22.606+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.404 seconds
[2023-09-07T17:02:18.894+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:02:18.902+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T17:02:18.908+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:02:18.908+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:02:19.343+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:02:19.599+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:02:19.598+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:02:19.677+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:02:19.677+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:02:20.003+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:02:19.997+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"dagrun_timeout": 60.0, "default_args": {"__var": {"owner": "airflow", "depends_on_past": false, "start_date": {"__var": 169 ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 17, 2, 19, 415390, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T17:02:20.011+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:02:20.010+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:02:20.017+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"dagrun_timeout": 60.0, "default_args": {"__var": {"owner": "airflow", "depends_on_past": false, "start_date": {"__var": 169 ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 7, 17, 2, 19, 415390, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-07T17:02:50.237+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:02:50.241+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T17:02:50.249+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:02:50.248+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:02:50.655+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:02:50.845+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:02:50.844+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:02:50.872+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:02:50.872+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:02:50.892+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.665 seconds
[2023-09-07T17:03:21.071+0000] {processor.py:157} INFO - Started process (PID=50) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:03:21.078+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T17:03:21.091+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:03:21.089+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:03:21.807+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:03:21.843+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:03:21.843+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:03:21.869+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:03:21.868+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:03:21.886+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.825 seconds
[2023-09-07T17:03:52.938+0000] {processor.py:157} INFO - Started process (PID=61) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:03:52.982+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T17:03:53.033+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:03:53.022+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:03:58.880+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:03:58.879+0000] {process_utils.py:259} INFO - Waiting up to 5 seconds for processes to exit...
[2023-09-07T17:29:12.136+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:29:12.147+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T17:29:12.211+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:29:12.195+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:29:13.278+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:29:13.776+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:29:13.774+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:29:13.915+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:29:13.912+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:29:14.070+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.970 seconds
[2023-09-07T17:29:14.462+0000] {processor.py:157} INFO - Started process (PID=33) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:29:14.479+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T17:29:14.500+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:29:14.491+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:29:15.425+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:29:15.499+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:29:15.498+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:29:15.550+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:29:15.549+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:29:15.591+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.137 seconds
[2023-09-07T17:29:46.084+0000] {processor.py:157} INFO - Started process (PID=42) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:29:46.088+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T17:29:46.105+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:29:46.104+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:29:46.373+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:29:46.396+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:29:46.395+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:29:46.421+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:29:46.421+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:29:46.437+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.364 seconds
[2023-09-07T17:30:17.033+0000] {processor.py:157} INFO - Started process (PID=52) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:30:17.036+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T17:30:17.040+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:30:17.040+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:30:17.443+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:30:17.644+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:30:17.643+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:30:17.664+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:30:17.664+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:30:17.684+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.658 seconds
[2023-09-07T17:30:48.175+0000] {processor.py:157} INFO - Started process (PID=62) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:30:48.178+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T17:30:48.182+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:30:48.182+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:30:48.302+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:30:48.339+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:30:48.339+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:30:48.360+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:30:48.360+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:30:48.377+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.207 seconds
[2023-09-07T17:32:06.263+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:32:06.268+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T17:32:06.280+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:32:06.279+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:32:06.673+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:32:06.913+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:32:06.912+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:32:06.967+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:32:06.966+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:32:06.999+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.746 seconds
[2023-09-07T17:32:37.352+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:32:37.355+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T17:32:37.361+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:32:37.360+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:32:37.732+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:32:37.834+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:32:37.832+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:32:37.885+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:32:37.884+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:32:37.909+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.564 seconds
[2023-09-07T17:33:08.927+0000] {processor.py:157} INFO - Started process (PID=51) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:33:08.932+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T17:33:08.944+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:33:08.941+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:33:09.790+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:33:10.005+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:33:10.004+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:33:10.032+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:33:10.032+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:33:10.052+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.134 seconds
[2023-09-07T17:33:40.591+0000] {processor.py:157} INFO - Started process (PID=61) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:33:40.594+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T17:33:40.600+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:33:40.600+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:33:40.695+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:33:40.730+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:33:40.729+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:33:40.752+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:33:40.752+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:33:40.769+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.184 seconds
[2023-09-07T17:34:11.401+0000] {processor.py:157} INFO - Started process (PID=71) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:34:11.405+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-07T17:34:11.421+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:34:11.419+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:34:11.578+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-07T17:34:11.773+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:34:11.773+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-07T17:34:11.798+0000] {logging_mixin.py:151} INFO - [2023-09-07T17:34:11.798+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-07T17:34:11.815+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.424 seconds
