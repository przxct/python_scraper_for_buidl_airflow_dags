[2023-09-10T13:02:17.008+0000] {processor.py:157} INFO - Started process (PID=34) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:02:17.020+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:02:17.050+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:02:17.049+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:02:17.890+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:02:18.003+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:02:18.001+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:02:18.049+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:02:18.048+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:02:18.085+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.093 seconds
[2023-09-10T13:02:48.553+0000] {processor.py:157} INFO - Started process (PID=56) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:02:48.573+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:02:48.647+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:02:48.640+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:02:51.768+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:02:52.820+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:02:52.817+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:02:52.903+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:02:52.899+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:02:53.137+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 4.652 seconds
[2023-09-10T13:03:18.706+0000] {processor.py:157} INFO - Started process (PID=66) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:03:18.714+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:03:18.759+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:03:18.756+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:03:22.703+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:03:23.433+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:03:23.432+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:03:23.738+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:03:23.738+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:03:23.872+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 5.241 seconds
[2023-09-10T13:03:55.547+0000] {processor.py:157} INFO - Started process (PID=76) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:03:55.556+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:03:55.580+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:03:55.579+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:03:55.863+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:03:56.199+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:03:56.198+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:03:56.265+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:03:56.265+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:03:56.306+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.764 seconds
[2023-09-10T13:04:27.893+0000] {processor.py:157} INFO - Started process (PID=87) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:04:27.951+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:04:28.090+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:04:28.070+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:04:30.933+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:04:31.656+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:04:31.640+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:04:31.794+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:04:31.792+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:04:31.855+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 4.465 seconds
[2023-09-10T13:05:02.821+0000] {processor.py:157} INFO - Started process (PID=103) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:05:02.838+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:05:02.907+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:05:02.896+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:05:03.887+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:05:04.774+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:05:04.773+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:05:04.856+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:05:04.854+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:05:04.936+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.234 seconds
[2023-09-10T13:05:56.530+0000] {processor.py:157} INFO - Started process (PID=112) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:05:57.098+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:05:57.311+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:05:57.295+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:06:00.952+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:06:03.312+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:06:03.263+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:06:04.072+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:06:04.067+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:06:04.446+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 10.911 seconds
[2023-09-10T13:06:36.199+0000] {processor.py:157} INFO - Started process (PID=122) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:06:36.206+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:06:36.230+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:06:36.227+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:06:36.876+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:06:37.750+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:06:37.750+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:06:37.782+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:06:37.781+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:06:37.820+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.684 seconds
[2023-09-10T13:07:09.104+0000] {processor.py:157} INFO - Started process (PID=132) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:07:09.121+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:07:09.232+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:07:09.213+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:07:10.626+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:07:10.945+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:07:10.944+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:07:11.316+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:07:11.315+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:07:11.429+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.463 seconds
[2023-09-10T13:07:42.590+0000] {processor.py:157} INFO - Started process (PID=142) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:07:42.612+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:07:42.667+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:07:42.662+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:07:43.831+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:07:44.659+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:07:44.657+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:07:44.918+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:07:44.918+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:07:44.959+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.474 seconds
[2023-09-10T13:08:16.101+0000] {processor.py:157} INFO - Started process (PID=152) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:08:16.132+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:08:16.218+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:08:16.211+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:08:19.087+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:08:19.865+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:08:19.863+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:08:20.177+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:08:20.176+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:08:20.358+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 4.352 seconds
[2023-09-10T13:08:53.895+0000] {processor.py:157} INFO - Started process (PID=169) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:08:53.996+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:08:54.136+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:08:54.126+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:08:56.511+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:08:59.639+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:08:59.627+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:08:59.768+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:08:59.767+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:08:59.912+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 6.367 seconds
[2023-09-10T13:09:31.485+0000] {processor.py:157} INFO - Started process (PID=178) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:09:31.612+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:09:31.781+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:09:31.767+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:09:33.733+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:09:33.879+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:09:33.878+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:09:33.980+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:09:33.978+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:09:34.037+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.872 seconds
[2023-09-10T13:10:09.218+0000] {processor.py:157} INFO - Started process (PID=189) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:10:09.415+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:10:12.261+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:10:12.208+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:10:17.205+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:10:20.878+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:10:20.874+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:10:21.298+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:10:21.298+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:10:21.480+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 13.964 seconds
[2023-09-10T13:10:52.885+0000] {processor.py:157} INFO - Started process (PID=200) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:10:53.114+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:10:53.541+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:10:53.516+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:10:59.900+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:11:01.555+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:11:01.463+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:11:02.637+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:11:02.626+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:11:03.337+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 11.149 seconds
[2023-09-10T13:11:35.111+0000] {processor.py:157} INFO - Started process (PID=209) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:11:35.116+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:11:35.138+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:11:35.132+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:11:35.628+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:11:35.893+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:11:35.892+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:11:35.925+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:11:35.924+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:11:35.945+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.872 seconds
[2023-09-10T13:12:06.720+0000] {processor.py:157} INFO - Started process (PID=219) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:12:06.724+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:12:06.730+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:12:06.729+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:12:06.866+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:12:06.912+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:12:06.912+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:12:06.941+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:12:06.941+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:12:06.971+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.258 seconds
[2023-09-10T13:12:37.651+0000] {processor.py:157} INFO - Started process (PID=229) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:12:37.656+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:12:37.662+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:12:37.661+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:12:37.800+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:12:37.834+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:12:37.834+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:12:37.864+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:12:37.863+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:12:37.880+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.237 seconds
[2023-09-10T13:13:08.699+0000] {processor.py:157} INFO - Started process (PID=239) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:13:08.703+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:13:08.712+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:13:08.710+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:13:09.012+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:13:09.280+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:13:09.280+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:13:09.304+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:13:09.304+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:13:09.333+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.643 seconds
[2023-09-10T13:13:40.020+0000] {processor.py:157} INFO - Started process (PID=249) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:13:40.024+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:13:40.044+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:13:40.042+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:13:40.292+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:13:40.388+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:13:40.388+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:13:40.474+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:13:40.473+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:13:40.532+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.522 seconds
[2023-09-10T13:14:10.765+0000] {processor.py:157} INFO - Started process (PID=259) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:14:10.769+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:14:10.776+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:14:10.775+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:14:10.943+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:14:11.000+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:14:11.000+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:14:11.111+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:14:11.110+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:14:11.147+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.392 seconds
[2023-09-10T13:14:43.223+0000] {processor.py:157} INFO - Started process (PID=269) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:14:43.281+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:14:43.401+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:14:43.384+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:14:44.428+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:14:46.150+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:14:46.098+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:14:46.318+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:14:46.318+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:14:46.465+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 3.368 seconds
[2023-09-10T13:15:17.552+0000] {processor.py:157} INFO - Started process (PID=286) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:15:17.564+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:15:17.603+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:15:17.599+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:15:17.990+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:15:18.043+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:15:18.043+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:15:18.073+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:15:18.072+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:15:18.096+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.582 seconds
[2023-09-10T13:15:49.201+0000] {processor.py:157} INFO - Started process (PID=296) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:15:49.210+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:15:49.262+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:15:49.256+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:15:49.576+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:15:49.952+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:15:49.951+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:15:49.975+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:15:49.974+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:15:49.996+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.836 seconds
[2023-09-10T13:16:20.930+0000] {processor.py:157} INFO - Started process (PID=306) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:16:20.980+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:16:21.019+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:16:21.004+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:16:23.069+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:16:23.504+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:16:23.500+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:16:23.776+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:16:23.775+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:16:23.907+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 3.092 seconds
[2023-09-10T13:16:54.561+0000] {processor.py:157} INFO - Started process (PID=315) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:16:54.568+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:16:54.588+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:16:54.583+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:16:54.877+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:16:55.138+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:16:55.137+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:16:55.175+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:16:55.175+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:16:55.201+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.656 seconds
[2023-09-10T13:17:25.852+0000] {processor.py:157} INFO - Started process (PID=325) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:17:25.858+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:17:25.874+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:17:25.870+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:17:26.105+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:17:26.217+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:17:26.216+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:17:26.291+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:17:26.291+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:17:26.333+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.490 seconds
[2023-09-10T13:17:57.271+0000] {processor.py:157} INFO - Started process (PID=335) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:17:57.286+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:17:57.339+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:17:57.335+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:17:58.334+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:17:58.645+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:17:58.643+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:17:58.671+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:17:58.670+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:17:58.690+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.439 seconds
[2023-09-10T13:32:03.463+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:32:03.472+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:32:03.501+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:32:03.500+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:32:04.374+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:32:04.892+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:32:04.889+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:32:04.977+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:32:04.976+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:32:05.224+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:32:05.215+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"schedule_interval": "@daily", "dataset_triggers": [], "_task_group": {"_group_id": null, "prefix_group_id": true, "tooltip" ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 13, 32, 4, 485248, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T13:32:05.232+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:32:05.232+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:32:05.246+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"schedule_interval": "@daily", "dataset_triggers": [], "_task_group": {"_group_id": null, "prefix_group_id": true, "tooltip" ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 13, 32, 4, 485248, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T13:32:35.955+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:32:35.961+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:32:35.972+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:32:35.971+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:32:36.533+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:32:36.704+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:32:36.704+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:32:36.729+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:32:36.728+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:32:36.752+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.826 seconds
[2023-09-10T13:33:08.051+0000] {processor.py:157} INFO - Started process (PID=50) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:33:08.199+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:33:08.247+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:33:08.227+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:33:18.198+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:33:18.658+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:33:18.624+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:33:18.756+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:33:18.755+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:33:18.829+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 10.844 seconds
[2023-09-10T13:33:50.033+0000] {processor.py:157} INFO - Started process (PID=66) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:33:50.050+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:33:50.091+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:33:50.086+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:33:50.755+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:33:51.672+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:33:51.671+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:33:51.715+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:33:51.714+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:33:51.747+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.756 seconds
[2023-09-10T13:37:51.172+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:37:51.177+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:37:51.189+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:37:51.187+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:37:51.922+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:37:52.546+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:37:52.541+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:37:52.732+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:37:52.728+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:37:52.882+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.724 seconds
[2023-09-10T13:38:24.401+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:38:24.484+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:38:24.618+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:38:24.611+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:38:30.484+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:38:30.796+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:38:30.774+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:38:30.865+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:38:30.865+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:38:30.934+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 6.609 seconds
[2023-09-10T13:39:12.648+0000] {processor.py:157} INFO - Started process (PID=50) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:39:12.698+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:39:12.988+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:39:12.963+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:39:44.306+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:39:47.764+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:39:47.542+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:39:48.363+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:39:48.360+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:39:51.129+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 40.569 seconds
[2023-09-10T13:40:46.662+0000] {processor.py:157} INFO - Started process (PID=75) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:40:46.668+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:40:46.696+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:40:46.695+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:40:47.732+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:40:48.053+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:40:48.048+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:40:48.262+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:40:48.262+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:40:48.468+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.821 seconds
[2023-09-10T13:41:21.281+0000] {processor.py:157} INFO - Started process (PID=85) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:41:21.350+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:41:21.399+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:41:21.395+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:41:23.241+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:41:25.327+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:41:25.298+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:41:25.499+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:41:25.498+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:41:25.602+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 4.457 seconds
[2023-09-10T13:41:56.787+0000] {processor.py:157} INFO - Started process (PID=94) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:41:56.793+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:41:56.799+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:41:56.797+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:41:57.085+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:41:57.181+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:41:57.180+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:41:57.225+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:41:57.225+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:41:57.260+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.493 seconds
[2023-09-10T13:48:43.541+0000] {processor.py:157} INFO - Started process (PID=29) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:48:43.548+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:48:43.581+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:48:43.580+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:48:44.583+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:48:45.027+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:48:45.023+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:48:45.134+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:48:45.132+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:48:45.218+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.690 seconds
[2023-09-10T13:49:16.304+0000] {processor.py:157} INFO - Started process (PID=39) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:49:16.309+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:49:16.318+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:49:16.317+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:49:16.927+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:49:17.031+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:49:17.031+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:49:17.091+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:49:17.091+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:49:17.126+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.834 seconds
[2023-09-10T13:49:48.072+0000] {processor.py:157} INFO - Started process (PID=50) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:49:48.095+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:49:48.144+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:49:48.138+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:50:57.920+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:50:57.887+0000] {timeout.py:68} ERROR - Process timed out, PID: 50
[2023-09-10T13:50:57.994+0000] {logging_mixin.py:151} WARNING - Exception ignored in: <function _collection_gced at 0xffff98a890d0>
[2023-09-10T13:50:58.007+0000] {logging_mixin.py:151} WARNING - Traceback (most recent call last):
[2023-09-10T13:50:58.016+0000] {logging_mixin.py:151} WARNING -   File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/event/registry.py", line 53, in _collection_gced
[2023-09-10T13:50:58.175+0000] {logging_mixin.py:151} WARNING -     def _collection_gced(ref):
[2023-09-10T13:50:58.214+0000] {logging_mixin.py:151} WARNING -   File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
[2023-09-10T13:50:58.236+0000] {logging_mixin.py:151} WARNING -     raise AirflowTaskTimeout(self.error_message)
[2023-09-10T13:50:58.264+0000] {logging_mixin.py:151} WARNING - airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/crawl_data_log_into_postgres.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.7.0/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.7.0/best-practices.html#reducing-dag-complexity, PID: 50
[2023-09-10T13:51:03.775+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:51:12.902+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:51:12.794+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:51:13.619+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:51:13.610+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:51:14.297+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 86.286 seconds
[2023-09-10T13:51:39.080+0000] {processor.py:157} INFO - Started process (PID=73) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:51:39.098+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:51:39.157+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:51:39.153+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:51:40.439+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:51:40.877+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:51:40.862+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:51:40.979+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:51:40.979+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:51:41.047+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.036 seconds
[2023-09-10T13:56:35.750+0000] {processor.py:157} INFO - Started process (PID=36) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:56:35.788+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:56:35.857+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:56:35.855+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:56:38.035+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:56:38.592+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:56:38.590+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:56:38.816+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:56:38.805+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:56:39.345+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:56:39.334+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"edge_info": {}, "fileloc": "/opt/airflow/dags/crawl_data_log_into_postgres.py", "_task_group": {"_group_id": null, "prefix_ ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 13, 56, 38, 170144, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T13:56:39.359+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:56:39.359+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:56:39.368+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"edge_info": {}, "fileloc": "/opt/airflow/dags/crawl_data_log_into_postgres.py", "_task_group": {"_group_id": null, "prefix_ ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 13, 56, 38, 170144, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T13:57:10.694+0000] {processor.py:157} INFO - Started process (PID=47) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:57:10.709+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:57:10.746+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:57:10.744+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:57:17.262+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:57:17.956+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:57:17.955+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:57:18.048+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:57:18.048+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:57:18.166+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 7.500 seconds
[2023-09-10T13:57:48.980+0000] {processor.py:157} INFO - Started process (PID=57) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:57:49.077+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:57:49.220+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:57:49.215+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:58:05.479+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:58:06.049+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:58:06.026+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:58:06.260+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:58:06.257+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:58:06.361+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 17.561 seconds
[2023-09-10T13:58:37.396+0000] {processor.py:157} INFO - Started process (PID=74) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:58:37.416+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:58:37.446+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:58:37.444+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:58:37.961+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:58:38.528+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:58:38.526+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:58:38.598+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:58:38.597+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:58:38.673+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.310 seconds
[2023-09-10T13:59:09.345+0000] {processor.py:157} INFO - Started process (PID=83) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:59:09.357+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:59:09.419+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:59:09.413+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:59:10.318+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:59:10.637+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:59:10.634+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:59:10.772+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:59:10.772+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:59:10.899+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.629 seconds
[2023-09-10T13:59:42.311+0000] {processor.py:157} INFO - Started process (PID=93) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:59:42.327+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T13:59:42.369+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:59:42.364+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:59:43.096+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T13:59:43.960+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:59:43.958+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:59:44.127+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:59:44.126+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:59:44.304+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.009 seconds
[2023-09-10T14:00:15.507+0000] {processor.py:157} INFO - Started process (PID=103) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:00:15.515+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:00:15.548+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:00:15.537+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:00:15.944+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:00:16.082+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:00:16.081+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:00:16.157+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:00:16.157+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:00:16.223+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.744 seconds
[2023-09-10T14:00:47.107+0000] {processor.py:157} INFO - Started process (PID=113) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:00:47.116+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:00:47.126+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:00:47.125+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:00:47.509+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:00:48.039+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:00:48.038+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:00:48.076+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:00:48.075+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:00:48.111+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.032 seconds
[2023-09-10T14:01:18.514+0000] {processor.py:157} INFO - Started process (PID=124) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:01:18.526+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:01:18.538+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:01:18.538+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:01:18.816+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:01:18.978+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:01:18.975+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:01:19.100+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:01:19.100+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:01:19.150+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.655 seconds
[2023-09-10T14:01:49.643+0000] {processor.py:157} INFO - Started process (PID=134) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:01:49.649+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:01:49.662+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:01:49.661+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:01:49.913+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:01:50.351+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:01:50.350+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:01:50.409+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:01:50.409+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:01:50.459+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.836 seconds
[2023-09-10T14:02:22.180+0000] {processor.py:157} INFO - Started process (PID=144) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:02:22.215+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:02:22.305+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:02:22.295+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:02:23.503+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:02:23.726+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:02:23.723+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:02:24.118+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:02:24.115+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:02:24.274+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.308 seconds
[2023-09-10T14:02:56.258+0000] {processor.py:157} INFO - Started process (PID=155) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:02:56.309+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:02:56.870+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:02:56.825+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:03:00.554+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:03:01.738+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:03:01.733+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:03:01.854+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:03:01.854+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:03:01.928+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 5.967 seconds
[2023-09-10T14:03:36.448+0000] {processor.py:157} INFO - Started process (PID=165) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:03:36.465+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:03:36.500+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:03:36.493+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:03:38.844+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:03:39.681+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:03:39.660+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:03:40.047+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:03:40.046+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:03:40.303+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 3.865 seconds
[2023-09-10T14:04:13.739+0000] {processor.py:157} INFO - Started process (PID=174) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:04:13.798+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:04:13.967+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:04:13.959+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:04:18.685+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:04:25.449+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:04:25.407+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:04:25.744+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:04:25.743+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:04:25.979+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 12.567 seconds
[2023-09-10T14:04:59.492+0000] {processor.py:157} INFO - Started process (PID=184) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:04:59.562+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:04:59.951+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:04:59.929+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:05:15.611+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:05:18.270+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:05:18.071+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:05:19.364+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:05:19.359+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:05:19.786+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 20.489 seconds
[2023-09-10T14:05:55.314+0000] {processor.py:157} INFO - Started process (PID=202) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:05:55.358+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:05:55.482+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:05:55.467+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:05:56.774+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:05:59.003+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:05:58.850+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:05:59.123+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:05:59.123+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:05:59.365+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 4.150 seconds
[2023-09-10T14:06:31.037+0000] {processor.py:157} INFO - Started process (PID=214) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:06:31.072+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:06:31.115+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:06:31.113+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:06:32.118+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:06:32.334+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:06:32.326+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:06:32.564+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:06:32.564+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:06:32.674+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.679 seconds
[2023-09-10T14:07:07.235+0000] {processor.py:157} INFO - Started process (PID=223) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:07:07.331+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:07:07.583+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:07:07.555+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:07:11.833+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:07:14.015+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:07:13.986+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:07:14.339+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:07:14.336+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:07:15.486+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 8.488 seconds
[2023-09-10T14:07:52.641+0000] {processor.py:157} INFO - Started process (PID=234) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:07:52.659+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:07:52.720+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:07:52.712+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:07:53.254+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:07:53.416+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:07:53.415+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:07:53.508+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:07:53.507+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:07:53.594+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.994 seconds
[2023-09-10T14:08:26.226+0000] {processor.py:157} INFO - Started process (PID=244) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:08:26.325+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:08:26.427+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:08:26.417+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:08:29.776+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:08:32.218+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:08:32.157+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:08:32.458+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:08:32.455+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:08:32.647+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 6.590 seconds
[2023-09-10T14:09:03.728+0000] {processor.py:157} INFO - Started process (PID=261) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:09:03.735+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:09:03.746+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:09:03.745+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:09:03.939+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:09:03.998+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:09:03.997+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:09:04.057+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:09:04.057+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:09:04.077+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.362 seconds
[2023-09-10T14:09:37.063+0000] {processor.py:157} INFO - Started process (PID=272) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:09:37.111+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:09:37.236+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:09:37.234+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:09:38.067+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:09:38.575+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:09:38.573+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:09:38.655+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:09:38.654+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:09:38.763+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.625 seconds
[2023-09-10T14:10:09.201+0000] {processor.py:157} INFO - Started process (PID=282) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:10:09.209+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:10:09.244+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:10:09.242+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:10:09.570+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:10:09.679+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:10:09.679+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:10:09.757+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:10:09.756+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:10:09.811+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.635 seconds
[2023-09-10T14:10:41.828+0000] {processor.py:157} INFO - Started process (PID=291) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:10:41.849+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:10:41.951+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:10:41.924+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:10:43.673+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:10:46.082+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:10:46.078+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:10:46.162+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:10:46.161+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:10:46.301+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 4.668 seconds
[2023-09-10T14:11:17.655+0000] {processor.py:157} INFO - Started process (PID=301) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:11:17.664+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:11:17.677+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:11:17.675+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:11:18.249+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:11:18.413+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:11:18.412+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:11:18.479+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:11:18.478+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:11:18.532+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.903 seconds
[2023-09-10T14:11:49.505+0000] {processor.py:157} INFO - Started process (PID=310) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:11:49.559+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:11:49.601+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:11:49.598+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:11:50.232+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:11:51.351+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:11:51.278+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:11:51.839+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:11:51.837+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:11:52.031+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.590 seconds
[2023-09-10T14:12:22.825+0000] {processor.py:157} INFO - Started process (PID=322) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:12:22.834+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:12:22.844+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:12:22.843+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:12:23.076+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:12:23.115+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:12:23.114+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:12:23.148+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:12:23.147+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:12:23.172+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.357 seconds
[2023-09-10T14:12:53.636+0000] {processor.py:157} INFO - Started process (PID=331) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:12:53.647+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:12:53.669+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:12:53.668+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:12:53.904+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:12:54.323+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:12:54.321+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:12:54.372+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:12:54.371+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:12:54.402+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.793 seconds
[2023-09-10T14:13:25.936+0000] {processor.py:157} INFO - Started process (PID=341) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:13:25.962+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:13:26.008+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:13:26.000+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:13:26.857+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:13:27.488+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:13:27.471+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:13:27.919+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:13:27.918+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:13:28.351+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.519 seconds
[2023-09-10T14:14:01.882+0000] {processor.py:157} INFO - Started process (PID=351) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:14:01.923+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:14:02.056+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:14:02.032+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:14:06.926+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:14:10.353+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:14:10.324+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:14:10.783+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:14:10.782+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:14:11.122+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 9.468 seconds
[2023-09-10T14:14:42.283+0000] {processor.py:157} INFO - Started process (PID=362) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:14:42.306+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:14:42.316+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:14:42.315+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:14:42.729+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:14:42.780+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:14:42.780+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:14:42.909+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:14:42.908+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:14:43.004+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.752 seconds
[2023-09-10T14:15:13.340+0000] {processor.py:157} INFO - Started process (PID=379) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:15:13.344+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:15:13.354+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:15:13.353+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:15:13.515+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:15:13.647+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:15:13.647+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:15:13.673+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:15:13.673+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:15:13.695+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.362 seconds
[2023-09-10T14:15:55.150+0000] {processor.py:157} INFO - Started process (PID=387) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:15:55.211+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:15:55.692+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:15:55.675+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:15:58.906+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:16:02.820+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:16:02.793+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:16:03.395+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:16:03.395+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:16:03.601+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 8.731 seconds
[2023-09-10T14:16:35.627+0000] {processor.py:157} INFO - Started process (PID=397) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:16:35.661+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:16:35.684+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:16:35.682+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:16:36.501+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:16:37.289+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:16:37.287+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:16:37.384+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:16:37.383+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:16:37.440+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.847 seconds
[2023-09-10T14:17:20.948+0000] {processor.py:157} INFO - Started process (PID=407) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:17:21.405+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:17:22.439+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:17:22.386+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:17:31.624+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:17:33.464+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:17:33.399+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:17:34.202+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:17:34.201+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:17:34.491+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 18.063 seconds
[2023-09-10T14:18:22.029+0000] {processor.py:157} INFO - Started process (PID=423) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:18:22.207+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:18:22.682+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:18:22.589+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:18:27.704+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:18:31.736+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:18:31.645+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:18:33.081+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:18:33.014+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:18:33.833+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 13.588 seconds
[2023-09-10T14:19:05.542+0000] {processor.py:157} INFO - Started process (PID=434) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:19:05.624+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:19:05.840+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:19:05.828+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:19:07.876+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:19:08.418+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:19:08.407+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:19:08.646+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:19:08.643+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:19:08.806+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 3.543 seconds
[2023-09-10T14:19:40.647+0000] {processor.py:157} INFO - Started process (PID=442) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:19:40.696+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:19:40.763+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:19:40.755+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:19:41.656+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:19:42.751+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:19:42.746+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:19:42.855+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:19:42.853+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:19:43.033+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.519 seconds
[2023-09-10T14:20:13.723+0000] {processor.py:157} INFO - Started process (PID=453) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:20:13.744+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:20:13.786+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:20:13.781+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:20:14.920+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:20:15.281+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:20:15.276+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:20:15.839+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:20:15.781+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:20:16.057+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.410 seconds
[2023-09-10T14:20:46.472+0000] {processor.py:157} INFO - Started process (PID=464) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:20:46.482+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:20:46.501+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:20:46.499+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:20:47.166+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:20:47.977+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:20:47.967+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:20:48.108+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:20:48.106+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:20:48.267+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.810 seconds
[2023-09-10T14:21:18.735+0000] {processor.py:157} INFO - Started process (PID=474) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:21:18.738+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:21:18.742+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:21:18.742+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:21:18.888+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:21:18.935+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:21:18.935+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:21:18.966+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:21:18.966+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:21:18.989+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.259 seconds
[2023-09-10T14:21:49.363+0000] {processor.py:157} INFO - Started process (PID=484) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:21:49.366+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:21:49.371+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:21:49.370+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:21:49.492+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:21:49.631+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:21:49.630+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:21:49.655+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:21:49.655+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:21:49.685+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.326 seconds
[2023-09-10T14:22:20.361+0000] {processor.py:157} INFO - Started process (PID=495) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:22:20.377+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:22:20.393+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:22:20.392+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:22:20.777+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:22:20.889+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:22:20.887+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:22:21.011+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:22:21.010+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:22:21.073+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.732 seconds
[2023-09-10T14:22:51.709+0000] {processor.py:157} INFO - Started process (PID=505) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:22:51.714+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:22:51.720+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:22:51.719+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:22:52.111+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:22:52.747+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:22:52.744+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:22:52.793+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:22:52.793+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:22:52.837+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.136 seconds
[2023-09-10T14:23:27.588+0000] {processor.py:157} INFO - Started process (PID=516) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:23:27.603+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:23:27.685+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:23:27.683+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:23:28.260+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:23:28.367+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:23:28.365+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:23:28.505+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:23:28.504+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:23:28.579+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.082 seconds
[2023-09-10T14:23:59.151+0000] {processor.py:157} INFO - Started process (PID=526) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:23:59.155+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:23:59.160+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:23:59.159+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:23:59.277+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:23:59.545+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:23:59.544+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:23:59.580+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:23:59.579+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:23:59.606+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.460 seconds
[2023-09-10T14:24:29.740+0000] {processor.py:157} INFO - Started process (PID=536) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:24:29.743+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:24:29.747+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:24:29.747+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:24:29.859+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:24:29.906+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:24:29.906+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:24:29.948+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:24:29.947+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:24:29.979+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.244 seconds
[2023-09-10T14:25:01.004+0000] {processor.py:157} INFO - Started process (PID=546) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:25:01.013+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:25:01.024+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:25:01.023+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:25:01.281+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:25:01.349+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:25:01.346+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:25:01.445+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:25:01.445+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:25:01.484+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.498 seconds
[2023-09-10T14:25:33.217+0000] {processor.py:157} INFO - Started process (PID=562) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:25:33.256+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:25:33.357+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:25:33.335+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:25:35.483+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:25:37.413+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:25:37.409+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:25:37.627+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:25:37.626+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:25:37.771+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 4.984 seconds
[2023-09-10T14:26:10.272+0000] {processor.py:157} INFO - Started process (PID=573) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:26:10.381+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:26:10.546+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:26:10.545+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:26:11.711+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:26:11.990+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:26:11.988+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:26:12.125+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:26:12.124+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:26:12.187+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.032 seconds
[2023-09-10T14:26:43.445+0000] {processor.py:157} INFO - Started process (PID=582) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:26:43.460+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:26:43.488+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:26:43.486+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:26:44.284+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:26:45.602+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:26:45.600+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:26:45.740+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:26:45.740+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:26:45.809+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.468 seconds
[2023-09-10T14:27:16.413+0000] {processor.py:157} INFO - Started process (PID=595) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:27:16.452+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:27:16.575+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:27:16.553+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:27:17.971+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:27:18.246+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:27:18.244+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:27:18.396+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:27:18.395+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:27:18.499+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.224 seconds
[2023-09-10T14:27:49.201+0000] {processor.py:157} INFO - Started process (PID=605) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:27:49.213+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:27:49.279+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:27:49.273+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:27:49.755+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:27:50.389+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:27:50.388+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:27:50.418+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:27:50.418+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:27:50.452+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.286 seconds
[2023-09-10T14:28:20.996+0000] {processor.py:157} INFO - Started process (PID=616) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:28:21.005+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:28:21.059+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:28:21.042+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:28:22.187+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:28:22.563+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:28:22.558+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:28:22.768+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:28:22.767+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:28:23.171+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.198 seconds
[2023-09-10T14:28:55.336+0000] {processor.py:157} INFO - Started process (PID=626) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:28:55.413+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:28:55.521+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:28:55.498+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:28:56.504+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:28:57.064+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:28:57.062+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:28:57.110+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:28:57.110+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:28:57.146+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.061 seconds
[2023-09-10T14:29:27.569+0000] {processor.py:157} INFO - Started process (PID=637) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:29:27.585+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:29:27.608+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:29:27.605+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:29:28.110+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:29:28.291+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:29:28.290+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:29:28.586+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:29:28.584+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:29:29.057+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.530 seconds
[2023-09-10T14:30:00.633+0000] {processor.py:157} INFO - Started process (PID=648) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:30:00.683+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:30:00.822+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:30:00.816+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:30:02.202+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:30:02.738+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:30:02.736+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:30:02.797+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:30:02.796+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:30:02.840+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.329 seconds
[2023-09-10T14:30:34.283+0000] {processor.py:157} INFO - Started process (PID=658) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:30:34.351+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:30:34.448+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:30:34.436+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:30:36.731+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:30:37.076+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:30:37.073+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:30:37.206+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:30:37.206+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:30:37.342+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 3.111 seconds
[2023-09-10T14:31:08.769+0000] {processor.py:157} INFO - Started process (PID=667) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:31:08.822+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:31:08.911+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:31:08.907+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:31:09.777+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:31:10.361+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:31:10.358+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:31:10.411+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:31:10.410+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:31:10.448+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.900 seconds
[2023-09-10T14:31:41.480+0000] {processor.py:157} INFO - Started process (PID=677) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:31:41.513+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:31:41.666+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:31:41.659+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:31:43.697+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:31:44.571+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:31:44.553+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:31:44.967+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:31:44.966+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:31:45.080+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 3.742 seconds
[2023-09-10T14:32:15.986+0000] {processor.py:157} INFO - Started process (PID=686) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:32:16.006+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:32:16.056+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:32:16.053+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:32:16.536+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:32:17.152+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:32:17.149+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:32:17.257+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:32:17.256+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:32:17.356+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.434 seconds
[2023-09-10T14:32:48.299+0000] {processor.py:157} INFO - Started process (PID=696) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:32:48.309+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:32:48.334+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:32:48.332+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:32:48.683+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:32:48.757+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:32:48.756+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:32:48.801+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:32:48.800+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:32:48.834+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.561 seconds
[2023-09-10T14:33:19.439+0000] {processor.py:157} INFO - Started process (PID=706) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:33:19.448+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:33:19.472+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:33:19.470+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:33:19.805+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:33:20.210+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:33:20.209+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:33:20.275+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:33:20.274+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:33:20.336+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.917 seconds
[2023-09-10T14:33:51.101+0000] {processor.py:157} INFO - Started process (PID=716) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:33:51.110+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:33:51.125+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:33:51.124+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:33:51.541+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:33:51.700+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:33:51.700+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:33:51.772+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:33:51.771+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:33:51.819+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.741 seconds
[2023-09-10T14:34:22.102+0000] {processor.py:157} INFO - Started process (PID=725) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:34:22.108+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:34:22.125+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:34:22.124+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:34:22.538+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:34:22.817+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:34:22.816+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:34:22.865+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:34:22.865+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:34:22.902+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.811 seconds
[2023-09-10T14:34:53.663+0000] {processor.py:157} INFO - Started process (PID=735) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:34:53.669+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:34:53.700+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:34:53.698+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:34:53.969+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:34:54.101+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:34:54.099+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:34:54.195+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:34:54.195+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:34:54.239+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.589 seconds
[2023-09-10T14:35:24.998+0000] {processor.py:157} INFO - Started process (PID=752) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:35:25.005+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:35:25.030+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:35:25.024+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:35:25.529+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:35:25.983+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:35:25.981+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:35:26.041+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:35:26.040+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:35:26.104+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.125 seconds
[2023-09-10T14:35:56.657+0000] {processor.py:157} INFO - Started process (PID=762) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:35:56.668+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:35:56.686+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:35:56.684+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:35:57.021+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:35:57.139+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:35:57.138+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:35:57.209+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:35:57.208+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:35:57.250+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.618 seconds
[2023-09-10T14:36:27.905+0000] {processor.py:157} INFO - Started process (PID=772) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:36:27.910+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:36:27.921+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:36:27.920+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:36:28.136+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:36:28.467+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:36:28.465+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:36:28.509+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:36:28.509+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:36:28.542+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.654 seconds
[2023-09-10T14:36:59.290+0000] {processor.py:157} INFO - Started process (PID=783) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:36:59.299+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:36:59.316+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:36:59.314+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:36:59.668+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:36:59.772+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:36:59.770+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:36:59.837+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:36:59.837+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:36:59.881+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.621 seconds
[2023-09-10T14:37:30.847+0000] {processor.py:157} INFO - Started process (PID=793) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:37:30.864+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:37:30.891+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:37:30.890+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:37:31.120+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:37:31.424+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:37:31.423+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:37:31.467+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:37:31.467+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:37:31.524+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.697 seconds
[2023-09-10T14:39:16.879+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:39:16.898+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:39:16.915+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:39:16.914+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:39:17.638+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:39:18.362+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:39:18.361+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:39:18.475+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:39:18.474+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:39:18.901+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:39:18.877+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"schedule_interval": "@daily", "fileloc": "/opt/airflow/dags/crawl_data_log_into_postgres.py", "_task_group": {"_group_id":  ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 14, 39, 17, 762568, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T14:39:18.910+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:39:18.908+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:39:18.927+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"schedule_interval": "@daily", "fileloc": "/opt/airflow/dags/crawl_data_log_into_postgres.py", "_task_group": {"_group_id":  ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 14, 39, 17, 762568, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T14:39:49.421+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:39:49.428+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:39:49.459+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:39:49.454+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:39:51.379+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:39:52.379+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:39:52.363+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:39:52.450+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:39:52.450+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:39:52.512+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 3.140 seconds
[2023-09-10T14:40:23.714+0000] {processor.py:157} INFO - Started process (PID=51) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:40:23.719+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:40:23.726+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:40:23.725+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:40:24.134+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:40:24.179+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:40:24.178+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:40:24.248+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:40:24.247+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:40:24.279+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.575 seconds
[2023-09-10T14:40:54.512+0000] {processor.py:157} INFO - Started process (PID=62) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:40:54.517+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:40:54.522+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:40:54.522+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:40:54.626+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:40:54.749+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:40:54.749+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:40:54.774+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:40:54.773+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:40:54.796+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.288 seconds
[2023-09-10T14:41:25.016+0000] {processor.py:157} INFO - Started process (PID=70) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:41:25.024+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:41:25.055+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:41:25.052+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:41:25.380+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:41:25.495+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:41:25.494+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:41:25.577+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:41:25.576+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:41:25.620+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.620 seconds
[2023-09-10T14:41:56.377+0000] {processor.py:157} INFO - Started process (PID=80) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:41:56.383+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:41:56.392+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:41:56.391+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:41:56.603+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:41:56.819+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:41:56.819+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:41:56.858+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:41:56.857+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:41:56.888+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.526 seconds
[2023-09-10T14:42:27.557+0000] {processor.py:157} INFO - Started process (PID=90) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:42:27.563+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:42:27.571+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:42:27.570+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:42:27.748+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:42:27.800+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:42:27.800+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:42:27.833+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:42:27.833+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:42:27.858+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.325 seconds
[2023-09-10T14:42:58.669+0000] {processor.py:157} INFO - Started process (PID=100) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:42:58.677+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:42:58.695+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:42:58.689+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:42:59.100+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:42:59.412+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:42:59.411+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:42:59.451+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:42:59.451+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:42:59.493+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.874 seconds
[2023-09-10T14:43:29.978+0000] {processor.py:157} INFO - Started process (PID=110) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:43:29.988+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:43:30.006+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:43:30.004+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:43:30.390+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:43:30.507+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:43:30.501+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:43:30.588+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:43:30.588+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:43:30.636+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.684 seconds
[2023-09-10T14:44:01.458+0000] {processor.py:157} INFO - Started process (PID=120) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:44:01.466+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:44:01.488+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:44:01.486+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:44:01.763+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:44:02.028+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:44:02.026+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:44:02.086+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:44:02.086+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:44:02.127+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.686 seconds
[2023-09-10T14:44:32.579+0000] {processor.py:157} INFO - Started process (PID=137) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:44:32.585+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:44:32.596+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:44:32.594+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:44:32.854+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:44:32.958+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:44:32.957+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:44:33.018+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:44:33.018+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:44:33.061+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.492 seconds
[2023-09-10T14:45:03.701+0000] {processor.py:157} INFO - Started process (PID=147) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:45:03.706+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:45:03.720+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:45:03.719+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:45:03.906+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:45:04.125+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:45:04.125+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:45:04.184+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:45:04.184+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:45:04.226+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.534 seconds
[2023-09-10T14:45:34.766+0000] {processor.py:157} INFO - Started process (PID=158) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:45:34.773+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:45:34.793+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:45:34.792+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:45:35.186+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:45:35.365+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:45:35.363+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:45:35.441+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:45:35.440+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:45:35.491+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.738 seconds
[2023-09-10T14:46:06.219+0000] {processor.py:157} INFO - Started process (PID=169) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:46:06.226+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:46:06.238+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:46:06.237+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:46:06.432+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:46:06.647+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:46:06.645+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:46:06.687+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:46:06.687+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:46:06.712+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.506 seconds
[2023-09-10T14:46:37.150+0000] {processor.py:157} INFO - Started process (PID=179) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:46:37.156+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:46:37.169+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:46:37.168+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:46:37.353+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:46:37.434+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:46:37.433+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:46:37.473+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:46:37.472+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:46:37.497+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.357 seconds
[2023-09-10T14:54:30.256+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:54:30.274+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:54:30.290+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:54:30.287+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:54:31.718+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:54:33.230+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:54:33.219+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:54:33.360+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:54:33.359+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:54:33.456+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 3.219 seconds
[2023-09-10T14:55:03.853+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:55:03.859+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:55:03.865+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:55:03.864+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:55:04.195+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:55:04.258+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:55:04.257+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:55:04.286+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:55:04.286+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:55:04.305+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.460 seconds
[2023-09-10T14:55:35.207+0000] {processor.py:157} INFO - Started process (PID=49) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:55:35.214+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T14:55:35.227+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:55:35.224+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:55:36.434+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T14:55:36.753+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:55:36.752+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:55:36.776+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:55:36.776+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:55:36.800+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.604 seconds
[2023-09-10T15:07:17.878+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:07:17.884+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:07:17.913+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:07:17.911+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:07:19.198+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:07:19.671+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:07:19.668+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:07:19.830+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:07:19.830+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:07:20.632+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:07:20.556+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"dagrun_timeout": 60.0, "schedule_interval": "@daily", "_dag_id": "website_crawler", "timezone": "UTC", "edge_info": {}, "fi ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 15, 7, 19, 317435, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T15:07:20.644+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:07:20.644+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:07:20.660+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"dagrun_timeout": 60.0, "schedule_interval": "@daily", "_dag_id": "website_crawler", "timezone": "UTC", "edge_info": {}, "fi ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 15, 7, 19, 317435, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T15:07:51.014+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:07:51.026+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:07:51.035+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:07:51.034+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:07:51.394+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:07:51.536+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:07:51.536+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:07:51.559+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:07:51.559+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:07:51.594+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.595 seconds
[2023-09-10T15:08:21.959+0000] {processor.py:157} INFO - Started process (PID=50) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:08:21.965+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:08:21.977+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:08:21.976+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:08:23.307+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:08:23.406+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:08:23.406+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:08:23.441+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:08:23.441+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:08:23.475+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.547 seconds
[2023-09-10T15:08:53.768+0000] {processor.py:157} INFO - Started process (PID=60) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:08:53.774+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:08:53.784+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:08:53.782+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:08:54.099+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:08:54.303+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:08:54.302+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:08:54.337+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:08:54.337+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:08:54.395+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.638 seconds
[2023-09-10T15:09:25.339+0000] {processor.py:157} INFO - Started process (PID=70) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:09:25.343+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:09:25.352+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:09:25.351+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:09:25.640+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:09:25.716+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:09:25.715+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:09:25.769+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:09:25.769+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:09:25.808+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.478 seconds
[2023-09-10T15:22:11.391+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:22:11.402+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:22:11.450+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:22:11.447+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:22:12.548+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:22:13.071+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:22:13.069+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:22:13.150+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:22:13.149+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:22:13.227+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.850 seconds
[2023-09-10T15:22:43.802+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:22:43.807+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:22:43.818+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:22:43.815+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:22:44.502+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:22:44.572+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:22:44.571+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:22:44.607+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:22:44.607+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:22:44.647+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.854 seconds
[2023-09-10T15:23:15.108+0000] {processor.py:157} INFO - Started process (PID=51) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:23:15.113+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:23:15.119+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:23:15.118+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:23:15.575+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:23:15.879+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:23:15.878+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:23:15.936+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:23:15.936+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:23:15.981+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.879 seconds
[2023-09-10T15:23:47.597+0000] {processor.py:157} INFO - Started process (PID=61) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:23:47.617+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:23:47.634+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:23:47.633+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:23:48.010+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:23:48.111+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:23:48.110+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:23:48.176+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:23:48.176+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:23:48.236+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.673 seconds
[2023-09-10T15:24:18.550+0000] {processor.py:157} INFO - Started process (PID=71) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:24:18.559+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:24:18.578+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:24:18.576+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:24:18.902+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:24:19.282+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:24:19.279+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:24:19.378+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:24:19.378+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:24:19.446+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.915 seconds
[2023-09-10T15:24:49.757+0000] {processor.py:157} INFO - Started process (PID=81) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:24:49.766+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:24:49.780+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:24:49.778+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:24:50.035+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:24:50.153+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:24:50.151+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:24:50.210+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:24:50.210+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:24:50.243+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.499 seconds
[2023-09-10T15:25:21.237+0000] {processor.py:157} INFO - Started process (PID=91) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:25:21.251+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:25:21.270+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:25:21.268+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:25:21.674+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:25:21.996+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:25:21.994+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:25:22.055+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:25:22.055+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:25:22.147+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.994 seconds
[2023-09-10T15:25:52.977+0000] {processor.py:157} INFO - Started process (PID=101) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:25:53.023+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:25:53.047+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:25:53.044+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:25:53.684+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:25:53.844+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:25:53.839+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:25:54.064+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:25:54.063+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:25:54.217+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.304 seconds
[2023-09-10T15:26:24.661+0000] {processor.py:157} INFO - Started process (PID=118) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:26:24.670+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:26:24.689+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:26:24.687+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:26:25.000+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:26:25.276+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:26:25.275+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:26:25.318+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:26:25.317+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:26:25.352+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.717 seconds
[2023-09-10T15:26:56.032+0000] {processor.py:157} INFO - Started process (PID=129) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:26:56.046+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:26:56.054+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:26:56.053+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:26:56.334+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:26:56.505+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:26:56.503+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:26:56.626+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:26:56.625+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:26:56.690+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.666 seconds
[2023-09-10T15:27:27.044+0000] {processor.py:157} INFO - Started process (PID=139) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:27:27.050+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:27:27.059+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:27:27.058+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:27:27.260+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:27:27.436+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:27:27.436+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:27:27.466+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:27:27.466+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:27:27.505+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.468 seconds
[2023-09-10T15:27:57.884+0000] {processor.py:157} INFO - Started process (PID=149) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:27:57.889+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:27:57.896+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:27:57.895+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:27:58.077+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:27:58.188+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:27:58.186+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:27:58.270+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:27:58.270+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:27:58.353+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.476 seconds
[2023-09-10T15:28:29.165+0000] {processor.py:157} INFO - Started process (PID=158) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:28:29.169+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:28:29.174+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:28:29.173+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:28:29.316+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:28:29.601+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:28:29.600+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:28:29.635+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:28:29.634+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:28:29.660+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.500 seconds
[2023-09-10T15:28:59.969+0000] {processor.py:157} INFO - Started process (PID=168) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:28:59.973+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:28:59.982+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:28:59.982+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:29:00.171+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:29:00.238+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:29:00.237+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:29:00.300+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:29:00.299+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:29:00.340+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.378 seconds
[2023-09-10T15:29:30.846+0000] {processor.py:157} INFO - Started process (PID=178) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:29:30.850+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:29:30.857+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:29:30.856+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:29:31.026+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:29:31.368+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:29:31.367+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:29:31.393+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:29:31.392+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:29:31.414+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.574 seconds
[2023-09-10T15:30:01.819+0000] {processor.py:157} INFO - Started process (PID=187) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:30:01.824+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:30:01.831+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:30:01.830+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:30:01.986+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:30:02.062+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:30:02.061+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:30:02.116+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:30:02.116+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:30:02.149+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.337 seconds
[2023-09-10T15:30:32.840+0000] {processor.py:157} INFO - Started process (PID=196) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:30:32.844+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:30:32.850+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:30:32.850+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:30:32.993+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:30:33.285+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:30:33.284+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:30:33.318+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:30:33.317+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:30:33.346+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.511 seconds
[2023-09-10T15:31:03.647+0000] {processor.py:157} INFO - Started process (PID=206) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:31:03.653+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:31:03.661+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:31:03.660+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:31:03.788+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:31:03.841+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:31:03.839+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:31:03.870+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:31:03.870+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:31:03.897+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.256 seconds
[2023-09-10T15:31:34.376+0000] {processor.py:157} INFO - Started process (PID=216) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:31:34.396+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:31:34.430+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:31:34.428+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:31:35.041+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:31:35.479+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:31:35.478+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:31:35.510+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:31:35.509+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:31:35.536+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.182 seconds
[2023-09-10T15:32:05.886+0000] {processor.py:157} INFO - Started process (PID=227) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:32:05.897+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:32:05.914+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:32:05.912+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:32:06.414+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:32:06.517+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:32:06.516+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:32:06.585+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:32:06.584+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:32:06.630+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.767 seconds
[2023-09-10T15:32:36.771+0000] {processor.py:157} INFO - Started process (PID=237) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:32:36.775+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:32:36.782+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:32:36.781+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:32:37.037+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:32:37.774+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:32:37.770+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:32:37.866+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:32:37.866+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:32:37.931+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.165 seconds
[2023-09-10T15:33:08.624+0000] {processor.py:157} INFO - Started process (PID=247) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:33:08.640+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:33:08.653+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:33:08.652+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:33:08.802+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:33:08.872+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:33:08.872+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:33:08.927+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:33:08.927+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:33:08.966+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.351 seconds
[2023-09-10T15:33:39.422+0000] {processor.py:157} INFO - Started process (PID=258) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:33:39.426+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:33:39.433+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:33:39.432+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:33:39.765+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:33:40.580+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:33:40.579+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:33:40.629+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:33:40.628+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:33:40.665+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.264 seconds
[2023-09-10T15:34:11.049+0000] {processor.py:157} INFO - Started process (PID=268) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:34:11.059+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:34:11.071+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:34:11.070+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:34:11.234+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:34:11.377+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:34:11.373+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:34:11.470+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:34:11.469+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:34:11.533+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.499 seconds
[2023-09-10T15:34:42.126+0000] {processor.py:157} INFO - Started process (PID=278) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:34:42.131+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:34:42.140+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:34:42.139+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:34:42.285+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:34:42.519+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:34:42.518+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:34:42.545+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:34:42.544+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:34:42.569+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.456 seconds
[2023-09-10T15:35:12.758+0000] {processor.py:157} INFO - Started process (PID=288) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:35:12.766+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:35:12.778+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:35:12.777+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:35:13.104+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:35:13.278+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:35:13.276+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:35:13.369+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:35:13.368+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:35:13.451+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.712 seconds
[2023-09-10T15:39:38.217+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:39:38.229+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:39:38.244+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:39:38.242+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:39:39.189+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:39:39.723+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:39:39.722+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:39:39.864+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:39:39.864+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:39:40.116+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:39:40.056+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"_task_group": {"_group_id": null, "prefix_group_id": true, "tooltip": "", "ui_color": "CornflowerBlue", "ui_fgcolor": "#000 ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 15, 39, 39, 276562, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T15:39:40.142+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:39:40.141+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:39:40.155+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"_task_group": {"_group_id": null, "prefix_group_id": true, "tooltip": "", "ui_color": "CornflowerBlue", "ui_fgcolor": "#000 ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 15, 39, 39, 276562, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T15:40:10.667+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:40:10.672+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:40:10.683+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:40:10.681+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:40:10.856+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:40:11.050+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:40:11.049+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:40:11.075+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:40:11.074+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:40:11.107+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.449 seconds
[2023-09-10T15:40:41.733+0000] {processor.py:157} INFO - Started process (PID=51) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:40:41.739+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:40:41.753+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:40:41.750+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:40:42.047+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:40:42.174+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:40:42.173+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:40:42.265+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:40:42.265+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:40:42.335+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.614 seconds
[2023-09-10T15:41:13.292+0000] {processor.py:157} INFO - Started process (PID=61) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:41:13.303+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:41:13.333+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:41:13.324+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:41:14.184+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:41:15.549+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:41:15.543+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:41:15.640+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:41:15.640+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:41:15.704+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.451 seconds
[2023-09-10T15:41:45.911+0000] {processor.py:157} INFO - Started process (PID=72) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:41:45.917+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:41:45.928+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:41:45.927+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:41:46.378+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:41:46.533+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:41:46.532+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:41:46.634+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:41:46.633+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:41:46.705+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.820 seconds
[2023-09-10T15:42:17.275+0000] {processor.py:157} INFO - Started process (PID=81) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:42:17.285+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:42:17.306+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:42:17.304+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:42:17.852+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:42:18.268+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:42:18.261+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:42:18.343+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:42:18.343+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:42:18.399+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.153 seconds
[2023-09-10T15:42:49.184+0000] {processor.py:157} INFO - Started process (PID=98) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:42:49.197+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:42:49.223+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:42:49.221+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:42:49.549+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:42:49.636+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:42:49.635+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:42:49.710+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:42:49.709+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:42:49.751+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.598 seconds
[2023-09-10T15:43:20.367+0000] {processor.py:157} INFO - Started process (PID=108) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:43:20.384+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:43:20.434+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:43:20.433+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:43:20.942+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:43:21.545+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:43:21.544+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:43:21.597+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:43:21.597+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:43:21.633+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.295 seconds
[2023-09-10T15:43:52.109+0000] {processor.py:157} INFO - Started process (PID=119) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:43:52.132+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:43:52.152+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:43:52.149+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:43:52.692+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:43:52.894+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:43:52.891+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:43:53.242+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:43:53.238+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:43:53.336+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.256 seconds
[2023-09-10T15:44:23.678+0000] {processor.py:157} INFO - Started process (PID=129) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:44:23.686+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:44:23.697+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:44:23.695+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:44:23.882+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:44:24.167+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:44:24.166+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:44:24.207+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:44:24.207+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:44:24.239+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.571 seconds
[2023-09-10T15:44:54.536+0000] {processor.py:157} INFO - Started process (PID=139) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:44:54.545+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:44:54.564+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:44:54.563+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:44:54.946+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:44:55.053+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:44:55.052+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:44:55.142+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:44:55.142+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:44:55.191+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.673 seconds
[2023-09-10T15:45:26.042+0000] {processor.py:157} INFO - Started process (PID=149) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:45:26.052+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:45:26.063+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:45:26.062+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:45:26.329+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:45:26.699+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:45:26.698+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:45:26.751+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:45:26.751+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:45:26.795+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.779 seconds
[2023-09-10T15:48:54.711+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:48:54.738+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:48:54.775+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:48:54.769+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:48:56.093+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:48:56.672+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:48:56.670+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:48:56.746+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:48:56.745+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:48:56.832+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.220 seconds
[2023-09-10T15:49:27.197+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:49:27.201+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:49:27.207+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:49:27.206+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:49:27.583+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:49:27.639+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:49:27.639+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:49:27.676+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:49:27.676+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:49:27.709+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.518 seconds
[2023-09-10T15:49:58.413+0000] {processor.py:157} INFO - Started process (PID=50) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:49:58.420+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:49:58.460+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:49:58.458+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:49:59.920+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:50:00.365+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:50:00.364+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:50:00.502+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:50:00.501+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:50:00.573+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.176 seconds
[2023-09-10T15:50:31.487+0000] {processor.py:157} INFO - Started process (PID=60) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:50:31.497+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:50:31.509+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:50:31.508+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:50:31.893+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:50:32.080+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:50:32.078+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:50:32.172+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:50:32.172+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:50:32.222+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.747 seconds
[2023-09-10T15:51:03.379+0000] {processor.py:157} INFO - Started process (PID=70) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:51:03.392+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:51:03.423+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:51:03.422+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:51:04.360+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:51:05.534+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:51:05.524+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:51:05.642+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:51:05.641+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:51:05.735+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.416 seconds
[2023-09-10T15:51:35.981+0000] {processor.py:157} INFO - Started process (PID=79) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:51:35.995+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:51:36.009+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:51:36.007+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:51:36.295+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:51:36.436+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:51:36.433+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:51:36.500+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:51:36.500+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:51:36.543+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.573 seconds
[2023-09-10T15:52:07.295+0000] {processor.py:157} INFO - Started process (PID=89) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:52:07.301+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:52:07.308+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:52:07.307+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:52:07.439+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:52:07.614+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:52:07.613+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:52:07.658+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:52:07.658+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:52:07.683+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.396 seconds
[2023-09-10T15:52:38.112+0000] {processor.py:157} INFO - Started process (PID=99) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:52:38.117+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:52:38.128+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:52:38.127+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:52:38.321+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:52:38.381+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:52:38.380+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:52:38.446+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:52:38.446+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:52:38.496+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.396 seconds
[2023-09-10T15:53:09.425+0000] {processor.py:157} INFO - Started process (PID=115) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:53:09.433+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:53:09.458+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:53:09.456+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:53:09.824+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:53:10.289+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:53:10.288+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:53:10.353+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:53:10.352+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:53:10.409+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.017 seconds
[2023-09-10T15:53:40.999+0000] {processor.py:157} INFO - Started process (PID=125) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:53:41.008+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:53:41.024+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:53:41.023+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:53:41.522+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:53:41.941+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:53:41.929+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:53:42.056+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:53:42.056+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:53:42.133+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.147 seconds
[2023-09-10T15:55:34.118+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:55:34.131+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:55:34.162+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:55:34.161+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:55:35.212+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:55:35.624+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:55:35.623+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:55:35.690+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:55:35.689+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:55:36.033+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:55:36.010+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"_dag_id": "website_crawler", "schedule_interval": "@daily", "dataset_triggers": [], "fileloc": "/opt/airflow/dags/crawl_dat ... (2040 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 15, 55, 35, 304320, tzinfo=Timezone('UTC')), 'dag_hash': '1fea61bac30761a6c1f9aa9bc06ee525', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T15:55:36.041+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:55:36.041+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:55:36.047+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"_dag_id": "website_crawler", "schedule_interval": "@daily", "dataset_triggers": [], "fileloc": "/opt/airflow/dags/crawl_dat ... (2040 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 15, 55, 35, 304320, tzinfo=Timezone('UTC')), 'dag_hash': '1fea61bac30761a6c1f9aa9bc06ee525', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T15:56:06.608+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:56:06.624+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:56:06.647+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:56:06.640+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:56:10.682+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:56:12.084+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:56:12.069+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:56:12.202+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:56:12.201+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:56:12.341+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 5.766 seconds
[2023-09-10T15:56:43.232+0000] {processor.py:157} INFO - Started process (PID=51) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:56:43.241+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:56:43.254+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:56:43.252+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:56:43.901+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:56:43.959+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:56:43.958+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:56:44.006+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:56:44.006+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:56:44.039+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.821 seconds
[2023-09-10T15:57:14.874+0000] {processor.py:157} INFO - Started process (PID=62) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:57:14.879+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:57:14.898+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:57:14.897+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:57:15.218+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:57:15.455+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:57:15.454+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:57:15.482+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:57:15.481+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:57:15.510+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.647 seconds
[2023-09-10T15:57:46.621+0000] {processor.py:157} INFO - Started process (PID=72) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:57:46.782+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:57:46.862+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:57:46.859+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:57:47.582+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:57:47.671+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:57:47.670+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:57:47.800+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:57:47.797+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:57:47.864+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.330 seconds
[2023-09-10T15:58:19.001+0000] {processor.py:157} INFO - Started process (PID=89) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:58:19.113+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T15:58:19.155+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:58:19.152+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:58:19.665+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T15:58:21.375+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:58:21.366+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:58:21.492+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:58:21.491+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:58:21.580+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.762 seconds
[2023-09-10T16:05:22.527+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:05:22.544+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:05:22.555+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:05:22.553+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:05:23.738+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:05:24.178+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:05:24.176+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:05:24.420+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:05:24.419+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:05:24.694+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:05:24.671+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"dataset_triggers": [], "fileloc": "/opt/airflow/dags/crawl_data_log_into_postgres.py", "dagrun_timeout": 60.0, "default_arg ... (1934 characters truncated) ...  NULL,\\n    date CHAR(5000) NOT NULL,\\n    time CHAR(5000) NOT NULL,\\n    summary CHAR(5000) NOT NULL);"}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 16, 5, 23, 854655, tzinfo=Timezone('UTC')), 'dag_hash': 'f6b07b979a7a6ceeff49f11494f1e617', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T16:05:24.712+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:05:24.712+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:05:24.718+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"dataset_triggers": [], "fileloc": "/opt/airflow/dags/crawl_data_log_into_postgres.py", "dagrun_timeout": 60.0, "default_arg ... (1934 characters truncated) ...  NULL,\\n    date CHAR(5000) NOT NULL,\\n    time CHAR(5000) NOT NULL,\\n    summary CHAR(5000) NOT NULL);"}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 16, 5, 23, 854655, tzinfo=Timezone('UTC')), 'dag_hash': 'f6b07b979a7a6ceeff49f11494f1e617', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T16:05:55.374+0000] {processor.py:157} INFO - Started process (PID=39) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:05:55.386+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:05:55.402+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:05:55.401+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:05:55.804+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:05:55.987+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:05:55.985+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:05:56.033+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:05:56.033+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:05:56.070+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.707 seconds
[2023-09-10T16:06:26.607+0000] {processor.py:157} INFO - Started process (PID=53) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:06:26.616+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:06:26.646+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:06:26.643+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:06:29.913+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:06:30.135+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:06:30.130+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:06:30.237+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:06:30.235+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:06:30.299+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 3.709 seconds
[2023-09-10T16:07:01.031+0000] {processor.py:157} INFO - Started process (PID=62) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:07:01.044+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:07:01.059+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:07:01.058+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:07:01.262+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:07:01.638+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:07:01.637+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:07:01.680+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:07:01.680+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:07:01.715+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.699 seconds
[2023-09-10T16:07:10.780+0000] {processor.py:157} INFO - Started process (PID=72) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:07:10.786+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:07:10.794+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:07:10.792+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:07:10.962+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:07:11.004+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:07:11.004+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:07:11.035+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:07:11.035+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:07:11.058+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.297 seconds
[2023-09-10T16:07:18.259+0000] {processor.py:157} INFO - Started process (PID=73) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:07:18.275+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:07:18.286+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:07:18.285+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:07:18.512+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:07:18.554+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:07:18.553+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:07:18.591+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:07:18.590+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:07:18.635+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.387 seconds
[2023-09-10T16:08:37.500+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:08:37.515+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:08:37.527+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:08:37.526+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:08:38.315+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:08:38.725+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:08:38.724+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:08:38.792+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:08:38.792+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:08:39.150+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:08:39.142+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"schedule_interval": "@daily", "dagrun_timeout": 60.0, "dataset_triggers": [], "edge_info": {}, "fileloc": "/opt/airflow/dag ... (1934 characters truncated) ...  NULL,\\n    date CHAR(5000) NOT NULL,\\n    time CHAR(5000) NOT NULL,\\n    summary CHAR(5000) NOT NULL);"}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 16, 8, 38, 399619, tzinfo=Timezone('UTC')), 'dag_hash': 'f6b07b979a7a6ceeff49f11494f1e617', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T16:08:39.163+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:08:39.163+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:08:39.170+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"schedule_interval": "@daily", "dagrun_timeout": 60.0, "dataset_triggers": [], "edge_info": {}, "fileloc": "/opt/airflow/dag ... (1934 characters truncated) ...  NULL,\\n    date CHAR(5000) NOT NULL,\\n    time CHAR(5000) NOT NULL,\\n    summary CHAR(5000) NOT NULL);"}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 16, 8, 38, 399619, tzinfo=Timezone('UTC')), 'dag_hash': 'f6b07b979a7a6ceeff49f11494f1e617', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T16:09:11.301+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:09:11.333+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:09:11.407+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:09:11.394+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:09:15.491+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:09:17.302+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:09:17.275+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:09:17.856+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:09:17.836+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:09:19.173+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 8.076 seconds
[2023-09-10T16:09:50.359+0000] {processor.py:157} INFO - Started process (PID=51) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:09:50.367+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:09:50.381+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:09:50.379+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:09:51.740+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:09:51.844+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:09:51.841+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:09:51.937+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:09:51.936+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:09:51.994+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.658 seconds
[2023-09-10T16:10:22.228+0000] {processor.py:157} INFO - Started process (PID=61) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:10:22.233+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:10:22.241+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:10:22.240+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:10:22.373+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:10:22.528+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:10:22.528+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:10:22.563+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:10:22.562+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:10:22.595+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.374 seconds
[2023-09-10T16:42:03.612+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:42:03.620+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:42:03.644+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:42:03.643+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:42:04.515+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:42:04.981+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:42:04.978+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:42:05.064+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:42:05.063+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:42:05.152+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.558 seconds
[2023-09-10T16:42:36.214+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:42:36.220+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:42:36.228+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:42:36.227+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:42:36.603+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:42:36.651+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:42:36.650+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:42:36.694+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:42:36.693+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:42:36.723+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.519 seconds
[2023-09-10T16:43:07.450+0000] {processor.py:157} INFO - Started process (PID=50) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:43:07.458+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:43:07.470+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:43:07.468+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:43:08.811+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:43:09.313+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:43:09.309+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:43:09.357+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:43:09.356+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:43:09.418+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.990 seconds
[2023-09-10T16:43:39.968+0000] {processor.py:157} INFO - Started process (PID=60) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:43:39.990+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:43:40.048+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:43:40.030+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:43:40.662+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:43:40.804+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:43:40.802+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:43:40.932+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:43:40.931+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:43:41.227+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.277 seconds
[2023-09-10T16:44:12.309+0000] {processor.py:157} INFO - Started process (PID=69) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:44:12.314+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:44:12.322+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:44:12.322+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:44:12.477+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:44:12.678+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:44:12.678+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:44:12.706+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:44:12.705+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:44:12.728+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.425 seconds
[2023-09-10T16:44:43.030+0000] {processor.py:157} INFO - Started process (PID=79) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:44:43.033+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:44:43.040+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:44:43.039+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:44:43.189+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:44:43.240+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:44:43.239+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:44:43.272+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:44:43.271+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:44:43.298+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.276 seconds
[2023-09-10T16:53:17.724+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:53:17.744+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:53:17.759+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:53:17.758+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:53:18.609+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:53:19.051+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:53:19.049+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:53:19.129+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:53:19.128+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:53:19.195+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.485 seconds
[2023-09-10T16:53:49.687+0000] {processor.py:157} INFO - Started process (PID=39) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:53:49.690+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:53:49.697+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:53:49.697+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:53:50.044+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:53:50.100+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:53:50.099+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:53:50.127+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:53:50.127+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:53:50.144+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.464 seconds
[2023-09-10T16:54:20.660+0000] {processor.py:157} INFO - Started process (PID=49) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:54:20.666+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:54:20.679+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:54:20.676+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:54:21.683+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:54:22.696+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:54:22.622+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:54:23.276+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:54:23.274+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:54:23.548+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.899 seconds
[2023-09-10T16:54:55.130+0000] {processor.py:157} INFO - Started process (PID=59) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:54:55.139+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:54:55.152+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:54:55.151+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:54:55.602+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:54:55.765+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:54:55.763+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:54:55.867+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:54:55.866+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:54:55.976+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.863 seconds
[2023-09-10T16:55:27.123+0000] {processor.py:157} INFO - Started process (PID=70) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:55:27.129+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:55:27.137+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:55:27.137+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:55:27.371+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:55:27.690+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:55:27.687+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:55:27.764+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:55:27.763+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:55:27.818+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.702 seconds
[2023-09-10T16:55:58.399+0000] {processor.py:157} INFO - Started process (PID=80) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:55:58.404+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:55:58.412+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:55:58.411+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:55:58.542+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:55:58.593+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:55:58.592+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:55:58.638+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:55:58.638+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:55:58.677+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.291 seconds
[2023-09-10T16:56:29.620+0000] {processor.py:157} INFO - Started process (PID=91) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:56:29.629+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:56:29.652+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:56:29.651+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:56:30.090+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:56:30.715+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:56:30.714+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:56:30.754+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:56:30.754+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:56:30.793+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.189 seconds
[2023-09-10T16:57:01.447+0000] {processor.py:157} INFO - Started process (PID=100) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:57:01.454+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:57:01.476+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:57:01.474+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:57:02.258+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:57:02.416+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:57:02.414+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:57:02.542+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:57:02.541+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:57:02.615+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.191 seconds
[2023-09-10T16:57:33.222+0000] {processor.py:157} INFO - Started process (PID=118) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:57:33.227+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:57:33.236+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:57:33.235+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:57:33.442+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:57:33.776+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:57:33.775+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:57:33.826+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:57:33.826+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:57:33.873+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.659 seconds
[2023-09-10T16:58:04.437+0000] {processor.py:157} INFO - Started process (PID=128) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:58:04.446+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T16:58:04.465+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:58:04.463+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:58:05.004+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T16:58:05.225+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:58:05.224+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:58:05.346+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:58:05.345+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:58:05.437+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.020 seconds
[2023-09-10T17:01:48.177+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:01:48.185+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:01:48.203+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:01:48.201+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:01:51.086+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:01:51.823+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:01:51.819+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:01:51.992+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:01:51.992+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:01:52.082+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:01:52.070+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"schedule_interval": "@daily", "_task_group": {"_group_id": null, "prefix_group_id": true, "tooltip": "", "ui_color": "Cornf ... (1106 characters truncated) ... ders.postgres.operators.postgres", "_is_empty": false, "sql": "-- drop table\\nDROP TABLE IF EXISTS test2;"}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 17, 1, 51, 198476, tzinfo=Timezone('UTC')), 'dag_hash': 'fa1a1a9ed0d1402164522cfbea960ff0', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T17:01:52.092+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:01:52.092+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:01:52.105+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"schedule_interval": "@daily", "_task_group": {"_group_id": null, "prefix_group_id": true, "tooltip": "", "ui_color": "Cornf ... (1106 characters truncated) ... ders.postgres.operators.postgres", "_is_empty": false, "sql": "-- drop table\\nDROP TABLE IF EXISTS test2;"}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 17, 1, 51, 198476, tzinfo=Timezone('UTC')), 'dag_hash': 'fa1a1a9ed0d1402164522cfbea960ff0', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T17:02:22.846+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:02:22.850+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:02:22.858+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:02:22.857+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:02:23.488+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:02:23.774+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:02:23.773+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:02:23.821+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:02:23.820+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:02:23.877+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.040 seconds
[2023-09-10T17:02:54.548+0000] {processor.py:157} INFO - Started process (PID=50) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:02:54.558+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:02:54.585+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:02:54.581+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:02:55.556+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:02:55.658+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:02:55.655+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:02:55.735+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:02:55.735+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:02:55.790+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.282 seconds
[2023-09-10T17:03:26.666+0000] {processor.py:157} INFO - Started process (PID=60) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:03:26.671+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:03:26.680+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:03:26.680+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:03:26.919+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:03:27.298+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:03:27.297+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:03:27.345+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:03:27.344+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:03:27.386+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.727 seconds
[2023-09-10T17:03:57.930+0000] {processor.py:157} INFO - Started process (PID=70) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:03:57.938+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:03:57.957+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:03:57.955+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:03:58.208+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:03:58.292+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:03:58.290+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:03:58.371+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:03:58.371+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:03:58.414+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.502 seconds
[2023-09-10T17:04:29.507+0000] {processor.py:157} INFO - Started process (PID=87) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:04:29.529+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:04:29.580+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:04:29.574+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:04:30.140+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:04:30.482+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:04:30.481+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:04:30.538+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:04:30.538+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:04:30.624+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.178 seconds
[2023-09-10T17:05:01.193+0000] {processor.py:157} INFO - Started process (PID=97) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:05:01.205+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:05:01.217+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:05:01.216+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:05:01.391+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:05:01.525+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:05:01.523+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:05:01.626+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:05:01.625+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:05:01.718+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.544 seconds
[2023-09-10T17:05:32.430+0000] {processor.py:157} INFO - Started process (PID=106) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:05:32.438+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:05:32.458+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:05:32.457+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:05:32.629+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:05:32.929+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:05:32.928+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:05:32.996+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:05:32.995+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:05:33.048+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.628 seconds
[2023-09-10T17:06:03.968+0000] {processor.py:157} INFO - Started process (PID=116) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:06:03.973+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:06:03.987+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:06:03.986+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:06:04.304+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:06:04.437+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:06:04.435+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:06:04.500+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:06:04.499+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:06:04.538+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.582 seconds
[2023-09-10T17:06:35.562+0000] {processor.py:157} INFO - Started process (PID=126) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:06:35.570+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:06:35.580+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:06:35.578+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:06:36.061+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:06:36.451+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:06:36.449+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:06:36.487+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:06:36.487+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:06:36.533+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.988 seconds
[2023-09-10T17:07:08.061+0000] {processor.py:157} INFO - Started process (PID=137) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:07:08.147+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:07:08.199+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:07:08.193+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:07:09.507+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:07:09.674+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:07:09.673+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:07:09.772+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:07:09.772+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:07:09.848+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.893 seconds
[2023-09-10T17:07:40.933+0000] {processor.py:157} INFO - Started process (PID=146) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:07:40.941+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:07:40.970+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:07:40.969+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:07:41.273+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:07:41.636+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:07:41.635+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:07:41.682+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:07:41.682+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:07:41.724+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.820 seconds
[2023-09-10T17:08:12.895+0000] {processor.py:157} INFO - Started process (PID=156) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:08:12.911+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:08:12.940+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:08:12.939+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:08:13.276+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:08:13.396+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:08:13.395+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:08:13.462+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:08:13.462+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:08:13.506+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.660 seconds
[2023-09-10T17:08:44.464+0000] {processor.py:157} INFO - Started process (PID=166) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:08:44.480+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:08:44.495+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:08:44.493+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:08:44.803+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:08:45.242+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:08:45.239+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:08:45.340+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:08:45.339+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:08:45.384+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.938 seconds
[2023-09-10T17:09:16.671+0000] {processor.py:157} INFO - Started process (PID=176) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:09:16.697+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:09:16.771+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:09:16.768+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:09:17.339+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:09:17.538+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:09:17.533+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:09:17.675+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:09:17.674+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:09:17.772+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.192 seconds
[2023-09-10T17:09:48.857+0000] {processor.py:157} INFO - Started process (PID=185) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:09:48.866+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:09:48.914+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:09:48.909+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:09:49.361+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:09:49.809+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:09:49.808+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:09:49.873+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:09:49.873+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:09:49.936+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.108 seconds
[2023-09-10T17:10:20.741+0000] {processor.py:157} INFO - Started process (PID=195) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:10:20.751+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:10:20.775+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:10:20.772+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:10:21.349+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:10:21.542+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:10:21.540+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:10:21.676+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:10:21.675+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:10:21.739+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.067 seconds
[2023-09-10T17:10:51.900+0000] {processor.py:157} INFO - Started process (PID=205) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:10:51.905+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:10:51.923+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:10:51.919+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:10:52.070+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:10:52.249+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:10:52.249+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:10:52.289+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:10:52.289+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:10:52.321+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.431 seconds
[2023-09-10T17:11:23.030+0000] {processor.py:157} INFO - Started process (PID=215) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:11:23.036+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:11:23.045+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:11:23.045+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:11:23.272+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:11:23.357+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:11:23.356+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:11:23.415+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:11:23.414+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:11:23.465+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.446 seconds
[2023-09-10T17:11:54.262+0000] {processor.py:157} INFO - Started process (PID=225) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:11:54.274+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:11:54.311+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:11:54.309+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:11:54.829+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:11:55.475+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:11:55.474+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:11:55.571+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:11:55.571+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:11:55.662+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.438 seconds
[2023-09-10T17:12:26.289+0000] {processor.py:157} INFO - Started process (PID=235) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:12:26.301+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:12:26.332+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:12:26.330+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:12:26.996+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:12:27.193+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:12:27.190+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:12:27.311+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:12:27.310+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:12:27.371+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.128 seconds
[2023-09-10T17:21:42.379+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:21:42.399+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:21:42.414+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:21:42.412+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:21:43.343+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:21:44.092+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:21:44.089+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:21:44.191+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:21:44.191+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:21:44.273+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.924 seconds
[2023-09-10T17:22:14.892+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:22:14.898+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:22:14.927+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:22:14.922+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:22:15.466+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:22:15.507+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:22:15.506+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:22:15.531+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:22:15.531+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:22:15.548+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.664 seconds
[2023-09-10T17:22:45.774+0000] {processor.py:157} INFO - Started process (PID=50) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:22:45.781+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:22:45.792+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:22:45.791+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:22:46.522+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:22:46.739+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:22:46.739+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:22:46.802+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:22:46.801+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:22:46.842+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.078 seconds
[2023-09-10T17:23:17.042+0000] {processor.py:157} INFO - Started process (PID=59) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:23:17.052+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:23:17.067+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:23:17.066+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:23:17.455+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:23:17.729+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:23:17.727+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:23:18.237+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:23:18.232+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:23:18.384+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.363 seconds
[2023-09-10T17:25:01.685+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:25:01.692+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:25:01.704+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:25:01.703+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:25:02.432+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:25:02.916+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:25:02.915+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:25:02.987+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:25:02.986+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:25:03.094+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.433 seconds
[2023-09-10T17:25:04.253+0000] {processor.py:157} INFO - Started process (PID=33) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:25:04.261+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:25:04.273+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:25:04.271+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:25:05.309+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:25:05.418+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:25:05.417+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:25:05.487+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:25:05.487+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:25:05.549+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.352 seconds
[2023-09-10T17:25:35.972+0000] {processor.py:157} INFO - Started process (PID=43) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:25:35.978+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:25:35.987+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:25:35.985+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:25:36.227+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:25:36.250+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:25:36.249+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:25:36.284+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:25:36.283+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:25:36.303+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.337 seconds
[2023-09-10T17:26:07.756+0000] {processor.py:157} INFO - Started process (PID=53) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:26:07.766+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:26:07.783+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:26:07.780+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:26:09.932+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:26:11.279+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:26:11.274+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:26:11.350+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:26:11.349+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:26:11.416+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 3.675 seconds
[2023-09-10T17:26:41.673+0000] {processor.py:157} INFO - Started process (PID=63) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:26:41.679+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:26:41.689+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:26:41.688+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:26:41.953+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:26:42.031+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:26:42.030+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:26:42.113+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:26:42.113+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:26:42.154+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.492 seconds
[2023-09-10T17:27:00.001+0000] {processor.py:157} INFO - Started process (PID=71) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:27:00.014+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:27:00.077+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:27:00.074+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:27:00.631+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:27:00.849+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:27:00.847+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:27:00.951+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:27:00.950+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:27:01.022+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.064 seconds
[2023-09-10T17:27:32.058+0000] {processor.py:157} INFO - Started process (PID=80) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:27:32.066+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:27:32.078+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:27:32.077+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:27:32.331+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:27:32.407+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:27:32.405+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:27:32.488+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:27:32.488+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:27:32.542+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.507 seconds
[2023-09-10T17:28:02.718+0000] {processor.py:157} INFO - Started process (PID=89) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:28:02.724+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:28:02.733+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:28:02.732+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:28:02.943+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:28:03.181+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:28:03.181+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:28:03.222+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:28:03.221+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:28:03.249+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.547 seconds
[2023-09-10T17:28:33.392+0000] {processor.py:157} INFO - Started process (PID=99) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:28:33.398+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:28:33.405+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:28:33.405+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:28:33.576+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:28:33.637+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:28:33.637+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:28:33.677+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:28:33.677+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:28:33.700+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.314 seconds
[2023-09-10T17:29:04.091+0000] {processor.py:157} INFO - Started process (PID=108) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:29:04.097+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:29:04.104+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:29:04.103+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:29:04.272+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:29:04.326+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:29:04.325+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:29:04.395+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:29:04.395+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:29:04.426+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.344 seconds
[2023-09-10T17:29:35.345+0000] {processor.py:157} INFO - Started process (PID=117) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:29:35.353+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:29:35.368+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:29:35.367+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:29:35.639+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:29:35.981+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:29:35.980+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:29:36.029+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:29:36.028+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:29:36.068+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.736 seconds
[2023-09-10T17:30:06.199+0000] {processor.py:157} INFO - Started process (PID=127) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:30:06.203+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:30:06.213+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:30:06.212+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:30:06.420+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:30:06.530+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:30:06.529+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:30:06.603+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:30:06.603+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:30:06.642+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.455 seconds
[2023-09-10T17:30:23.318+0000] {processor.py:157} INFO - Started process (PID=137) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:30:23.326+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:30:23.343+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:30:23.340+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:30:23.635+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:30:23.698+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:30:23.696+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:30:23.791+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:30:23.791+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:30:23.834+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.528 seconds
[2023-09-10T17:32:20.120+0000] {processor.py:157} INFO - Started process (PID=29) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:32:20.133+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:32:20.176+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:32:20.174+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:32:21.496+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:32:23.756+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:32:23.742+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:32:24.052+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:32:24.051+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:32:24.303+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:32:24.294+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"edge_info": {}, "_dag_id": "website_crawler", "fileloc": "/opt/airflow/dags/crawl_data_log_into_postgres.py", "dagrun_timeo ... (2570 characters truncated) ... ders.postgres.operators.postgres", "_is_empty": false, "sql": "-- drop table\\nDROP TABLE IF EXISTS test2;"}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 17, 32, 21, 649907, tzinfo=Timezone('UTC')), 'dag_hash': '713bc551e844e4f24fdff36c8fc2b73a', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T17:32:24.312+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:32:24.312+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:32:24.320+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"edge_info": {}, "_dag_id": "website_crawler", "fileloc": "/opt/airflow/dags/crawl_data_log_into_postgres.py", "dagrun_timeo ... (2570 characters truncated) ... ders.postgres.operators.postgres", "_is_empty": false, "sql": "-- drop table\\nDROP TABLE IF EXISTS test2;"}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 17, 32, 21, 649907, tzinfo=Timezone('UTC')), 'dag_hash': '713bc551e844e4f24fdff36c8fc2b73a', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T17:32:54.790+0000] {processor.py:157} INFO - Started process (PID=39) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:32:54.800+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:32:54.809+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:32:54.808+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:32:55.297+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:32:55.691+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:32:55.690+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:32:55.746+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:32:55.746+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:32:55.817+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.040 seconds
[2023-09-10T17:33:26.406+0000] {processor.py:157} INFO - Started process (PID=48) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:33:26.421+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:33:26.474+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:33:26.451+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:33:32.536+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:33:32.813+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:33:32.809+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:33:32.988+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:33:32.986+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:33:33.098+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 6.734 seconds
[2023-09-10T17:34:04.099+0000] {processor.py:157} INFO - Started process (PID=66) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:34:04.108+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:34:04.125+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:34:04.124+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:34:04.448+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:34:04.829+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:34:04.828+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:34:04.883+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:34:04.881+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:34:04.940+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.875 seconds
[2023-09-10T17:34:35.230+0000] {processor.py:157} INFO - Started process (PID=77) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:34:35.246+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:34:35.264+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:34:35.261+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:34:35.547+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:34:35.693+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:34:35.693+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:34:35.765+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:34:35.764+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:34:35.815+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.614 seconds
[2023-09-10T17:35:06.927+0000] {processor.py:157} INFO - Started process (PID=86) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:35:06.934+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:35:06.947+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:35:06.946+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:35:07.254+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:35:07.624+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:35:07.623+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:35:07.677+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:35:07.676+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:35:07.734+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.829 seconds
[2023-09-10T17:41:22.132+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:41:22.148+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:41:22.169+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:41:22.168+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:41:22.976+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:41:23.438+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:41:23.437+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:41:23.510+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:41:23.510+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:41:23.607+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.489 seconds
[2023-09-10T17:41:53.850+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:41:53.856+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:41:53.868+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:41:53.867+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:41:54.246+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:41:54.360+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:41:54.359+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:41:54.412+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:41:54.411+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:41:54.448+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.606 seconds
[2023-09-10T17:42:28.547+0000] {processor.py:157} INFO - Started process (PID=52) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:42:29.081+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:42:29.551+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:42:29.465+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:42:34.194+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:42:35.609+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:42:35.594+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:42:36.275+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:42:36.263+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:42:36.487+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 8.081 seconds
[2023-09-10T17:43:07.307+0000] {processor.py:157} INFO - Started process (PID=71) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:43:07.319+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:43:07.348+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:43:07.346+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:43:10.230+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:43:12.985+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:43:12.962+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:43:16.433+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:43:16.405+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:43:17.016+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 9.737 seconds
[2023-09-10T17:43:48.304+0000] {processor.py:157} INFO - Started process (PID=80) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:43:48.310+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:43:48.320+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:43:48.320+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:43:48.703+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:43:49.035+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:43:49.034+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:43:49.078+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:43:49.077+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:43:49.114+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.820 seconds
[2023-09-10T17:44:03.183+0000] {processor.py:157} INFO - Started process (PID=83) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:44:03.192+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:44:03.205+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:44:03.203+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:44:03.552+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:44:03.654+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:44:03.653+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:44:03.852+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:44:03.851+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:44:03.988+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.824 seconds
[2023-09-10T17:44:34.748+0000] {processor.py:157} INFO - Started process (PID=93) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:44:34.755+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-10T17:44:34.767+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:44:34.765+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:44:35.057+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-10T17:44:35.198+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:44:35.197+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:44:35.328+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:44:35.327+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:44:35.389+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.663 seconds
