[2023-09-10T13:02:17.160+0000] {processor.py:157} INFO - Started process (PID=35) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:02:17.163+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:02:17.174+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:02:17.171+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:02:17.877+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:02:18.296+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:02:18.295+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:02:18.356+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:02:18.356+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:02:18.399+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.244 seconds
[2023-09-10T13:02:49.504+0000] {processor.py:157} INFO - Started process (PID=57) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:02:49.523+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:02:49.543+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:02:49.541+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:02:51.763+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:02:51.849+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:02:51.848+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:02:51.928+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:02:51.926+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:02:51.991+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 2.519 seconds
[2023-09-10T13:03:22.711+0000] {processor.py:157} INFO - Started process (PID=67) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:03:22.725+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:03:22.732+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:03:22.731+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:03:24.323+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:03:25.827+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:03:25.826+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:03:25.899+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:03:25.899+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:03:25.942+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 3.239 seconds
[2023-09-10T13:03:56.432+0000] {processor.py:157} INFO - Started process (PID=77) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:03:56.439+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:03:56.450+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:03:56.450+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:03:56.586+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:03:56.618+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:03:56.617+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:03:56.665+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:03:56.664+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:03:56.685+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.258 seconds
[2023-09-10T13:04:27.955+0000] {processor.py:157} INFO - Started process (PID=88) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:04:28.021+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:04:28.094+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:04:28.073+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:04:30.953+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:04:32.029+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:04:32.025+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:04:32.083+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:04:32.082+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:04:32.120+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 4.248 seconds
[2023-09-10T13:05:02.822+0000] {processor.py:157} INFO - Started process (PID=104) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:05:02.840+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:05:02.907+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:05:02.893+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:05:03.844+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:05:04.168+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:05:04.168+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:05:04.355+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:05:04.354+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:05:04.535+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.837 seconds
[2023-09-10T13:05:56.089+0000] {processor.py:157} INFO - Started process (PID=113) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:05:57.107+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:05:57.379+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:05:57.377+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:06:00.719+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:06:04.678+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:06:04.676+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:06:04.845+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:06:04.844+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:06:04.958+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 12.263 seconds
[2023-09-10T13:06:36.199+0000] {processor.py:157} INFO - Started process (PID=123) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:06:36.206+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:06:36.230+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:06:36.228+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:06:36.840+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:06:37.035+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:06:37.034+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:06:37.276+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:06:37.273+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:06:37.402+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.240 seconds
[2023-09-10T13:07:09.104+0000] {processor.py:157} INFO - Started process (PID=133) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:07:09.119+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:07:09.232+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:07:09.213+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:07:10.405+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:07:11.806+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:07:11.805+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:07:11.829+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:07:11.829+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:07:11.855+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 2.889 seconds
[2023-09-10T13:07:42.590+0000] {processor.py:157} INFO - Started process (PID=143) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:07:42.612+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:07:42.667+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:07:42.662+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:07:43.789+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:07:44.137+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:07:44.136+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:07:44.409+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:07:44.408+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:07:44.926+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 2.442 seconds
[2023-09-10T13:08:16.101+0000] {processor.py:157} INFO - Started process (PID=153) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:08:16.131+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:08:16.218+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:08:16.213+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:08:18.828+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:08:20.537+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:08:20.536+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:08:20.586+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:08:20.586+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:08:20.699+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 4.692 seconds
[2023-09-10T13:08:53.886+0000] {processor.py:157} INFO - Started process (PID=170) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:08:53.992+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:08:54.133+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:08:54.132+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:08:56.232+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:08:58.041+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:08:58.024+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:08:58.856+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:08:58.843+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:08:59.367+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 5.822 seconds
[2023-09-10T13:09:31.501+0000] {processor.py:157} INFO - Started process (PID=179) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:09:31.592+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:09:31.780+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:09:31.752+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:09:33.608+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:09:34.204+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:09:34.198+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:09:34.258+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:09:34.258+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:09:34.283+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 3.053 seconds
[2023-09-10T13:10:09.230+0000] {processor.py:157} INFO - Started process (PID=190) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:10:09.427+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:10:12.270+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:10:12.250+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:10:17.073+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:10:17.975+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:10:17.966+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:10:18.635+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:10:18.631+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:10:19.582+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 10.410 seconds
[2023-09-10T13:10:52.895+0000] {processor.py:157} INFO - Started process (PID=199) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:10:53.127+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:10:53.541+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:10:53.509+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:10:59.320+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:11:06.276+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:11:05.738+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:11:06.646+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:11:06.646+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:11:07.812+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 15.764 seconds
[2023-09-10T13:11:42.073+0000] {processor.py:157} INFO - Started process (PID=217) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:11:42.077+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:11:42.081+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:11:42.080+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:11:42.232+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:11:42.275+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:11:42.274+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:11:42.321+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:11:42.321+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:11:42.362+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.296 seconds
[2023-09-10T13:12:12.849+0000] {processor.py:157} INFO - Started process (PID=227) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:12:12.852+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:12:12.861+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:12:12.860+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:12:13.017+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:12:13.171+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:12:13.171+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:12:13.189+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:12:13.189+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:12:13.204+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.362 seconds
[2023-09-10T13:12:43.762+0000] {processor.py:157} INFO - Started process (PID=237) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:12:43.765+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:12:43.770+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:12:43.769+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:12:43.888+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:12:43.927+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:12:43.926+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:12:43.949+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:12:43.948+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:12:43.963+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.210 seconds
[2023-09-10T13:13:14.461+0000] {processor.py:157} INFO - Started process (PID=247) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:13:14.464+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:13:14.470+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:13:14.470+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:13:14.597+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:13:14.620+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:13:14.619+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:13:14.641+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:13:14.641+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:13:14.655+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.202 seconds
[2023-09-10T13:13:45.127+0000] {processor.py:157} INFO - Started process (PID=257) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:13:45.131+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:13:45.136+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:13:45.135+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:13:45.278+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:13:45.422+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:13:45.421+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:13:45.444+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:13:45.444+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:13:45.462+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.348 seconds
[2023-09-10T13:14:16.000+0000] {processor.py:157} INFO - Started process (PID=267) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:14:16.009+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:14:16.038+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:14:16.036+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:14:16.456+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:14:16.539+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:14:16.538+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:14:16.619+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:14:16.619+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:14:16.652+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.672 seconds
[2023-09-10T13:14:46.838+0000] {processor.py:157} INFO - Started process (PID=277) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:14:46.846+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:14:46.858+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:14:46.856+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:14:47.174+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:14:47.262+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:14:47.258+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:14:47.373+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:14:47.373+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:14:47.437+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.616 seconds
[2023-09-10T13:15:18.159+0000] {processor.py:157} INFO - Started process (PID=288) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:15:18.165+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:15:18.169+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:15:18.168+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:15:18.271+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:15:18.507+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:15:18.506+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:15:18.525+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:15:18.525+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:15:18.541+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.387 seconds
[2023-09-10T13:15:49.201+0000] {processor.py:157} INFO - Started process (PID=297) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:15:49.211+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:15:49.262+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:15:49.256+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:15:49.561+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:15:49.685+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:15:49.684+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:15:49.750+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:15:49.749+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:15:49.814+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.637 seconds
[2023-09-10T13:16:20.804+0000] {processor.py:157} INFO - Started process (PID=305) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:16:20.840+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:16:20.938+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:16:20.931+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:16:23.032+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:16:24.432+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:16:24.417+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:16:24.633+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:16:24.632+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:16:24.885+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 4.323 seconds
[2023-09-10T13:16:55.258+0000] {processor.py:157} INFO - Started process (PID=316) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:16:55.261+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:16:55.265+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:16:55.265+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:16:55.372+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:16:55.395+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:16:55.395+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:16:55.420+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:16:55.419+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:16:55.436+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.184 seconds
[2023-09-10T13:17:25.863+0000] {processor.py:157} INFO - Started process (PID=326) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:17:25.868+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:17:25.884+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:17:25.881+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:17:26.075+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:17:26.428+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:17:26.427+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:17:26.453+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:17:26.453+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:17:26.474+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.621 seconds
[2023-09-10T13:17:57.351+0000] {processor.py:157} INFO - Started process (PID=336) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:17:57.365+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:17:57.414+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:17:57.404+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:17:58.293+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:17:58.448+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:17:58.446+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:17:58.514+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:17:58.514+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:17:58.547+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.214 seconds
[2023-09-10T13:32:03.532+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:32:03.538+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:32:03.546+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:32:03.545+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:32:04.331+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:32:04.893+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:32:04.891+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:32:04.936+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:32:04.935+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:32:04.976+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.475 seconds
[2023-09-10T13:32:35.975+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:32:35.987+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:32:36.011+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:32:36.011+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:32:36.527+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:32:36.564+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:32:36.564+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:32:36.587+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:32:36.587+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:32:36.607+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.642 seconds
[2023-09-10T13:33:08.049+0000] {processor.py:157} INFO - Started process (PID=51) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:33:08.125+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:33:08.244+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:33:08.242+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:33:18.195+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:33:18.963+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:33:18.961+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:33:19.045+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:33:19.044+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:33:19.172+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 11.167 seconds
[2023-09-10T13:33:50.033+0000] {processor.py:157} INFO - Started process (PID=67) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:33:50.048+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:33:50.090+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:33:50.086+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:33:50.722+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:33:51.114+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:33:51.112+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:33:51.297+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:33:51.294+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:33:51.390+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.398 seconds
[2023-09-10T13:37:51.235+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:37:51.244+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:37:51.253+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:37:51.252+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:37:51.904+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:37:52.605+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:37:52.594+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:37:52.936+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:37:52.936+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:37:53.342+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:37:53.334+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"edge_info": {}, "dataset_triggers": [], "schedule_interval": "@daily", "timezone": "UTC", "default_args": {"__var": {"owner ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 13, 37, 52, 28380, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T13:37:53.345+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:37:53.345+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:37:53.348+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"edge_info": {}, "dataset_triggers": [], "schedule_interval": "@daily", "timezone": "UTC", "default_args": {"__var": {"owner ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 13, 37, 52, 28380, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T13:38:24.563+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:38:24.594+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:38:24.626+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:38:24.625+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:38:30.475+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:38:31.387+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:38:31.383+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:38:31.448+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:38:31.447+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:38:31.503+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 6.975 seconds
[2023-09-10T13:39:12.646+0000] {processor.py:157} INFO - Started process (PID=51) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:39:12.707+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:39:12.985+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:39:12.956+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:39:43.815+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:39:45.622+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:39:45.593+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:39:46.553+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:39:46.544+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:39:47.347+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 35.596 seconds
[2023-09-10T13:40:46.638+0000] {processor.py:157} INFO - Started process (PID=76) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:40:46.655+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:40:46.693+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:40:46.692+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:40:47.655+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:40:48.766+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:40:48.756+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:40:48.878+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:40:48.877+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:40:48.936+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 2.318 seconds
[2023-09-10T13:41:25.847+0000] {processor.py:157} INFO - Started process (PID=86) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:41:25.859+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:41:25.871+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:41:25.870+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:41:26.209+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:41:26.331+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:41:26.330+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:41:26.373+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:41:26.373+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:41:26.435+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.606 seconds
[2023-09-10T13:41:56.787+0000] {processor.py:157} INFO - Started process (PID=95) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:41:56.793+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:41:56.799+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:41:56.797+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:41:57.067+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:41:57.332+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:41:57.331+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:41:57.352+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:41:57.352+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:41:57.370+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.603 seconds
[2023-09-10T13:48:43.593+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:48:43.598+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:48:43.612+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:48:43.611+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:48:44.574+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:48:45.040+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:48:45.034+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:48:45.235+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:48:45.234+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:48:45.369+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:48:45.362+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"_dag_id": "website_crawler", "schedule_interval": "@daily", "dataset_triggers": [], "default_args": {"__var": {"owner": "ai ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 13, 48, 44, 650529, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T13:48:45.379+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:48:45.378+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:48:45.385+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"_dag_id": "website_crawler", "schedule_interval": "@daily", "dataset_triggers": [], "default_args": {"__var": {"owner": "ai ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 13, 48, 44, 650529, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T13:49:16.310+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:49:16.314+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:49:16.320+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:49:16.319+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:49:16.904+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:49:17.227+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:49:17.224+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:49:17.283+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:49:17.283+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:49:17.327+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.021 seconds
[2023-09-10T13:49:48.072+0000] {processor.py:157} INFO - Started process (PID=51) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:49:48.089+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:49:48.144+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:49:48.140+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:50:58.060+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:50:57.899+0000] {timeout.py:68} ERROR - Process timed out, PID: 51
[2023-09-10T13:50:58.176+0000] {logging_mixin.py:151} WARNING - Exception ignored in: <function _collection_gced at 0xffff98a890d0>
[2023-09-10T13:50:58.205+0000] {logging_mixin.py:151} WARNING - Traceback (most recent call last):
[2023-09-10T13:50:58.285+0000] {logging_mixin.py:151} WARNING -   File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/event/registry.py", line 53, in _collection_gced
[2023-09-10T13:50:58.430+0000] {logging_mixin.py:151} WARNING -     def _collection_gced(ref):
[2023-09-10T13:50:58.455+0000] {logging_mixin.py:151} WARNING -   File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
[2023-09-10T13:50:58.471+0000] {logging_mixin.py:151} WARNING -     raise AirflowTaskTimeout(self.error_message)
[2023-09-10T13:50:58.489+0000] {logging_mixin.py:151} WARNING - airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/crawl_data_using_BeautifulSoup.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.7.0/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.7.0/best-practices.html#reducing-dag-complexity, PID: 51
[2023-09-10T13:51:03.525+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:51:06.677+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:51:06.573+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:51:08.301+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:51:08.215+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:51:10.783+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 82.749 seconds
[2023-09-10T13:51:39.258+0000] {processor.py:157} INFO - Started process (PID=74) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:51:39.278+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:51:39.297+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:51:39.289+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:51:40.446+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:51:41.346+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:51:41.344+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:51:41.456+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:51:41.455+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:51:41.530+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 2.340 seconds
[2023-09-10T13:56:35.898+0000] {processor.py:157} INFO - Started process (PID=37) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:56:35.935+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:56:35.958+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:56:35.957+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:56:37.752+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:56:38.596+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:56:38.593+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:56:38.663+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:56:38.662+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:56:38.746+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 2.935 seconds
[2023-09-10T13:57:09.644+0000] {processor.py:157} INFO - Started process (PID=46) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:57:09.663+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:57:09.697+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:57:09.695+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:57:17.260+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:57:17.574+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:57:17.572+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:57:17.784+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:57:17.774+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:57:17.952+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 8.362 seconds
[2023-09-10T13:57:49.008+0000] {processor.py:157} INFO - Started process (PID=58) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:57:49.066+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:57:49.220+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:57:49.216+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:58:05.428+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:58:06.766+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:58:06.738+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:58:06.931+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:58:06.931+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:58:07.178+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 18.377 seconds
[2023-09-10T13:58:40.568+0000] {processor.py:157} INFO - Started process (PID=75) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:58:40.598+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:58:40.650+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:58:40.646+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:58:41.024+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:58:41.130+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:58:41.129+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:58:41.233+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:58:41.232+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:58:41.288+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.753 seconds
[2023-09-10T13:59:11.542+0000] {processor.py:157} INFO - Started process (PID=84) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:59:11.555+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:59:11.572+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:59:11.571+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:59:12.348+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:59:13.002+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:59:12.992+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:59:13.084+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:59:13.083+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:59:13.141+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.635 seconds
[2023-09-10T13:59:44.554+0000] {processor.py:157} INFO - Started process (PID=94) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:59:44.572+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T13:59:44.596+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:59:44.594+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:59:45.059+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T13:59:45.294+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:59:45.294+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T13:59:45.539+0000] {logging_mixin.py:151} INFO - [2023-09-10T13:59:45.538+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T13:59:45.642+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.108 seconds
[2023-09-10T14:00:16.317+0000] {processor.py:157} INFO - Started process (PID=104) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:00:16.322+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:00:16.330+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:00:16.329+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:00:16.495+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:00:16.821+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:00:16.820+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:00:16.854+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:00:16.853+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:00:16.885+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.574 seconds
[2023-09-10T14:00:47.111+0000] {processor.py:157} INFO - Started process (PID=114) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:00:47.116+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:00:47.126+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:00:47.125+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:00:47.482+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:00:47.727+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:00:47.726+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:00:47.837+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:00:47.837+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:00:47.925+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.821 seconds
[2023-09-10T14:01:18.523+0000] {processor.py:157} INFO - Started process (PID=125) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:01:18.528+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:01:18.538+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:01:18.537+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:01:18.802+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:01:19.242+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:01:19.241+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:01:19.272+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:01:19.272+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:01:19.306+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.793 seconds
[2023-09-10T14:01:49.664+0000] {processor.py:157} INFO - Started process (PID=135) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:01:49.669+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:01:49.685+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:01:49.683+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:01:49.898+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:01:49.996+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:01:49.996+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:01:50.083+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:01:50.082+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:01:50.202+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.549 seconds
[2023-09-10T14:02:22.193+0000] {processor.py:157} INFO - Started process (PID=145) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:02:22.213+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:02:22.304+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:02:22.251+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:02:23.433+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:02:25.051+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:02:25.035+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:02:25.925+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:02:25.909+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:02:26.344+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 4.271 seconds
[2023-09-10T14:02:58.270+0000] {processor.py:157} INFO - Started process (PID=156) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:02:58.299+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:02:58.414+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:02:58.407+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:03:00.502+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:03:01.087+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:03:01.084+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:03:01.310+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:03:01.301+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:03:01.453+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 3.310 seconds
[2023-09-10T14:03:36.141+0000] {processor.py:157} INFO - Started process (PID=164) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:03:36.183+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:03:36.447+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:03:36.418+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:03:38.728+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:03:40.894+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:03:40.867+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:03:41.220+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:03:41.218+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:03:41.419+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 5.738 seconds
[2023-09-10T14:04:13.986+0000] {processor.py:157} INFO - Started process (PID=175) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:04:14.028+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:04:14.098+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:04:14.096+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:04:18.514+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:04:20.972+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:04:20.684+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:04:21.560+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:04:21.557+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:04:25.250+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 11.400 seconds
[2023-09-10T14:04:59.492+0000] {processor.py:157} INFO - Started process (PID=185) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:04:59.569+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:04:59.954+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:04:59.896+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:05:15.433+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:05:19.981+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:05:19.931+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:05:20.524+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:05:20.521+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:05:21.196+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 21.895 seconds
[2023-09-10T14:05:55.314+0000] {processor.py:157} INFO - Started process (PID=203) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:05:55.358+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:05:55.482+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:05:55.466+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:05:56.748+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:05:57.305+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:05:57.283+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:05:57.985+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:05:57.954+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:05:58.367+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 3.151 seconds
[2023-09-10T14:06:30.975+0000] {processor.py:157} INFO - Started process (PID=213) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:06:31.021+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:06:31.115+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:06:31.108+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:06:32.056+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:06:33.186+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:06:33.179+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:06:33.359+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:06:33.358+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:06:33.456+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 2.680 seconds
[2023-09-10T14:07:07.440+0000] {processor.py:157} INFO - Started process (PID=224) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:07:07.501+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:07:07.580+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:07:07.566+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:07:11.722+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:07:12.586+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:07:12.570+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:07:13.049+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:07:13.048+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:07:13.323+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 5.912 seconds
[2023-09-10T14:07:46.268+0000] {processor.py:157} INFO - Started process (PID=233) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:07:46.421+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:07:46.811+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:07:46.789+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:07:48.506+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:07:50.413+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:07:50.384+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:07:50.729+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:07:50.728+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:07:50.992+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 5.260 seconds
[2023-09-10T14:08:23.927+0000] {processor.py:157} INFO - Started process (PID=243) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:08:24.081+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:08:24.541+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:08:24.479+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:08:28.765+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:08:29.833+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:08:29.827+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:08:30.011+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:08:30.009+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:08:30.369+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 7.334 seconds
[2023-09-10T14:09:01.909+0000] {processor.py:157} INFO - Started process (PID=260) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:09:01.916+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:09:01.930+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:09:01.929+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:09:02.173+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:09:02.462+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:09:02.461+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:09:02.549+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:09:02.548+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:09:02.590+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.694 seconds
[2023-09-10T14:09:33.742+0000] {processor.py:157} INFO - Started process (PID=271) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:09:33.789+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:09:33.895+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:09:33.882+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:09:37.519+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:09:37.834+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:09:37.832+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:09:38.097+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:09:38.097+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:09:38.244+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 4.545 seconds
[2023-09-10T14:10:09.201+0000] {processor.py:157} INFO - Started process (PID=281) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:10:09.211+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:10:09.243+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:10:09.240+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:10:09.554+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:10:09.948+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:10:09.947+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:10:10.001+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:10:10.001+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:10:10.105+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.939 seconds
[2023-09-10T14:10:41.828+0000] {processor.py:157} INFO - Started process (PID=292) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:10:41.849+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:10:41.949+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:10:41.924+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:10:43.531+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:10:44.943+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:10:44.937+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:10:45.520+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:10:45.517+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:10:45.668+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 4.027 seconds
[2023-09-10T14:11:16.582+0000] {processor.py:157} INFO - Started process (PID=300) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:11:16.595+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:11:16.687+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:11:16.674+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:11:17.190+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:11:18.225+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:11:18.217+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:11:18.358+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:11:18.357+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:11:18.439+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.883 seconds
[2023-09-10T14:11:49.601+0000] {processor.py:157} INFO - Started process (PID=311) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:11:49.627+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:11:49.658+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:11:49.657+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:11:50.154+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:11:50.556+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:11:50.552+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:11:50.726+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:11:50.725+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:11:50.898+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.321 seconds
[2023-09-10T14:12:21.923+0000] {processor.py:157} INFO - Started process (PID=321) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:12:21.930+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:12:21.945+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:12:21.944+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:12:22.192+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:12:22.642+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:12:22.641+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:12:22.685+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:12:22.685+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:12:22.727+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.820 seconds
[2023-09-10T14:12:53.645+0000] {processor.py:157} INFO - Started process (PID=332) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:12:53.654+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:12:53.669+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:12:53.668+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:12:53.889+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:12:54.084+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:12:54.083+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:12:54.168+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:12:54.167+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:12:54.209+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.576 seconds
[2023-09-10T14:13:25.936+0000] {processor.py:157} INFO - Started process (PID=342) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:13:25.957+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:13:26.016+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:13:26.004+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:13:26.806+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:13:28.892+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:13:28.875+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:13:29.361+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:13:29.358+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:13:29.800+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 3.890 seconds
[2023-09-10T14:14:01.882+0000] {processor.py:157} INFO - Started process (PID=352) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:14:01.922+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:14:02.056+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:14:02.027+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:14:06.523+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:14:08.187+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:14:08.177+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:14:09.049+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:14:09.046+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:14:10.080+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 8.333 seconds
[2023-09-10T14:14:41.164+0000] {processor.py:157} INFO - Started process (PID=361) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:14:41.179+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:14:41.234+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:14:41.232+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:14:41.716+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:14:42.288+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:14:42.287+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:14:42.380+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:14:42.380+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:14:42.469+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.329 seconds
[2023-09-10T14:15:13.068+0000] {processor.py:157} INFO - Started process (PID=378) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:15:13.076+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:15:13.092+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:15:13.091+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:15:13.243+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:15:13.305+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:15:13.304+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:15:13.343+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:15:13.343+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:15:13.382+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.322 seconds
[2023-09-10T14:15:55.172+0000] {processor.py:157} INFO - Started process (PID=388) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:15:55.217+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:15:55.665+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:15:55.649+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:15:58.914+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:16:04.338+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:16:04.306+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:16:05.071+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:16:05.058+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:16:05.870+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 10.957 seconds
[2023-09-10T14:16:36.986+0000] {processor.py:157} INFO - Started process (PID=398) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:16:36.993+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:16:37.009+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:16:37.007+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:16:37.401+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:16:37.444+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:16:37.442+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:16:37.525+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:16:37.524+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:16:37.562+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.594 seconds
[2023-09-10T14:17:22.935+0000] {processor.py:157} INFO - Started process (PID=408) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:17:23.057+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:17:23.467+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:17:23.408+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:17:31.481+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:17:34.674+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:17:34.652+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:17:35.012+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:17:35.008+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:17:35.474+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 12.881 seconds
[2023-09-10T14:18:22.338+0000] {processor.py:157} INFO - Started process (PID=424) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:18:22.424+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:18:22.675+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:18:22.586+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:18:27.398+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:18:28.708+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:18:28.697+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:18:29.080+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:18:29.072+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:18:29.506+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 7.327 seconds
[2023-09-10T14:19:05.507+0000] {processor.py:157} INFO - Started process (PID=433) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:19:05.624+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:19:05.850+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:19:05.828+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:19:07.801+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:19:09.408+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:19:09.404+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:19:09.526+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:19:09.523+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:19:09.664+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 4.427 seconds
[2023-09-10T14:19:42.462+0000] {processor.py:157} INFO - Started process (PID=444) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:19:42.473+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:19:42.500+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:19:42.497+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:19:43.281+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:19:43.425+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:19:43.422+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:19:43.595+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:19:43.594+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:19:43.697+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.260 seconds
[2023-09-10T14:20:16.251+0000] {processor.py:157} INFO - Started process (PID=455) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:20:16.259+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:20:16.275+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:20:16.272+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:20:16.575+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:20:17.062+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:20:17.061+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:20:17.101+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:20:17.101+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:20:17.143+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.933 seconds
[2023-09-10T14:20:47.627+0000] {processor.py:157} INFO - Started process (PID=465) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:20:47.639+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:20:47.672+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:20:47.670+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:20:48.086+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:20:48.226+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:20:48.225+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:20:48.574+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:20:48.564+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:20:48.710+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.094 seconds
[2023-09-10T14:21:19.056+0000] {processor.py:157} INFO - Started process (PID=475) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:21:19.059+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:21:19.064+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:21:19.064+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:21:19.170+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:21:19.450+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:21:19.450+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:21:19.485+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:21:19.484+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:21:19.513+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.463 seconds
[2023-09-10T14:21:49.760+0000] {processor.py:157} INFO - Started process (PID=485) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:21:49.764+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:21:49.770+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:21:49.770+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:21:49.902+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:21:49.935+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:21:49.934+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:21:49.970+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:21:49.969+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:21:50.001+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.247 seconds
[2023-09-10T14:22:20.429+0000] {processor.py:157} INFO - Started process (PID=496) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:22:20.437+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:22:20.488+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:22:20.487+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:22:20.834+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:22:21.272+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:22:21.268+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:22:21.330+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:22:21.330+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:22:21.372+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.969 seconds
[2023-09-10T14:22:51.719+0000] {processor.py:157} INFO - Started process (PID=506) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:22:51.723+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:22:51.729+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:22:51.729+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:22:52.053+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:22:52.298+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:22:52.295+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:22:52.476+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:22:52.475+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:22:52.567+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.853 seconds
[2023-09-10T14:23:24.329+0000] {processor.py:157} INFO - Started process (PID=515) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:23:24.353+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:23:24.424+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:23:24.423+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:23:26.422+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:23:27.652+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:23:27.640+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:23:27.784+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:23:27.784+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:23:27.899+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 3.640 seconds
[2023-09-10T14:23:58.137+0000] {processor.py:157} INFO - Started process (PID=525) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:23:58.142+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:23:58.148+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:23:58.147+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:23:58.321+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:23:58.373+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:23:58.372+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:23:58.404+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:23:58.403+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:23:58.434+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.303 seconds
[2023-09-10T14:24:28.727+0000] {processor.py:157} INFO - Started process (PID=535) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:24:28.732+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:24:28.740+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:24:28.739+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:24:28.948+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:24:28.985+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:24:28.984+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:24:29.032+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:24:29.032+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:24:29.069+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.351 seconds
[2023-09-10T14:24:59.974+0000] {processor.py:157} INFO - Started process (PID=544) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:25:00.057+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:25:00.129+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:25:00.128+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:25:00.759+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:25:01.194+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:25:01.192+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:25:01.253+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:25:01.252+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:25:01.304+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.484 seconds
[2023-09-10T14:25:37.175+0000] {processor.py:157} INFO - Started process (PID=564) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:25:37.189+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:25:37.243+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:25:37.231+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:25:38.111+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:25:38.244+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:25:38.241+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:25:38.378+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:25:38.377+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:25:38.566+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.422 seconds
[2023-09-10T14:26:12.391+0000] {processor.py:157} INFO - Started process (PID=574) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:26:12.420+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:26:12.446+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:26:12.444+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:26:12.755+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:26:13.381+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:26:13.380+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:26:13.453+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:26:13.453+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:26:13.530+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.189 seconds
[2023-09-10T14:26:45.045+0000] {processor.py:157} INFO - Started process (PID=584) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:26:45.057+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:26:45.074+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:26:45.069+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:26:45.496+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:26:45.782+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:26:45.781+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:26:45.888+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:26:45.888+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:26:45.978+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.946 seconds
[2023-09-10T14:27:18.633+0000] {processor.py:157} INFO - Started process (PID=596) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:27:18.646+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:27:18.665+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:27:18.663+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:27:18.895+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:27:19.797+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:27:19.783+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:27:20.015+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:27:20.015+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:27:20.269+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.663 seconds
[2023-09-10T14:27:51.390+0000] {processor.py:157} INFO - Started process (PID=607) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:27:51.399+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:27:51.411+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:27:51.411+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:27:51.628+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:27:51.691+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:27:51.691+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:27:51.778+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:27:51.778+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:27:51.820+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.443 seconds
[2023-09-10T14:28:24.385+0000] {processor.py:157} INFO - Started process (PID=618) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:28:24.417+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:28:24.457+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:28:24.454+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:28:25.447+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:28:26.744+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:28:26.673+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:28:27.205+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:28:27.203+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:28:27.437+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 3.119 seconds
[2023-09-10T14:28:58.450+0000] {processor.py:157} INFO - Started process (PID=628) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:28:58.458+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:28:58.475+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:28:58.472+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:28:58.917+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:28:59.005+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:28:59.004+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:28:59.212+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:28:59.211+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:28:59.295+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.858 seconds
[2023-09-10T14:29:29.957+0000] {processor.py:157} INFO - Started process (PID=638) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:29:29.971+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:29:30.018+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:29:30.008+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:29:31.325+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:29:32.184+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:29:32.178+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:29:32.364+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:29:32.363+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:29:32.489+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 2.572 seconds
[2023-09-10T14:30:03.049+0000] {processor.py:157} INFO - Started process (PID=649) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:30:03.055+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:30:03.068+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:30:03.067+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:30:03.310+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:30:03.361+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:30:03.360+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:30:03.407+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:30:03.407+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:30:03.459+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.431 seconds
[2023-09-10T14:30:37.664+0000] {processor.py:157} INFO - Started process (PID=659) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:30:37.680+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:30:37.727+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:30:37.726+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:30:38.073+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:30:38.805+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:30:38.801+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:30:38.929+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:30:38.929+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:30:39.001+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.518 seconds
[2023-09-10T14:31:10.195+0000] {processor.py:157} INFO - Started process (PID=669) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:31:10.211+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:31:10.227+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:31:10.226+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:31:10.462+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:31:10.529+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:31:10.527+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:31:10.572+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:31:10.572+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:31:10.599+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.426 seconds
[2023-09-10T14:31:41.521+0000] {processor.py:157} INFO - Started process (PID=678) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:31:41.573+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:31:41.666+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:31:41.659+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:31:43.508+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:31:45.875+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:31:45.851+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:31:46.190+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:31:46.188+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:31:46.398+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 4.964 seconds
[2023-09-10T14:32:16.918+0000] {processor.py:157} INFO - Started process (PID=687) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:32:16.927+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:32:16.943+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:32:16.942+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:32:17.324+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:32:17.412+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:32:17.410+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:32:17.509+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:32:17.508+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:32:17.586+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.684 seconds
[2023-09-10T14:32:48.325+0000] {processor.py:157} INFO - Started process (PID=697) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:32:48.341+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:32:48.363+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:32:48.361+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:32:48.673+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:32:48.916+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:32:48.915+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:32:48.957+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:32:48.957+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:32:48.992+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.680 seconds
[2023-09-10T14:33:19.487+0000] {processor.py:157} INFO - Started process (PID=707) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:33:19.497+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:33:19.508+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:33:19.506+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:33:19.807+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:33:19.903+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:33:19.901+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:33:19.983+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:33:19.983+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:33:20.071+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.596 seconds
[2023-09-10T14:33:51.120+0000] {processor.py:157} INFO - Started process (PID=717) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:33:51.130+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:33:51.177+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:33:51.175+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:33:51.541+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:33:51.941+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:33:51.940+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:33:51.998+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:33:51.998+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:33:52.056+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.953 seconds
[2023-09-10T14:34:23.006+0000] {processor.py:157} INFO - Started process (PID=726) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:34:23.013+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:34:23.020+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:34:23.019+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:34:23.190+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:34:23.231+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:34:23.231+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:34:23.281+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:34:23.281+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:34:23.309+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.315 seconds
[2023-09-10T14:34:53.704+0000] {processor.py:157} INFO - Started process (PID=736) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:34:53.709+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:34:53.720+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:34:53.719+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:34:53.957+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:34:54.333+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:34:54.332+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:34:54.374+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:34:54.374+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:34:54.404+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.712 seconds
[2023-09-10T14:35:25.062+0000] {processor.py:157} INFO - Started process (PID=753) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:35:25.074+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:35:25.094+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:35:25.092+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:35:25.495+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:35:25.646+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:35:25.644+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:35:25.763+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:35:25.762+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:35:25.853+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.815 seconds
[2023-09-10T14:35:56.662+0000] {processor.py:157} INFO - Started process (PID=763) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:35:56.669+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:35:56.686+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:35:56.684+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:35:57.003+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:35:57.427+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:35:57.425+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:35:57.492+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:35:57.492+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:35:57.538+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.890 seconds
[2023-09-10T14:36:27.908+0000] {processor.py:157} INFO - Started process (PID=773) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:36:27.915+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:36:27.926+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:36:27.925+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:36:28.123+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:36:28.219+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:36:28.218+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:36:28.285+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:36:28.285+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:36:28.330+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.433 seconds
[2023-09-10T14:36:59.328+0000] {processor.py:157} INFO - Started process (PID=784) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:36:59.337+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:36:59.358+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:36:59.355+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:36:59.647+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:37:00.060+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:37:00.059+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:37:00.216+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:37:00.214+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:37:00.261+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.953 seconds
[2023-09-10T14:37:30.847+0000] {processor.py:157} INFO - Started process (PID=794) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:37:30.865+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:37:30.889+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:37:30.888+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:37:31.101+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:37:31.203+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:37:31.202+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:37:31.285+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:37:31.285+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:37:31.328+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.492 seconds
[2023-09-10T14:39:16.909+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:39:16.917+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:39:16.939+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:39:16.936+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:39:17.642+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:39:18.300+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:39:18.298+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:39:18.384+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:39:18.384+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:39:18.452+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.554 seconds
[2023-09-10T14:39:49.250+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:39:49.291+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:39:49.315+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:39:49.310+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:39:51.362+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:39:51.564+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:39:51.558+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:39:51.649+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:39:51.648+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:39:51.795+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 2.699 seconds
[2023-09-10T14:40:22.699+0000] {processor.py:157} INFO - Started process (PID=50) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:40:22.705+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:40:22.716+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:40:22.716+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:40:23.322+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:40:23.508+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:40:23.508+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:40:23.565+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:40:23.565+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:40:23.595+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.907 seconds
[2023-09-10T14:40:53.988+0000] {processor.py:157} INFO - Started process (PID=60) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:40:53.993+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:40:54.000+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:40:53.999+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:40:54.143+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:40:54.216+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:40:54.215+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:40:54.248+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:40:54.248+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:40:54.270+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.300 seconds
[2023-09-10T14:41:25.048+0000] {processor.py:157} INFO - Started process (PID=71) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:41:25.079+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:41:25.094+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:41:25.092+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:41:25.357+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:41:25.730+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:41:25.729+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:41:25.772+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:41:25.771+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:41:25.806+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.784 seconds
[2023-09-10T14:41:56.392+0000] {processor.py:157} INFO - Started process (PID=81) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:41:56.398+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:41:56.416+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:41:56.415+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:41:56.589+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:41:56.665+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:41:56.665+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:41:56.704+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:41:56.704+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:41:56.736+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.353 seconds
[2023-09-10T14:42:27.561+0000] {processor.py:157} INFO - Started process (PID=91) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:42:27.566+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:42:27.577+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:42:27.577+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:42:27.727+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:42:27.941+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:42:27.940+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:42:27.969+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:42:27.969+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:42:27.998+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.443 seconds
[2023-09-10T14:42:58.684+0000] {processor.py:157} INFO - Started process (PID=101) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:42:58.692+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:42:58.728+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:42:58.727+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:42:59.053+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:42:59.199+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:42:59.198+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:42:59.270+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:42:59.269+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:42:59.307+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.643 seconds
[2023-09-10T14:43:30.035+0000] {processor.py:157} INFO - Started process (PID=111) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:43:30.045+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:43:30.058+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:43:30.056+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:43:30.368+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:43:30.844+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:43:30.841+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:43:30.922+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:43:30.922+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:43:30.975+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.965 seconds
[2023-09-10T14:44:01.496+0000] {processor.py:157} INFO - Started process (PID=127) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:44:01.504+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:44:01.523+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:44:01.517+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:44:01.756+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:44:01.849+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:44:01.848+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:44:01.902+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:44:01.902+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:44:01.930+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.443 seconds
[2023-09-10T14:44:32.599+0000] {processor.py:157} INFO - Started process (PID=138) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:44:32.605+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:44:32.616+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:44:32.615+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:44:32.842+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:44:33.184+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:44:33.181+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:44:33.225+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:44:33.225+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:44:33.254+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.670 seconds
[2023-09-10T14:45:03.719+0000] {processor.py:157} INFO - Started process (PID=148) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:45:03.724+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:45:03.734+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:45:03.733+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:45:03.895+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:45:03.970+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:45:03.970+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:45:04.014+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:45:04.014+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:45:04.044+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.333 seconds
[2023-09-10T14:45:34.805+0000] {processor.py:157} INFO - Started process (PID=159) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:45:34.812+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:45:34.824+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:45:34.823+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:45:35.177+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:45:35.614+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:45:35.613+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:45:35.670+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:45:35.669+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:45:35.809+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.015 seconds
[2023-09-10T14:46:06.235+0000] {processor.py:157} INFO - Started process (PID=170) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:46:06.241+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:46:06.250+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:46:06.249+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:46:06.415+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:46:06.490+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:46:06.489+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:46:06.541+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:46:06.541+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:46:06.567+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.342 seconds
[2023-09-10T14:46:37.183+0000] {processor.py:157} INFO - Started process (PID=180) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:46:37.188+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:46:37.199+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:46:37.198+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:46:37.370+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:46:37.587+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:46:37.586+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:46:37.623+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:46:37.622+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:46:37.646+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.474 seconds
[2023-09-10T14:54:30.398+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:54:30.415+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:54:30.467+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:54:30.466+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:54:31.690+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:54:33.240+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:54:33.236+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:54:33.576+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:54:33.575+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:54:33.852+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:54:33.815+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"default_args": {"__var": {"owner": "airflow", "depends_on_past": false, "start_date": {"__var": 1693180800.0, "__type": "da ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 14, 54, 32, 443126, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T14:54:33.899+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:54:33.897+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:54:33.915+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"default_args": {"__var": {"owner": "airflow", "depends_on_past": false, "start_date": {"__var": 1693180800.0, "__type": "da ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 14, 54, 32, 443126, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T14:55:04.377+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:55:04.381+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:55:04.388+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:55:04.387+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:55:04.612+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:55:04.723+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:55:04.723+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:55:04.745+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:55:04.745+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:55:04.763+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.390 seconds
[2023-09-10T14:55:35.216+0000] {processor.py:157} INFO - Started process (PID=50) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:55:35.221+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T14:55:35.228+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:55:35.228+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:55:36.427+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T14:55:36.522+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:55:36.521+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T14:55:36.573+0000] {logging_mixin.py:151} INFO - [2023-09-10T14:55:36.573+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T14:55:36.626+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.427 seconds
[2023-09-10T15:07:17.901+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:07:17.914+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:07:17.936+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:07:17.935+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:07:19.171+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:07:19.665+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:07:19.663+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:07:19.749+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:07:19.749+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:07:19.992+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 2.115 seconds
[2023-09-10T15:07:50.721+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:07:50.726+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:07:50.732+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:07:50.732+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:07:51.081+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:07:51.171+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:07:51.171+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:07:51.212+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:07:51.211+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:07:51.257+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.543 seconds
[2023-09-10T15:08:21.965+0000] {processor.py:157} INFO - Started process (PID=51) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:08:21.969+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:08:21.977+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:08:21.975+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:08:23.307+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:08:23.658+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:08:23.657+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:08:23.692+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:08:23.692+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:08:23.724+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.767 seconds
[2023-09-10T15:08:54.548+0000] {processor.py:157} INFO - Started process (PID=61) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:08:54.559+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:08:54.583+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:08:54.582+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:08:54.840+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:08:54.892+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:08:54.890+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:08:54.973+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:08:54.973+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:08:55.035+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.496 seconds
[2023-09-10T15:09:25.363+0000] {processor.py:157} INFO - Started process (PID=71) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:09:25.367+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:09:25.373+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:09:25.372+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:09:25.618+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:09:25.892+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:09:25.892+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:09:25.945+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:09:25.945+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:09:25.979+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.621 seconds
[2023-09-10T15:22:11.443+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:22:11.456+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:22:11.497+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:22:11.495+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:22:12.533+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:22:13.080+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:22:13.078+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:22:13.221+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:22:13.221+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:22:13.687+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:22:13.669+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"dataset_triggers": [], "dagrun_timeout": 60.0, "fileloc": "/opt/airflow/dags/crawl_data_using_BeautifulSoup.py", "_dag_id": ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 15, 22, 12, 709825, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T15:22:13.696+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:22:13.696+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:22:13.704+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"dataset_triggers": [], "dagrun_timeout": 60.0, "fileloc": "/opt/airflow/dags/crawl_data_using_BeautifulSoup.py", "_dag_id": ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 15, 22, 12, 709825, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T15:22:44.738+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:22:44.743+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:22:44.754+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:22:44.754+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:22:45.098+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:22:45.262+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:22:45.261+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:22:45.292+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:22:45.292+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:22:45.317+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.586 seconds
[2023-09-10T15:23:15.695+0000] {processor.py:157} INFO - Started process (PID=52) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:23:15.702+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:23:15.717+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:23:15.716+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:23:16.287+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:23:16.321+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:23:16.320+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:23:16.372+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:23:16.372+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:23:16.427+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.743 seconds
[2023-09-10T15:23:47.642+0000] {processor.py:157} INFO - Started process (PID=62) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:23:47.652+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:23:47.668+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:23:47.666+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:23:48.002+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:23:48.612+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:23:48.611+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:23:48.665+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:23:48.664+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:23:48.715+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.090 seconds
[2023-09-10T15:24:19.613+0000] {processor.py:157} INFO - Started process (PID=72) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:24:19.623+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:24:19.634+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:24:19.633+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:24:19.979+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:24:20.082+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:24:20.081+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:24:20.227+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:24:20.226+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:24:20.271+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.668 seconds
[2023-09-10T15:24:51.037+0000] {processor.py:157} INFO - Started process (PID=83) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:24:51.047+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:24:51.065+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:24:51.064+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:24:51.412+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:24:51.745+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:24:51.744+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:24:51.827+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:24:51.826+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:24:51.927+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.909 seconds
[2023-09-10T15:25:22.311+0000] {processor.py:157} INFO - Started process (PID=93) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:25:22.334+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:25:22.352+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:25:22.351+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:25:22.554+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:25:22.600+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:25:22.599+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:25:22.642+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:25:22.642+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:25:22.675+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.383 seconds
[2023-09-10T15:25:54.094+0000] {processor.py:157} INFO - Started process (PID=109) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:25:54.102+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:25:54.130+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:25:54.125+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:25:54.416+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:25:54.674+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:25:54.673+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:25:54.721+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:25:54.721+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:25:54.765+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.714 seconds
[2023-09-10T15:26:24.938+0000] {processor.py:157} INFO - Started process (PID=119) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:26:24.945+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:26:24.961+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:26:24.959+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:26:25.193+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:26:25.275+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:26:25.274+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:26:25.361+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:26:25.360+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:26:25.399+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.468 seconds
[2023-09-10T15:26:56.058+0000] {processor.py:157} INFO - Started process (PID=130) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:26:56.063+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:26:56.074+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:26:56.073+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:26:56.357+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:26:56.885+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:26:56.881+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:26:57.086+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:26:57.084+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:26:57.216+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.165 seconds
[2023-09-10T15:27:27.584+0000] {processor.py:157} INFO - Started process (PID=140) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:27:27.588+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:27:27.594+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:27:27.594+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:27:27.716+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:27:27.745+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:27:27.744+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:27:27.783+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:27:27.782+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:27:27.807+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.229 seconds
[2023-09-10T15:27:58.201+0000] {processor.py:157} INFO - Started process (PID=150) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:27:58.218+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:27:58.236+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:27:58.234+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:27:58.516+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:27:58.731+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:27:58.730+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:27:58.762+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:27:58.762+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:27:58.786+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.607 seconds
[2023-09-10T15:28:29.185+0000] {processor.py:157} INFO - Started process (PID=159) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:28:29.192+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:28:29.199+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:28:29.199+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:28:29.359+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:28:29.438+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:28:29.437+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:28:29.481+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:28:29.480+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:28:29.536+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.359 seconds
[2023-09-10T15:28:59.999+0000] {processor.py:157} INFO - Started process (PID=169) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:29:00.020+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:29:00.032+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:29:00.031+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:29:00.166+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:29:00.403+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:29:00.402+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:29:00.439+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:29:00.439+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:29:00.467+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.476 seconds
[2023-09-10T15:29:30.867+0000] {processor.py:157} INFO - Started process (PID=179) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:29:30.871+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:29:30.881+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:29:30.880+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:29:31.068+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:29:31.210+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:29:31.208+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:29:31.270+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:29:31.269+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:29:31.308+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.446 seconds
[2023-09-10T15:30:01.846+0000] {processor.py:157} INFO - Started process (PID=188) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:30:01.851+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:30:01.859+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:30:01.858+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:30:02.016+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:30:02.241+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:30:02.240+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:30:02.276+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:30:02.276+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:30:02.311+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.469 seconds
[2023-09-10T15:30:32.856+0000] {processor.py:157} INFO - Started process (PID=197) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:30:32.861+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:30:32.869+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:30:32.868+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:30:33.027+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:30:33.095+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:30:33.094+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:30:33.162+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:30:33.161+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:30:33.207+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.361 seconds
[2023-09-10T15:31:03.681+0000] {processor.py:157} INFO - Started process (PID=207) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:31:03.685+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:31:03.691+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:31:03.690+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:31:03.812+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:31:03.964+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:31:03.964+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:31:03.986+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:31:03.986+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:31:04.010+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.339 seconds
[2023-09-10T15:31:34.398+0000] {processor.py:157} INFO - Started process (PID=217) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:31:34.405+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:31:34.431+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:31:34.429+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:31:35.022+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:31:35.220+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:31:35.218+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:31:35.331+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:31:35.330+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:31:35.405+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.021 seconds
[2023-09-10T15:32:05.988+0000] {processor.py:157} INFO - Started process (PID=228) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:32:06.015+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:32:06.047+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:32:06.043+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:32:06.467+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:32:06.813+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:32:06.812+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:32:06.870+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:32:06.870+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:32:06.994+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.032 seconds
[2023-09-10T15:32:37.584+0000] {processor.py:157} INFO - Started process (PID=238) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:32:37.593+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:32:37.610+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:32:37.608+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:32:37.967+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:32:38.022+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:32:38.022+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:32:38.106+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:32:38.106+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:32:38.161+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.593 seconds
[2023-09-10T15:33:08.656+0000] {processor.py:157} INFO - Started process (PID=248) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:33:08.661+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:33:08.667+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:33:08.666+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:33:08.781+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:33:09.022+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:33:09.021+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:33:09.043+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:33:09.043+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:33:09.063+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.416 seconds
[2023-09-10T15:33:39.429+0000] {processor.py:157} INFO - Started process (PID=259) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:33:39.433+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:33:39.443+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:33:39.442+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:33:39.727+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:33:40.143+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:33:40.140+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:33:40.316+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:33:40.315+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:33:40.468+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.048 seconds
[2023-09-10T15:34:11.080+0000] {processor.py:157} INFO - Started process (PID=269) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:34:11.086+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:34:11.093+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:34:11.093+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:34:11.220+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:34:11.613+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:34:11.613+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:34:11.638+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:34:11.638+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:34:11.661+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.590 seconds
[2023-09-10T15:34:42.137+0000] {processor.py:157} INFO - Started process (PID=279) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:34:42.142+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:34:42.148+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:34:42.147+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:34:42.274+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:34:42.354+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:34:42.353+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:34:42.406+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:34:42.405+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:34:42.448+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.319 seconds
[2023-09-10T15:35:12.782+0000] {processor.py:157} INFO - Started process (PID=289) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:35:12.787+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:35:12.800+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:35:12.799+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:35:13.081+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:35:13.617+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:35:13.616+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:35:13.683+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:35:13.680+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:35:13.743+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.971 seconds
[2023-09-10T15:39:38.295+0000] {processor.py:157} INFO - Started process (PID=32) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:39:38.312+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:39:38.331+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:39:38.329+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:39:39.177+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:39:39.711+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:39:39.710+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:39:39.762+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:39:39.761+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:39:39.850+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.579 seconds
[2023-09-10T15:40:10.679+0000] {processor.py:157} INFO - Started process (PID=42) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:40:10.684+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:40:10.693+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:40:10.693+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:40:10.847+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:40:10.906+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:40:10.905+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:40:10.957+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:40:10.956+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:40:10.999+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.325 seconds
[2023-09-10T15:40:41.748+0000] {processor.py:157} INFO - Started process (PID=52) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:40:41.757+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:40:41.772+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:40:41.771+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:40:42.028+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:40:42.469+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:40:42.468+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:40:42.533+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:40:42.533+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:40:42.595+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.857 seconds
[2023-09-10T15:41:13.340+0000] {processor.py:157} INFO - Started process (PID=62) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:41:13.351+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:41:13.379+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:41:13.375+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:41:13.916+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:41:14.297+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:41:14.295+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:41:14.988+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:41:14.975+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:41:15.185+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.875 seconds
[2023-09-10T15:41:45.825+0000] {processor.py:157} INFO - Started process (PID=71) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:41:45.847+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:41:45.863+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:41:45.861+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:41:46.310+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:41:46.810+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:41:46.808+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:41:46.901+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:41:46.900+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:41:46.975+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.173 seconds
[2023-09-10T15:42:17.283+0000] {processor.py:157} INFO - Started process (PID=82) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:42:17.291+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:42:17.312+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:42:17.310+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:42:17.796+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:42:17.950+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:42:17.947+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:42:18.039+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:42:18.039+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:42:18.092+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.824 seconds
[2023-09-10T15:42:49.196+0000] {processor.py:157} INFO - Started process (PID=99) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:42:49.203+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:42:49.222+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:42:49.220+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:42:49.530+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:42:49.844+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:42:49.843+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:42:49.885+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:42:49.885+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:42:49.920+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.731 seconds
[2023-09-10T15:43:20.444+0000] {processor.py:157} INFO - Started process (PID=109) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:43:20.482+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:43:20.508+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:43:20.505+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:43:20.893+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:43:21.058+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:43:21.057+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:43:21.252+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:43:21.238+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:43:21.381+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.964 seconds
[2023-09-10T15:43:52.218+0000] {processor.py:157} INFO - Started process (PID=120) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:43:52.226+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:43:52.250+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:43:52.247+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:43:52.751+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:43:53.894+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:43:53.885+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:43:53.987+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:43:53.987+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:43:54.087+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.919 seconds
[2023-09-10T15:44:24.324+0000] {processor.py:157} INFO - Started process (PID=130) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:44:24.329+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:44:24.336+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:44:24.336+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:44:24.469+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:44:24.503+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:44:24.502+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:44:24.536+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:44:24.536+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:44:24.560+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.243 seconds
[2023-09-10T15:44:55.282+0000] {processor.py:157} INFO - Started process (PID=140) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:44:55.288+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:44:55.297+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:44:55.296+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:44:55.612+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:44:55.866+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:44:55.865+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:44:55.900+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:44:55.900+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:44:55.932+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.658 seconds
[2023-09-10T15:45:26.525+0000] {processor.py:157} INFO - Started process (PID=151) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:45:26.530+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:45:26.541+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:45:26.540+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:45:26.772+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:45:26.822+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:45:26.821+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:45:26.860+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:45:26.860+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:45:26.885+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.371 seconds
[2023-09-10T15:48:54.793+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:48:54.825+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:48:54.879+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:48:54.877+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:48:56.083+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:48:56.702+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:48:56.700+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:48:56.895+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:48:56.894+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:48:57.021+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:48:57.010+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"_dag_id": "website_crawler", "_task_group": {"_group_id": null, "prefix_group_id": true, "tooltip": "", "ui_color": "Cornfl ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 15, 48, 56, 214305, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T15:48:57.041+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:48:57.040+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:48:57.052+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"_dag_id": "website_crawler", "_task_group": {"_group_id": null, "prefix_group_id": true, "tooltip": "", "ui_color": "Cornfl ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 15, 48, 56, 214305, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T15:49:27.807+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:49:27.812+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:49:27.819+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:49:27.819+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:49:28.232+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:49:28.447+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:49:28.446+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:49:28.479+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:49:28.479+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:49:28.512+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.712 seconds
[2023-09-10T15:49:59.501+0000] {processor.py:157} INFO - Started process (PID=51) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:49:59.515+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:49:59.545+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:49:59.541+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:50:00.557+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:50:00.726+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:50:00.723+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:50:01.052+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:50:01.050+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:50:01.197+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.705 seconds
[2023-09-10T15:50:31.528+0000] {processor.py:157} INFO - Started process (PID=61) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:50:31.539+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:50:31.563+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:50:31.561+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:50:31.914+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:50:32.322+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:50:32.321+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:50:32.375+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:50:32.374+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:50:32.437+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.921 seconds
[2023-09-10T15:51:03.373+0000] {processor.py:157} INFO - Started process (PID=71) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:51:03.402+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:51:03.420+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:51:03.418+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:51:04.354+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:51:04.568+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:51:04.564+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:51:04.686+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:51:04.681+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:51:04.850+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.497 seconds
[2023-09-10T15:51:36.007+0000] {processor.py:157} INFO - Started process (PID=80) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:51:36.013+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:51:36.021+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:51:36.020+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:51:36.274+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:51:36.663+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:51:36.662+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:51:36.717+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:51:36.717+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:51:36.763+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.769 seconds
[2023-09-10T15:52:07.308+0000] {processor.py:157} INFO - Started process (PID=90) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:52:07.311+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:52:07.316+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:52:07.315+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:52:07.430+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:52:07.477+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:52:07.477+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:52:07.507+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:52:07.507+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:52:07.531+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.228 seconds
[2023-09-10T15:52:38.123+0000] {processor.py:157} INFO - Started process (PID=100) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:52:38.131+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:52:38.139+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:52:38.138+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:52:38.307+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:52:38.719+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:52:38.718+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:52:38.827+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:52:38.826+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:52:38.886+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.768 seconds
[2023-09-10T15:53:09.433+0000] {processor.py:157} INFO - Started process (PID=116) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:53:09.449+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:53:09.463+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:53:09.462+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:53:09.781+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:53:09.917+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:53:09.916+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:53:10.010+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:53:10.009+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:53:10.093+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.677 seconds
[2023-09-10T15:53:41.027+0000] {processor.py:157} INFO - Started process (PID=126) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:53:41.035+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:53:41.049+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:53:41.048+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:53:41.488+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:53:42.001+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:53:41.999+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:53:42.142+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:53:42.141+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:53:42.210+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.193 seconds
[2023-09-10T15:55:34.167+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:55:34.179+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:55:34.192+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:55:34.191+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:55:35.046+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:55:35.533+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:55:35.532+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:55:35.585+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:55:35.583+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:55:35.641+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.491 seconds
[2023-09-10T15:56:06.168+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:56:06.189+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:56:06.248+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:56:06.233+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:56:09.977+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:56:10.760+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:56:10.732+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:56:10.985+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:56:10.979+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:56:11.145+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 5.024 seconds
[2023-09-10T15:56:42.143+0000] {processor.py:157} INFO - Started process (PID=50) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:56:42.148+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:56:42.159+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:56:42.158+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:56:43.392+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:56:43.705+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:56:43.703+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:56:43.759+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:56:43.759+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:56:43.815+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.693 seconds
[2023-09-10T15:57:14.167+0000] {processor.py:157} INFO - Started process (PID=61) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:57:14.181+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:57:14.208+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:57:14.206+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:57:14.561+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:57:14.671+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:57:14.670+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:57:14.736+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:57:14.735+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:57:14.767+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.620 seconds
[2023-09-10T15:57:45.226+0000] {processor.py:157} INFO - Started process (PID=71) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:57:45.266+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:57:45.322+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:57:45.320+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:57:45.731+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:57:46.423+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:57:46.418+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:57:46.952+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:57:46.927+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:57:47.159+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.952 seconds
[2023-09-10T15:58:03.478+0000] {processor.py:157} INFO - Started process (PID=81) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:58:03.487+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:58:03.502+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:58:03.500+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:58:03.913+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:58:04.127+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:58:04.126+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:58:04.204+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:58:04.203+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:58:04.259+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.810 seconds
[2023-09-10T15:58:34.975+0000] {processor.py:157} INFO - Started process (PID=92) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:58:34.986+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T15:58:35.011+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:58:35.009+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:58:35.203+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T15:58:35.264+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:58:35.264+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T15:58:35.334+0000] {logging_mixin.py:151} INFO - [2023-09-10T15:58:35.334+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T15:58:35.380+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.416 seconds
[2023-09-10T16:05:22.584+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:05:22.602+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T16:05:22.618+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:05:22.617+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:05:23.728+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:05:24.186+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:05:24.183+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:05:24.304+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:05:24.303+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:05:24.408+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.832 seconds
[2023-09-10T16:05:55.401+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:05:55.406+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T16:05:55.418+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:05:55.416+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:05:55.793+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:05:55.856+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:05:55.855+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:05:55.890+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:05:55.889+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:05:55.916+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.524 seconds
[2023-09-10T16:06:26.650+0000] {processor.py:157} INFO - Started process (PID=54) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:06:26.665+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T16:06:26.693+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:06:26.691+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:06:29.901+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:06:30.503+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:06:30.500+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:06:30.564+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:06:30.564+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:06:30.619+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 4.003 seconds
[2023-09-10T16:07:01.040+0000] {processor.py:157} INFO - Started process (PID=63) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:07:01.049+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T16:07:01.059+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:07:01.058+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:07:01.245+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:07:01.329+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:07:01.328+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:07:01.390+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:07:01.389+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:07:01.446+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.420 seconds
[2023-09-10T16:08:37.541+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:08:37.546+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T16:08:37.564+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:08:37.562+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:08:38.326+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:08:38.667+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:08:38.666+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:08:38.737+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:08:38.737+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:08:38.781+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.250 seconds
[2023-09-10T16:09:08.992+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:09:09.004+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T16:09:09.044+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:09:09.041+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:09:13.253+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:09:13.557+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:09:13.555+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:09:13.728+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:09:13.727+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:09:14.062+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 5.088 seconds
[2023-09-10T16:09:45.003+0000] {processor.py:157} INFO - Started process (PID=50) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:09:45.017+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T16:09:45.048+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:09:45.044+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:09:46.898+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:09:47.423+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:09:47.419+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:09:47.488+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:09:47.488+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:09:47.530+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 2.568 seconds
[2023-09-10T16:10:18.838+0000] {processor.py:157} INFO - Started process (PID=60) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:10:18.843+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T16:10:18.850+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:10:18.850+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:10:18.989+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:10:19.061+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:10:19.061+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:10:19.092+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:10:19.092+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:10:19.114+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.284 seconds
[2023-09-10T16:42:03.667+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:42:03.688+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T16:42:03.708+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:42:03.707+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:42:04.497+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:42:04.998+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:42:04.994+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:42:05.158+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:42:05.157+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:42:05.296+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:42:05.283+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"dagrun_timeout": 60.0, "_task_group": {"_group_id": null, "prefix_group_id": true, "tooltip": "", "ui_color": "CornflowerBl ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 16, 42, 4, 622294, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T16:42:05.310+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:42:05.310+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:42:05.316+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"dagrun_timeout": 60.0, "_task_group": {"_group_id": null, "prefix_group_id": true, "tooltip": "", "ui_color": "CornflowerBl ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 16, 42, 4, 622294, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T16:42:36.233+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:42:36.239+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T16:42:36.246+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:42:36.245+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:42:36.596+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:42:36.788+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:42:36.787+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:42:36.813+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:42:36.813+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:42:36.856+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.629 seconds
[2023-09-10T16:43:07.476+0000] {processor.py:157} INFO - Started process (PID=51) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:43:07.484+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T16:43:07.499+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:43:07.498+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:43:08.730+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:43:08.972+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:43:08.970+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:43:09.051+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:43:09.050+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:43:09.137+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.672 seconds
[2023-09-10T16:43:40.071+0000] {processor.py:157} INFO - Started process (PID=61) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:43:40.089+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T16:43:40.132+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:43:40.130+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:43:40.647+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:43:42.050+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:43:42.032+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:43:42.253+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:43:42.252+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:43:42.494+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 2.479 seconds
[2023-09-10T16:44:13.366+0000] {processor.py:157} INFO - Started process (PID=70) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:44:13.372+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T16:44:13.377+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:44:13.377+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:44:13.541+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:44:13.571+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:44:13.571+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:44:13.621+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:44:13.620+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:44:13.659+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.299 seconds
[2023-09-10T16:44:44.054+0000] {processor.py:157} INFO - Started process (PID=80) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:44:44.058+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T16:44:44.067+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:44:44.066+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:44:44.219+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:44:44.374+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:44:44.373+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:44:44.396+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:44:44.396+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:44:44.417+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.370 seconds
[2023-09-10T16:53:17.766+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:53:17.774+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T16:53:17.804+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:53:17.800+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:53:18.612+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:53:19.071+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:53:19.069+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:53:19.199+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:53:19.199+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:53:19.796+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:53:19.751+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"dataset_triggers": [], "fileloc": "/opt/airflow/dags/crawl_data_using_BeautifulSoup.py", "timezone": "UTC", "schedule_inter ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 16, 53, 18, 723896, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T16:53:19.842+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:53:19.841+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:53:19.856+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"dataset_triggers": [], "fileloc": "/opt/airflow/dags/crawl_data_using_BeautifulSoup.py", "timezone": "UTC", "schedule_inter ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 16, 53, 18, 723896, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T16:53:50.217+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:53:50.221+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T16:53:50.233+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:53:50.232+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:53:50.478+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:53:50.596+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:53:50.595+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:53:50.619+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:53:50.619+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:53:50.637+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.425 seconds
[2023-09-10T16:54:21.125+0000] {processor.py:157} INFO - Started process (PID=50) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:54:21.133+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T16:54:21.168+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:54:21.167+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:54:21.841+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:54:21.923+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:54:21.921+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:54:22.012+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:54:22.011+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:54:22.220+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.118 seconds
[2023-09-10T16:54:55.157+0000] {processor.py:157} INFO - Started process (PID=60) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:54:55.164+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T16:54:55.177+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:54:55.176+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:54:55.513+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:54:56.431+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:54:56.429+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:54:56.471+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:54:56.471+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:54:56.513+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.367 seconds
[2023-09-10T16:55:27.011+0000] {processor.py:157} INFO - Started process (PID=69) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:55:27.016+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T16:55:27.025+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:55:27.024+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:55:27.250+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:55:27.381+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:55:27.379+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:55:27.445+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:55:27.444+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:55:27.484+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.483 seconds
[2023-09-10T16:55:58.406+0000] {processor.py:157} INFO - Started process (PID=81) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:55:58.409+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T16:55:58.415+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:55:58.414+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:55:58.529+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:55:58.845+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:55:58.842+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:55:58.945+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:55:58.944+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:55:59.080+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.681 seconds
[2023-09-10T16:56:29.681+0000] {processor.py:157} INFO - Started process (PID=92) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:56:29.689+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T16:56:29.705+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:56:29.704+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:56:30.031+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:56:30.289+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:56:30.287+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:56:30.408+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:56:30.408+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:56:30.499+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.834 seconds
[2023-09-10T16:57:01.447+0000] {processor.py:157} INFO - Started process (PID=101) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:57:01.453+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T16:57:01.470+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:57:01.468+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:57:02.105+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:57:02.707+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:57:02.704+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:57:02.778+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:57:02.777+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:57:02.833+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.398 seconds
[2023-09-10T16:57:33.239+0000] {processor.py:157} INFO - Started process (PID=119) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:57:33.244+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T16:57:33.257+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:57:33.256+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:57:33.444+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:57:33.557+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:57:33.557+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:57:33.628+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:57:33.627+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:57:33.674+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.443 seconds
[2023-09-10T16:58:04.454+0000] {processor.py:157} INFO - Started process (PID=129) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:58:04.462+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T16:58:04.484+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:58:04.477+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:58:04.916+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T16:58:05.726+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:58:05.722+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T16:58:05.849+0000] {logging_mixin.py:151} INFO - [2023-09-10T16:58:05.848+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T16:58:06.145+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.697 seconds
[2023-09-10T17:01:48.301+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:01:48.324+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:01:48.377+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:01:48.375+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:01:51.053+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:01:51.731+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:01:51.725+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:01:51.883+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:01:51.882+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:01:51.973+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 3.692 seconds
[2023-09-10T17:02:22.858+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:02:22.862+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:02:22.867+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:02:22.866+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:02:23.474+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:02:23.581+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:02:23.580+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:02:23.637+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:02:23.637+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:02:23.682+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.829 seconds
[2023-09-10T17:02:54.603+0000] {processor.py:157} INFO - Started process (PID=51) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:02:54.612+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:02:54.630+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:02:54.628+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:02:55.550+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:02:55.915+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:02:55.914+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:02:55.974+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:02:55.974+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:02:56.036+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.467 seconds
[2023-09-10T17:03:26.701+0000] {processor.py:157} INFO - Started process (PID=61) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:03:26.707+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:03:26.719+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:03:26.718+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:03:26.941+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:03:27.047+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:03:27.046+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:03:27.112+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:03:27.112+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:03:27.172+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.484 seconds
[2023-09-10T17:03:57.967+0000] {processor.py:157} INFO - Started process (PID=71) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:03:57.972+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:03:57.983+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:03:57.982+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:03:58.196+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:03:58.526+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:03:58.525+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:03:58.584+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:03:58.584+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:03:58.691+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.733 seconds
[2023-09-10T17:04:29.525+0000] {processor.py:157} INFO - Started process (PID=88) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:04:29.544+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:04:29.603+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:04:29.583+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:04:30.129+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:04:30.239+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:04:30.237+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:04:30.317+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:04:30.317+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:04:30.369+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.869 seconds
[2023-09-10T17:05:01.218+0000] {processor.py:157} INFO - Started process (PID=98) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:05:01.225+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:05:01.247+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:05:01.245+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:05:01.381+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:05:01.838+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:05:01.837+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:05:01.872+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:05:01.872+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:05:01.904+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.696 seconds
[2023-09-10T17:05:32.452+0000] {processor.py:157} INFO - Started process (PID=107) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:05:32.459+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:05:32.467+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:05:32.466+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:05:32.617+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:05:32.700+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:05:32.700+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:05:32.775+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:05:32.774+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:05:32.824+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.382 seconds
[2023-09-10T17:06:03.970+0000] {processor.py:157} INFO - Started process (PID=117) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:06:03.975+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:06:03.987+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:06:03.986+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:06:04.283+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:06:04.636+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:06:04.636+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:06:04.672+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:06:04.672+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:06:04.705+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.741 seconds
[2023-09-10T17:06:35.603+0000] {processor.py:157} INFO - Started process (PID=127) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:06:35.613+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:06:35.627+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:06:35.624+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:06:36.054+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:06:36.168+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:06:36.167+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:06:36.216+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:06:36.216+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:06:36.252+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.670 seconds
[2023-09-10T17:07:07.532+0000] {processor.py:157} INFO - Started process (PID=136) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:07:07.665+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:07:07.772+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:07:07.753+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:07:09.478+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:07:10.040+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:07:10.037+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:07:10.515+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:07:10.511+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:07:10.649+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 3.234 seconds
[2023-09-10T17:07:40.951+0000] {processor.py:157} INFO - Started process (PID=147) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:07:40.968+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:07:40.984+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:07:40.983+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:07:41.258+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:07:41.370+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:07:41.369+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:07:41.440+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:07:41.439+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:07:41.483+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.552 seconds
[2023-09-10T17:08:12.961+0000] {processor.py:157} INFO - Started process (PID=157) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:08:12.972+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:08:12.988+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:08:12.987+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:08:13.257+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:08:13.602+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:08:13.601+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:08:13.653+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:08:13.652+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:08:13.696+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.756 seconds
[2023-09-10T17:08:44.465+0000] {processor.py:157} INFO - Started process (PID=167) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:08:44.481+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:08:44.495+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:08:44.494+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:08:44.785+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:08:44.921+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:08:44.920+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:08:45.014+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:08:45.014+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:08:45.090+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.635 seconds
[2023-09-10T17:09:16.681+0000] {processor.py:157} INFO - Started process (PID=177) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:09:16.702+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:09:16.777+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:09:16.775+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:09:17.299+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:09:17.958+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:09:17.954+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:09:18.031+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:09:18.031+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:09:18.128+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.492 seconds
[2023-09-10T17:09:48.857+0000] {processor.py:157} INFO - Started process (PID=186) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:09:48.872+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:09:48.914+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:09:48.909+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:09:49.316+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:09:49.516+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:09:49.514+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:09:49.615+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:09:49.615+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:09:49.667+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.828 seconds
[2023-09-10T17:10:20.741+0000] {processor.py:157} INFO - Started process (PID=196) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:10:20.751+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:10:20.775+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:10:20.772+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:10:21.322+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:10:21.920+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:10:21.918+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:10:22.063+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:10:22.061+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:10:22.244+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.556 seconds
[2023-09-10T17:10:52.406+0000] {processor.py:157} INFO - Started process (PID=206) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:10:52.410+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:10:52.416+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:10:52.416+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:10:52.543+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:10:52.572+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:10:52.571+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:10:52.603+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:10:52.603+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:10:52.627+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.227 seconds
[2023-09-10T17:11:23.043+0000] {processor.py:157} INFO - Started process (PID=216) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:11:23.047+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:11:23.058+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:11:23.057+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:11:23.257+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:11:23.574+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:11:23.573+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:11:23.631+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:11:23.630+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:11:23.675+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.638 seconds
[2023-09-10T17:11:54.330+0000] {processor.py:157} INFO - Started process (PID=226) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:11:54.338+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:11:54.358+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:11:54.354+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:11:54.829+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:11:54.962+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:11:54.961+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:11:55.137+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:11:55.137+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:11:55.354+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.042 seconds
[2023-09-10T17:12:26.301+0000] {processor.py:157} INFO - Started process (PID=236) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:12:26.312+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:12:26.342+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:12:26.341+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:12:26.956+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:12:27.585+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:12:27.583+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:12:27.694+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:12:27.693+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:12:27.820+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.539 seconds
[2023-09-10T17:21:42.463+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:21:42.481+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:21:42.502+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:21:42.500+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:21:43.366+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:21:44.205+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:21:44.201+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:21:44.286+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:21:44.286+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:21:44.857+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:21:44.823+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"dataset_triggers": [], "_task_group": {"_group_id": null, "prefix_group_id": true, "tooltip": "", "ui_color": "CornflowerBl ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 17, 21, 43, 681568, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T17:21:44.866+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:21:44.866+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:21:44.894+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"dataset_triggers": [], "_task_group": {"_group_id": null, "prefix_group_id": true, "tooltip": "", "ui_color": "CornflowerBl ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 17, 21, 43, 681568, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T17:22:15.638+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:22:15.643+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:22:15.649+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:22:15.649+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:22:15.867+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:22:15.972+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:22:15.971+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:22:15.991+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:22:15.991+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:22:16.011+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.377 seconds
[2023-09-10T17:22:46.819+0000] {processor.py:157} INFO - Started process (PID=51) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:22:46.830+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:22:46.840+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:22:46.839+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:22:47.169+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:22:47.195+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:22:47.195+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:22:47.224+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:22:47.224+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:22:47.251+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.441 seconds
[2023-09-10T17:23:18.393+0000] {processor.py:157} INFO - Started process (PID=60) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:23:18.402+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:23:18.420+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:23:18.418+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:23:18.679+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:23:19.060+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:23:19.058+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:23:19.211+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:23:19.210+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:23:19.264+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.896 seconds
[2023-09-10T17:25:01.740+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:25:01.754+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:25:01.789+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:25:01.786+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:25:02.418+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:25:02.955+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:25:02.954+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:25:03.092+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:25:03.090+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:25:03.162+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:25:03.152+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"fileloc": "/opt/airflow/dags/crawl_data_using_BeautifulSoup.py", "dagrun_timeout": 60.0, "timezone": "UTC", "dataset_trigge ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 17, 25, 2, 590721, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T17:25:03.182+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:25:03.181+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:25:03.189+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"fileloc": "/opt/airflow/dags/crawl_data_using_BeautifulSoup.py", "dagrun_timeout": 60.0, "timezone": "UTC", "dataset_trigge ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 17, 25, 2, 590721, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T17:25:33.933+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:25:33.938+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:25:33.947+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:25:33.946+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:25:34.393+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:25:34.609+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:25:34.608+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:25:34.643+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:25:34.643+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:25:34.699+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.780 seconds
[2023-09-10T17:26:04.838+0000] {processor.py:157} INFO - Started process (PID=51) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:26:04.845+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:26:04.853+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:26:04.852+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:26:07.057+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:26:07.328+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:26:07.326+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:26:07.438+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:26:07.437+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:26:07.511+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 2.685 seconds
[2023-09-10T17:26:39.193+0000] {processor.py:157} INFO - Started process (PID=61) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:26:39.200+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:26:39.213+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:26:39.212+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:26:39.420+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:26:39.465+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:26:39.464+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:26:39.506+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:26:39.506+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:26:39.533+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.354 seconds
[2023-09-10T17:27:10.832+0000] {processor.py:157} INFO - Started process (PID=72) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:27:10.845+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:27:10.880+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:27:10.877+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:27:11.620+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:27:12.374+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:27:12.370+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:27:12.526+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:27:12.522+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:27:12.636+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.842 seconds
[2023-09-10T17:27:42.927+0000] {processor.py:157} INFO - Started process (PID=82) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:27:42.933+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:27:42.955+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:27:42.952+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:27:43.193+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:27:43.261+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:27:43.261+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:27:43.325+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:27:43.324+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:27:43.373+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.462 seconds
[2023-09-10T17:28:14.143+0000] {processor.py:157} INFO - Started process (PID=91) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:28:14.152+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:28:14.166+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:28:14.164+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:28:14.553+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:28:14.692+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:28:14.688+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:28:14.902+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:28:14.900+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:28:15.027+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.898 seconds
[2023-09-10T17:28:45.847+0000] {processor.py:157} INFO - Started process (PID=107) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:28:45.851+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:28:45.856+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:28:45.855+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:28:46.008+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:28:46.204+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:28:46.202+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:28:46.242+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:28:46.241+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:28:46.270+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.427 seconds
[2023-09-10T17:29:16.729+0000] {processor.py:157} INFO - Started process (PID=116) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:29:16.734+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:29:16.741+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:29:16.740+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:29:16.893+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:29:16.993+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:29:16.992+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:29:17.091+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:29:17.091+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:29:17.149+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.427 seconds
[2023-09-10T17:29:47.485+0000] {processor.py:157} INFO - Started process (PID=126) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:29:47.488+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:29:47.493+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:29:47.492+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:29:47.623+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:29:47.652+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:29:47.652+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:29:47.701+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:29:47.701+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:29:47.733+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.253 seconds
[2023-09-10T17:30:17.891+0000] {processor.py:157} INFO - Started process (PID=136) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:30:17.894+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:30:17.900+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:30:17.899+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:30:18.032+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:30:18.238+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:30:18.238+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:30:18.287+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:30:18.287+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:30:18.324+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.438 seconds
[2023-09-10T17:32:20.225+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:32:20.240+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:32:20.307+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:32:20.305+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:32:21.427+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:32:23.744+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:32:23.721+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:32:23.959+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:32:23.958+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:32:24.053+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 3.879 seconds
[2023-09-10T17:32:54.794+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:32:54.799+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:32:54.809+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:32:54.808+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:32:55.281+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:32:55.374+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:32:55.373+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:32:55.535+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:32:55.534+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:32:55.587+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.799 seconds
[2023-09-10T17:33:26.426+0000] {processor.py:157} INFO - Started process (PID=49) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:33:26.451+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:33:26.479+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:33:26.462+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:33:32.374+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:33:33.575+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:33:33.559+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:33:33.672+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:33:33.672+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:33:33.757+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 7.369 seconds
[2023-09-10T17:34:04.103+0000] {processor.py:157} INFO - Started process (PID=67) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:34:04.112+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:34:04.125+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:34:04.123+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:34:04.412+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:34:04.594+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:34:04.593+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:34:04.670+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:34:04.670+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:34:04.738+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.662 seconds
[2023-09-10T17:34:35.154+0000] {processor.py:157} INFO - Started process (PID=76) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:34:35.170+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:34:35.189+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:34:35.186+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:34:35.499+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:34:35.914+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:34:35.913+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:34:35.985+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:34:35.984+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:34:36.028+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.899 seconds
[2023-09-10T17:35:06.957+0000] {processor.py:157} INFO - Started process (PID=87) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:35:06.964+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:35:06.979+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:35:06.978+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:35:07.261+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:35:07.410+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:35:07.409+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:35:07.464+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:35:07.464+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:35:07.503+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.559 seconds
[2023-09-10T17:41:22.176+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:41:22.183+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:41:22.212+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:41:22.208+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:41:22.959+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:41:23.456+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:41:23.442+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:41:23.608+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:41:23.607+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:41:23.958+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:41:23.936+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"dagrun_timeout": 60.0, "dataset_triggers": [], "_dag_id": "website_crawler", "timezone": "UTC", "edge_info": {}, "_task_gro ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 17, 41, 23, 65148, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T17:41:23.967+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:41:23.967+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:41:23.987+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_using_BeautifulSoup.py', 'fileloc_hash': 57524288905393048, 'data': '{"__version": 1, "dag": {"dagrun_timeout": 60.0, "dataset_triggers": [], "_dag_id": "website_crawler", "timezone": "UTC", "edge_info": {}, "_task_gro ... (1133 characters truncated) ... onOperator", "_task_module": "airflow.operators.python", "_is_empty": false, "op_args": [], "op_kwargs": {}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 10, 17, 41, 23, 65148, tzinfo=Timezone('UTC')), 'dag_hash': 'd1b7687b611f6693bbf23a8b8675ff15', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-10T17:41:54.562+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:41:54.582+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:41:54.589+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:41:54.588+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:41:54.902+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:41:55.174+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:41:55.173+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:41:55.213+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:41:55.213+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:41:55.248+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.693 seconds
[2023-09-10T17:42:29.829+0000] {processor.py:157} INFO - Started process (PID=53) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:42:29.974+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:42:30.092+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:42:30.090+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:42:33.753+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:42:35.001+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:42:34.987+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:42:35.242+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:42:35.242+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:42:35.379+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 5.984 seconds
[2023-09-10T17:43:07.125+0000] {processor.py:157} INFO - Started process (PID=70) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:43:07.151+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:43:07.205+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:43:07.192+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:43:09.567+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:43:17.582+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:43:17.566+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:43:17.965+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:43:17.962+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:43:18.169+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 11.127 seconds
[2023-09-10T17:43:49.205+0000] {processor.py:157} INFO - Started process (PID=81) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:43:49.211+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:43:49.218+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:43:49.217+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:43:49.361+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:43:49.404+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:43:49.404+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:43:49.497+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:43:49.497+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:43:49.581+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.384 seconds
[2023-09-10T17:44:20.514+0000] {processor.py:157} INFO - Started process (PID=91) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:44:20.521+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:44:20.529+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:44:20.528+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:44:20.759+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:44:21.166+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:44:21.165+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:44:21.201+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:44:21.200+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:44:21.233+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 0.728 seconds
[2023-09-10T17:44:51.885+0000] {processor.py:157} INFO - Started process (PID=100) to work on /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:44:51.891+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_using_BeautifulSoup.py for tasks to queue
[2023-09-10T17:44:51.906+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:44:51.904+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:44:52.510+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_using_BeautifulSoup.py
[2023-09-10T17:44:52.835+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:44:52.821+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-10T17:44:53.000+0000] {logging_mixin.py:151} INFO - [2023-09-10T17:44:52.999+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00
[2023-09-10T17:44:53.046+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_using_BeautifulSoup.py took 1.173 seconds
