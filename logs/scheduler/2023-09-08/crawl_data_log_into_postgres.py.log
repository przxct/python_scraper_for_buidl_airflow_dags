[2023-09-08T03:43:14.170+0000] {processor.py:157} INFO - Started process (PID=31) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T03:43:14.180+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T03:43:14.198+0000] {logging_mixin.py:151} INFO - [2023-09-08T03:43:14.198+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T03:43:14.763+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T03:43:15.119+0000] {logging_mixin.py:151} INFO - [2023-09-08T03:43:15.118+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T03:43:15.201+0000] {logging_mixin.py:151} INFO - [2023-09-08T03:43:15.201+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00
[2023-09-08T03:43:15.556+0000] {logging_mixin.py:151} INFO - [2023-09-08T03:43:15.553+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"default_args": {"__var": {"owner": "airflow", "depends_on_past": false, "start_date": {"__var": 1693180800.0, "__type": "da ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 8, 3, 43, 14, 855182, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-08T03:43:15.559+0000] {logging_mixin.py:151} INFO - [2023-09-08T03:43:15.559+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T03:43:15.562+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"default_args": {"__var": {"owner": "airflow", "depends_on_past": false, "start_date": {"__var": 1693180800.0, "__type": "da ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 8, 3, 43, 14, 855182, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-08T03:43:45.993+0000] {processor.py:157} INFO - Started process (PID=44) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T03:43:45.999+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T03:43:46.015+0000] {logging_mixin.py:151} INFO - [2023-09-08T03:43:46.013+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T03:43:46.223+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T03:43:46.464+0000] {logging_mixin.py:151} INFO - [2023-09-08T03:43:46.463+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T03:43:46.500+0000] {logging_mixin.py:151} INFO - [2023-09-08T03:43:46.499+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T03:43:46.518+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.538 seconds
[2023-09-08T03:44:16.907+0000] {processor.py:157} INFO - Started process (PID=54) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T03:44:16.913+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T03:44:16.926+0000] {logging_mixin.py:151} INFO - [2023-09-08T03:44:16.923+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T03:44:17.073+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T03:44:17.162+0000] {logging_mixin.py:151} INFO - [2023-09-08T03:44:17.162+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T03:44:17.201+0000] {logging_mixin.py:151} INFO - [2023-09-08T03:44:17.201+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T03:44:17.221+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.330 seconds
[2023-09-08T03:56:23.382+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T03:56:23.391+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T03:56:23.398+0000] {logging_mixin.py:151} INFO - [2023-09-08T03:56:23.397+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T03:56:24.625+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T03:56:25.228+0000] {logging_mixin.py:151} INFO - [2023-09-08T03:56:25.226+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T03:56:25.313+0000] {logging_mixin.py:151} INFO - [2023-09-08T03:56:25.312+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T03:56:25.721+0000] {logging_mixin.py:151} INFO - [2023-09-08T03:56:25.706+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"schedule_interval": "@daily", "edge_info": {}, "default_args": {"__var": {"owner": "airflow", "depends_on_past": false, "st ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 8, 3, 56, 24, 782302, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-08T03:56:25.733+0000] {logging_mixin.py:151} INFO - [2023-09-08T03:56:25.732+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T03:56:25.738+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"schedule_interval": "@daily", "edge_info": {}, "default_args": {"__var": {"owner": "airflow", "depends_on_past": false, "st ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 8, 3, 56, 24, 782302, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-08T03:56:56.147+0000] {processor.py:157} INFO - Started process (PID=42) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T03:56:56.152+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T03:56:56.160+0000] {logging_mixin.py:151} INFO - [2023-09-08T03:56:56.159+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T03:56:56.684+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T03:56:56.836+0000] {logging_mixin.py:151} INFO - [2023-09-08T03:56:56.835+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T03:56:56.870+0000] {logging_mixin.py:151} INFO - [2023-09-08T03:56:56.870+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T03:56:56.886+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.746 seconds
[2023-09-08T03:57:27.780+0000] {processor.py:157} INFO - Started process (PID=52) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T03:57:27.787+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T03:57:27.802+0000] {logging_mixin.py:151} INFO - [2023-09-08T03:57:27.800+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T03:57:29.674+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T03:57:29.724+0000] {logging_mixin.py:151} INFO - [2023-09-08T03:57:29.723+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T03:57:29.749+0000] {logging_mixin.py:151} INFO - [2023-09-08T03:57:29.749+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T03:57:29.767+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.997 seconds
[2023-09-08T03:58:00.708+0000] {processor.py:157} INFO - Started process (PID=61) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T03:58:00.714+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T03:58:00.730+0000] {logging_mixin.py:151} INFO - [2023-09-08T03:58:00.729+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T03:58:01.176+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:03:28.850+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:03:28.855+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:03:28.862+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:03:28.861+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:03:29.283+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:03:29.489+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:03:29.487+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:03:29.521+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:03:29.520+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:03:29.546+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.705 seconds
[2023-09-08T04:03:58.312+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:03:58.317+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:03:58.322+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:03:58.322+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:03:58.666+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:03:58.692+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:03:58.692+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:03:58.713+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:03:58.712+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:03:58.741+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.439 seconds
[2023-09-08T04:04:29.180+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:04:29.183+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:04:29.189+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:04:29.188+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:04:29.518+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:04:29.578+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:04:29.578+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:04:29.611+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:04:29.611+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:04:29.644+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.472 seconds
[2023-09-08T04:05:00.345+0000] {processor.py:157} INFO - Started process (PID=49) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:05:00.353+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:05:00.376+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:05:00.375+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:05:01.929+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:05:02.166+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:05:02.165+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:05:02.189+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:05:02.189+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:05:02.212+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.876 seconds
[2023-09-08T04:05:32.659+0000] {processor.py:157} INFO - Started process (PID=59) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:05:32.667+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:05:32.674+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:05:32.673+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:05:32.794+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:05:32.826+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:05:32.825+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:05:32.849+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:05:32.849+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:05:32.868+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.218 seconds
[2023-09-08T04:20:17.760+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:20:17.765+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:20:17.773+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:20:17.773+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:20:18.303+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:20:18.501+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:20:18.500+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:20:18.534+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:20:18.533+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:20:18.564+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.813 seconds
[2023-09-08T04:20:49.593+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:20:49.600+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:20:49.607+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:20:49.606+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:20:49.974+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:20:50.011+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:20:50.011+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:20:50.046+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:20:50.046+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:20:50.089+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.517 seconds
[2023-09-08T04:21:20.417+0000] {processor.py:157} INFO - Started process (PID=50) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:21:20.428+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:21:20.445+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:21:20.443+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:21:21.268+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:21:21.486+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:21:21.486+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:21:21.516+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:21:21.515+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:21:21.537+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.149 seconds
[2023-09-08T04:21:51.896+0000] {processor.py:157} INFO - Started process (PID=60) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:21:51.898+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:21:51.901+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:21:51.901+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:21:52.035+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:21:52.078+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:21:52.077+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:21:52.107+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:21:52.107+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:21:52.131+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.240 seconds
[2023-09-08T04:22:22.673+0000] {processor.py:157} INFO - Started process (PID=70) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:22:22.676+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:22:22.682+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:22:22.681+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:22:22.812+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:22:22.964+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:22:22.963+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:22:22.990+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:22:22.990+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:22:23.008+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.343 seconds
[2023-09-08T04:22:53.433+0000] {processor.py:157} INFO - Started process (PID=80) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:22:53.439+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:22:53.445+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:22:53.444+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:22:53.566+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:22:53.599+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:22:53.598+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:22:53.636+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:22:53.636+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:22:53.656+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.234 seconds
[2023-09-08T04:23:24.241+0000] {processor.py:157} INFO - Started process (PID=90) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:23:24.246+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:23:24.255+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:23:24.254+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:23:24.471+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:23:24.741+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:23:24.740+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:23:24.780+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:23:24.779+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:23:24.814+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.584 seconds
[2023-09-08T04:23:55.240+0000] {processor.py:157} INFO - Started process (PID=100) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:23:55.245+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:23:55.267+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:23:55.259+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:23:55.395+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:23:55.448+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:23:55.448+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:23:55.482+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:23:55.482+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:23:55.502+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.278 seconds
[2023-09-08T04:24:26.057+0000] {processor.py:157} INFO - Started process (PID=110) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:24:26.061+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:24:26.067+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:24:26.067+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:24:26.189+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:24:26.367+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:24:26.366+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:24:26.391+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:24:26.391+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:24:26.408+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.364 seconds
[2023-09-08T04:24:56.851+0000] {processor.py:157} INFO - Started process (PID=120) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:24:56.857+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:24:56.865+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:24:56.864+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:24:57.003+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:24:57.051+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:24:57.050+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:24:57.086+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:24:57.086+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:24:57.115+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.282 seconds
[2023-09-08T04:25:27.663+0000] {processor.py:157} INFO - Started process (PID=130) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:25:27.666+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:25:27.673+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:25:27.673+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:25:27.814+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:25:27.964+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:25:27.964+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:25:27.990+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:25:27.989+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:25:28.007+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.353 seconds
[2023-09-08T04:25:58.617+0000] {processor.py:157} INFO - Started process (PID=140) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:25:58.622+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:25:58.633+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:25:58.631+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:25:58.803+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:25:58.855+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:25:58.854+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:25:58.895+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:25:58.894+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:25:58.927+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.322 seconds
[2023-09-08T04:26:29.503+0000] {processor.py:157} INFO - Started process (PID=150) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:26:29.509+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:26:29.533+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:26:29.531+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:26:29.749+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:26:30.067+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:26:30.066+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:26:30.104+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:26:30.104+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:26:30.127+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.649 seconds
[2023-09-08T04:27:00.876+0000] {processor.py:157} INFO - Started process (PID=161) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:27:00.882+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:27:00.892+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:27:00.891+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:27:01.050+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:27:01.100+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:27:01.099+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:27:01.146+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:27:01.146+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:27:01.170+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.305 seconds
[2023-09-08T04:27:31.708+0000] {processor.py:157} INFO - Started process (PID=171) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:27:31.713+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:27:31.721+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:27:31.720+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:27:31.867+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:27:32.024+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:27:32.023+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:27:32.045+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:27:32.045+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:27:32.066+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.372 seconds
[2023-09-08T04:28:02.464+0000] {processor.py:157} INFO - Started process (PID=181) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:28:02.468+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:28:02.481+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:28:02.480+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:28:02.636+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:28:02.683+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:28:02.682+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:28:02.730+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:28:02.730+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:28:02.755+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.304 seconds
[2023-09-08T04:28:33.739+0000] {processor.py:157} INFO - Started process (PID=191) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:28:33.881+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:28:34.038+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:28:34.034+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:28:35.174+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:28:35.722+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:28:35.717+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:28:35.807+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:28:35.807+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:28:35.867+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.525 seconds
[2023-09-08T04:29:06.614+0000] {processor.py:157} INFO - Started process (PID=201) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:29:06.621+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:29:06.630+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:29:06.629+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:29:06.787+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:29:06.830+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:29:06.829+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:29:06.874+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:29:06.874+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:29:06.900+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.305 seconds
[2023-09-08T04:29:37.516+0000] {processor.py:157} INFO - Started process (PID=210) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:29:37.519+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:29:37.525+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:29:37.525+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:29:37.670+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:29:37.833+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:29:37.833+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:29:37.856+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:29:37.855+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:29:37.875+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.368 seconds
[2023-09-08T04:30:08.247+0000] {processor.py:157} INFO - Started process (PID=220) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:30:08.252+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:30:08.258+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:30:08.257+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:30:08.399+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:30:08.442+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:30:08.441+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:30:08.472+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:30:08.472+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:30:08.505+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.269 seconds
[2023-09-08T04:30:39.055+0000] {processor.py:157} INFO - Started process (PID=230) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:30:39.059+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:30:39.066+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:30:39.065+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:30:39.231+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:30:39.415+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:30:39.414+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:30:39.442+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:30:39.442+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:30:39.470+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.425 seconds
[2023-09-08T04:31:09.913+0000] {processor.py:157} INFO - Started process (PID=240) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:31:09.917+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:31:09.926+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:31:09.925+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:31:10.081+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:31:10.144+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:31:10.143+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:31:10.177+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:31:10.176+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:31:10.197+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.295 seconds
[2023-09-08T04:31:41.026+0000] {processor.py:157} INFO - Started process (PID=250) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:31:41.030+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:31:41.041+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:31:41.040+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:31:41.207+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:31:41.496+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:31:41.495+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:31:41.528+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:31:41.527+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:31:41.551+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.535 seconds
[2023-09-08T04:32:12.089+0000] {processor.py:157} INFO - Started process (PID=265) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:32:12.093+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:32:12.100+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:32:12.099+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:32:12.250+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:32:12.296+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:32:12.295+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:32:12.330+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:32:12.330+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:32:12.353+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.272 seconds
[2023-09-08T04:32:42.881+0000] {processor.py:157} INFO - Started process (PID=275) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:32:42.884+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:32:42.890+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:32:42.890+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:32:43.020+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:32:43.158+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:32:43.157+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:32:43.184+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:32:43.184+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:32:43.205+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.331 seconds
[2023-09-08T04:33:13.635+0000] {processor.py:157} INFO - Started process (PID=285) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:33:13.639+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:33:13.653+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:33:13.651+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:33:13.843+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:33:13.923+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:33:13.922+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:33:13.962+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:33:13.962+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:33:13.985+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.362 seconds
[2023-09-08T04:33:45.592+0000] {processor.py:157} INFO - Started process (PID=296) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:33:45.648+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:33:45.721+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:33:45.720+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:33:46.103+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:33:46.451+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:33:46.451+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:33:46.503+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:33:46.502+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:33:46.564+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.083 seconds
[2023-09-08T04:34:17.180+0000] {processor.py:157} INFO - Started process (PID=306) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:34:17.183+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:34:17.188+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:34:17.188+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:34:17.296+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:34:17.328+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:34:17.328+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:34:17.353+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:34:17.352+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:34:17.370+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.196 seconds
[2023-09-08T04:34:47.858+0000] {processor.py:157} INFO - Started process (PID=316) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:34:47.861+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:34:47.864+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:34:47.864+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:34:47.957+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:34:48.067+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:34:48.067+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:34:48.092+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:34:48.091+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:34:48.111+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.259 seconds
[2023-09-08T04:35:18.617+0000] {processor.py:157} INFO - Started process (PID=326) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:35:18.625+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:35:18.633+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:35:18.632+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:35:18.783+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:35:18.850+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:35:18.850+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:35:18.894+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:35:18.894+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:35:18.942+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.351 seconds
[2023-09-08T04:35:49.656+0000] {processor.py:157} INFO - Started process (PID=336) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:35:49.672+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:35:49.687+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:35:49.685+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:35:49.914+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:35:50.111+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:35:50.111+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:35:50.147+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:35:50.147+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:35:50.190+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.549 seconds
[2023-09-08T04:36:20.635+0000] {processor.py:157} INFO - Started process (PID=346) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:36:20.638+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:36:20.646+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:36:20.645+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:36:20.796+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:36:20.872+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:36:20.871+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:36:20.914+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:36:20.913+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:36:20.946+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.321 seconds
[2023-09-08T04:36:51.533+0000] {processor.py:157} INFO - Started process (PID=356) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:36:51.538+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:36:51.547+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:36:51.545+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:36:51.756+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:36:51.944+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:36:51.944+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:36:51.970+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:36:51.969+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:36:51.989+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.467 seconds
[2023-09-08T04:37:22.398+0000] {processor.py:157} INFO - Started process (PID=366) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:37:22.402+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:37:22.410+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:37:22.409+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:37:22.583+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:37:22.648+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:37:22.648+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:37:22.691+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:37:22.690+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:37:22.734+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.345 seconds
[2023-09-08T04:37:23.952+0000] {processor.py:157} INFO - Started process (PID=369) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:37:23.955+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:37:23.961+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:37:23.960+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:37:24.162+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:37:24.187+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:37:24.187+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:37:24.215+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:37:24.214+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:37:24.253+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.306 seconds
[2023-09-08T04:38:24.315+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:38:24.319+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:38:24.327+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:38:24.326+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:38:24.861+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:38:25.090+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:38:25.089+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:38:25.198+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:38:25.198+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:38:25.382+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:38:25.378+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/crawl_data_log_into_postgres.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 636, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"edge_info": {}, "dagrun_timeout": 60.0, "default_args": {"__var": {"owner": "airflow", "depends_on_past": false, "start_dat ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 8, 4, 38, 24, 909235, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-08T04:38:25.392+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:38:25.391+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:38:25.397+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 671, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2919, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(website_crawler) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'website_crawler', 'fileloc': '/opt/airflow/dags/crawl_data_log_into_postgres.py', 'fileloc_hash': 53823519574956812, 'data': '{"__version": 1, "dag": {"edge_info": {}, "dagrun_timeout": 60.0, "default_args": {"__var": {"owner": "airflow", "depends_on_past": false, "start_dat ... (2052 characters truncated) ... \\n            time CHAR(5000) NOT NULL,\\n            summary CHAR(5000) NOT NULL);\\n          \\"\\"\\""}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2023, 9, 8, 4, 38, 24, 909235, tzinfo=Timezone('UTC')), 'dag_hash': 'ea2eff1d671d601d6f935a5ec91eae24', 'processor_subdir': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2023-09-08T04:38:55.594+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:38:55.599+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:38:55.608+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:38:55.607+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:38:56.405+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:38:56.769+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:38:56.768+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:38:56.791+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:38:56.790+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:38:56.810+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.225 seconds
[2023-09-08T04:39:27.387+0000] {processor.py:157} INFO - Started process (PID=49) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:39:27.393+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:39:27.406+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:39:27.405+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:39:27.965+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:39:28.024+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:39:28.023+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:39:28.052+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:39:28.052+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:39:28.072+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.704 seconds
[2023-09-08T04:39:58.644+0000] {processor.py:157} INFO - Started process (PID=59) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:39:58.649+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:39:58.654+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:39:58.654+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:39:58.830+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:39:59.042+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:39:59.042+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:39:59.067+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:39:59.067+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:39:59.088+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.453 seconds
[2023-09-08T04:40:29.829+0000] {processor.py:157} INFO - Started process (PID=69) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:40:29.833+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:40:29.842+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:40:29.841+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:40:30.004+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:40:30.059+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:40:30.058+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:40:30.089+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:40:30.089+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:40:30.108+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.291 seconds
[2023-09-08T04:41:00.668+0000] {processor.py:157} INFO - Started process (PID=78) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:41:00.671+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:41:00.679+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:41:00.678+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:41:00.822+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:41:00.963+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:41:00.962+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:41:00.984+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:41:00.984+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:41:01.004+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.345 seconds
[2023-09-08T04:41:31.557+0000] {processor.py:157} INFO - Started process (PID=88) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:41:31.564+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:41:31.577+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:41:31.575+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:41:31.749+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:41:31.807+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:41:31.807+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:41:31.839+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:41:31.838+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:41:31.869+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.332 seconds
[2023-09-08T04:42:02.717+0000] {processor.py:157} INFO - Started process (PID=98) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:42:02.722+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:42:02.735+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:42:02.735+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:42:03.209+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:42:04.005+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:42:03.974+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:42:04.150+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:42:04.150+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:42:04.222+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.540 seconds
[2023-09-08T04:42:34.867+0000] {processor.py:157} INFO - Started process (PID=108) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:42:34.927+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:42:35.104+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:42:35.098+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:42:35.726+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:42:35.841+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:42:35.839+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:42:35.911+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:42:35.911+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:42:35.994+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.171 seconds
[2023-09-08T04:43:06.483+0000] {processor.py:157} INFO - Started process (PID=118) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:43:06.486+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:43:06.493+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:43:06.492+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:43:06.645+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:43:06.822+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:43:06.822+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:43:06.847+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:43:06.847+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:43:06.869+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.394 seconds
[2023-09-08T04:43:37.742+0000] {processor.py:157} INFO - Started process (PID=128) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:43:37.746+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:43:37.756+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:43:37.755+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:43:37.922+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:43:37.966+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:43:37.965+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:43:37.997+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:43:37.996+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:43:38.021+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.295 seconds
[2023-09-08T04:44:08.829+0000] {processor.py:157} INFO - Started process (PID=138) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:44:08.832+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:44:08.840+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:44:08.839+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:44:09.025+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:44:09.204+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:44:09.204+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:44:09.230+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:44:09.230+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:44:09.249+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.430 seconds
[2023-09-08T04:44:39.873+0000] {processor.py:157} INFO - Started process (PID=149) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:44:39.886+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:44:39.903+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:44:39.901+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:44:40.174+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:44:40.265+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:44:40.264+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:44:40.340+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:44:40.339+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:44:40.405+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.553 seconds
[2023-09-08T04:45:11.002+0000] {processor.py:157} INFO - Started process (PID=160) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:45:11.008+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:45:11.026+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:45:11.024+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:45:11.239+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:45:11.480+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:45:11.478+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:45:11.523+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:45:11.523+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:45:11.557+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.581 seconds
[2023-09-08T04:45:41.943+0000] {processor.py:157} INFO - Started process (PID=170) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:45:41.946+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:45:41.953+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:45:41.952+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:45:42.079+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:45:42.161+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:45:42.158+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:45:42.203+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:45:42.203+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:45:42.228+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.293 seconds
[2023-09-08T04:46:12.914+0000] {processor.py:157} INFO - Started process (PID=180) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:46:12.919+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:46:12.933+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:46:12.931+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:46:13.088+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:46:13.243+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:46:13.242+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:46:13.266+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:46:13.265+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:46:13.283+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.384 seconds
[2023-09-08T04:46:43.678+0000] {processor.py:157} INFO - Started process (PID=190) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:46:43.681+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:46:43.688+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:46:43.687+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:46:43.846+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:46:43.889+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:46:43.889+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:46:43.921+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:46:43.921+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:46:43.944+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.274 seconds
[2023-09-08T04:47:14.461+0000] {processor.py:157} INFO - Started process (PID=200) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:47:14.469+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:47:14.479+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:47:14.478+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:47:14.614+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:47:14.747+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:47:14.747+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:47:14.768+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:47:14.768+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:47:14.789+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.336 seconds
[2023-09-08T04:47:45.139+0000] {processor.py:157} INFO - Started process (PID=209) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:47:45.146+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:47:45.154+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:47:45.154+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:47:45.272+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:47:45.308+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:47:45.307+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:47:45.332+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:47:45.332+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:47:45.350+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.216 seconds
[2023-09-08T04:56:30.180+0000] {processor.py:157} INFO - Started process (PID=30) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:56:30.190+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:56:30.222+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:56:30.219+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:56:30.989+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:56:31.229+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:56:31.228+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:56:31.263+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:56:31.262+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:56:31.318+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 1.148 seconds
[2023-09-08T04:57:01.541+0000] {processor.py:157} INFO - Started process (PID=40) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:57:01.544+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:57:01.550+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:57:01.549+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:57:01.978+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:57:02.037+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:57:02.037+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:57:02.068+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:57:02.068+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:57:02.101+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.568 seconds
[2023-09-08T04:57:33.406+0000] {processor.py:157} INFO - Started process (PID=50) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:57:33.442+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:57:33.568+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:57:33.557+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:57:35.803+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:57:36.069+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:57:36.068+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:57:36.101+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:57:36.101+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:57:36.138+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 2.771 seconds
[2023-09-08T04:58:06.706+0000] {processor.py:157} INFO - Started process (PID=61) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:58:06.709+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:58:06.714+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:58:06.713+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:58:06.856+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:58:06.899+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:58:06.898+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:58:06.923+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:58:06.923+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:58:06.938+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.240 seconds
[2023-09-08T04:58:37.755+0000] {processor.py:157} INFO - Started process (PID=70) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:58:37.767+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:58:37.781+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:58:37.780+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:58:37.951+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:58:38.114+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:58:38.113+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:58:38.141+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:58:38.141+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:58:38.165+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.426 seconds
[2023-09-08T04:59:08.868+0000] {processor.py:157} INFO - Started process (PID=80) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:59:08.874+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:59:08.883+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:59:08.882+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:59:09.031+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:59:09.083+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:59:09.081+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:59:09.116+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:59:09.116+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:59:09.139+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.294 seconds
[2023-09-08T04:59:40.050+0000] {processor.py:157} INFO - Started process (PID=89) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:59:40.058+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T04:59:40.069+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:59:40.067+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:59:40.217+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T04:59:40.393+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:59:40.393+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T04:59:40.416+0000] {logging_mixin.py:151} INFO - [2023-09-08T04:59:40.416+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T04:59:40.435+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.414 seconds
[2023-09-08T05:00:10.927+0000] {processor.py:157} INFO - Started process (PID=99) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T05:00:10.932+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T05:00:10.943+0000] {logging_mixin.py:151} INFO - [2023-09-08T05:00:10.941+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T05:00:11.079+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T05:00:11.121+0000] {logging_mixin.py:151} INFO - [2023-09-08T05:00:11.120+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T05:00:11.160+0000] {logging_mixin.py:151} INFO - [2023-09-08T05:00:11.160+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T05:00:11.181+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.268 seconds
[2023-09-08T05:00:41.964+0000] {processor.py:157} INFO - Started process (PID=109) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T05:00:41.978+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T05:00:41.999+0000] {logging_mixin.py:151} INFO - [2023-09-08T05:00:41.998+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T05:00:42.143+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T05:00:42.338+0000] {logging_mixin.py:151} INFO - [2023-09-08T05:00:42.337+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T05:00:42.359+0000] {logging_mixin.py:151} INFO - [2023-09-08T05:00:42.359+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T05:00:42.380+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.450 seconds
[2023-09-08T05:01:13.163+0000] {processor.py:157} INFO - Started process (PID=118) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T05:01:13.169+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T05:01:13.178+0000] {logging_mixin.py:151} INFO - [2023-09-08T05:01:13.177+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T05:01:13.338+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T05:01:13.382+0000] {logging_mixin.py:151} INFO - [2023-09-08T05:01:13.382+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T05:01:13.418+0000] {logging_mixin.py:151} INFO - [2023-09-08T05:01:13.418+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T05:01:13.442+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.301 seconds
[2023-09-08T05:01:44.147+0000] {processor.py:157} INFO - Started process (PID=128) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T05:01:44.151+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T05:01:44.170+0000] {logging_mixin.py:151} INFO - [2023-09-08T05:01:44.169+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T05:01:44.346+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T05:01:44.517+0000] {logging_mixin.py:151} INFO - [2023-09-08T05:01:44.516+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T05:01:44.545+0000] {logging_mixin.py:151} INFO - [2023-09-08T05:01:44.545+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T05:01:44.563+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.425 seconds
[2023-09-08T05:02:14.987+0000] {processor.py:157} INFO - Started process (PID=138) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T05:02:14.992+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T05:02:14.998+0000] {logging_mixin.py:151} INFO - [2023-09-08T05:02:14.997+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T05:02:15.120+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T05:02:15.163+0000] {logging_mixin.py:151} INFO - [2023-09-08T05:02:15.163+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T05:02:15.189+0000] {logging_mixin.py:151} INFO - [2023-09-08T05:02:15.189+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T05:02:15.208+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.227 seconds
[2023-09-08T05:02:45.770+0000] {processor.py:157} INFO - Started process (PID=154) to work on /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T05:02:45.776+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/crawl_data_log_into_postgres.py for tasks to queue
[2023-09-08T05:02:45.785+0000] {logging_mixin.py:151} INFO - [2023-09-08T05:02:45.784+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T05:02:45.935+0000] {processor.py:839} INFO - DAG(s) dict_keys(['website_crawler']) retrieved from /opt/airflow/dags/crawl_data_log_into_postgres.py
[2023-09-08T05:02:46.079+0000] {logging_mixin.py:151} INFO - [2023-09-08T05:02:46.079+0000] {dag.py:2907} INFO - Sync 1 DAGs
[2023-09-08T05:02:46.100+0000] {logging_mixin.py:151} INFO - [2023-09-08T05:02:46.100+0000] {dag.py:3677} INFO - Setting next_dagrun for website_crawler to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00
[2023-09-08T05:02:46.119+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/crawl_data_log_into_postgres.py took 0.372 seconds
